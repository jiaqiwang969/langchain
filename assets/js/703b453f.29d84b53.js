"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[80938],{3905:(e,n,r)=>{r.d(n,{Zo:()=>c,kt:()=>h});var t=r(67294);function a(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function l(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),r.push.apply(r,t)}return r}function o(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?l(Object(r),!0).forEach((function(n){a(e,n,r[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):l(Object(r)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))}))}return e}function p(e,n){if(null==e)return{};var r,t,a=function(e,n){if(null==e)return{};var r,t,a={},l=Object.keys(e);for(t=0;t<l.length;t++)r=l[t],n.indexOf(r)>=0||(a[r]=e[r]);return a}(e,n);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(t=0;t<l.length;t++)r=l[t],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var s=t.createContext({}),i=function(e){var n=t.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):o(o({},n),e)),r},c=function(e){var n=i(e.components);return t.createElement(s.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},d=t.forwardRef((function(e,n){var r=e.components,a=e.mdxType,l=e.originalType,s=e.parentName,c=p(e,["components","mdxType","originalType","parentName"]),m=i(r),d=a,h=m["".concat(s,".").concat(d)]||m[d]||u[d]||l;return r?t.createElement(h,o(o({ref:n},c),{},{components:r})):t.createElement(h,o({ref:n},c))}));function h(e,n){var r=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var l=r.length,o=new Array(l);o[0]=d;var p={};for(var s in n)hasOwnProperty.call(n,s)&&(p[s]=n[s]);p.originalType=e,p[m]="string"==typeof e?e:a,o[1]=p;for(var i=2;i<l;i++)o[i]=r[i];return t.createElement.apply(null,o)}return t.createElement.apply(null,r)}d.displayName="MDXCreateElement"},80856:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>p,toc:()=>i});var t=r(87462),a=(r(67294),r(3905));const l={},o="OpenLLM",p={unversionedId:"integrations/providers/openllm",id:"integrations/providers/openllm",title:"OpenLLM",description:"This page demonstrates how to use OpenLLM",source:"@site/docs/integrations/providers/openllm.mdx",sourceDirName:"integrations/providers",slug:"/integrations/providers/openllm",permalink:"/langchain/docs/integrations/providers/openllm",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"OpenAI",permalink:"/langchain/docs/integrations/providers/openai"},next:{title:"OpenSearch",permalink:"/langchain/docs/integrations/providers/opensearch"}},s={},i=[{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"LLM",id:"llm",level:2},{value:"Wrappers",id:"wrappers",level:2},{value:"Wrapper for OpenLLM server",id:"wrapper-for-openllm-server",level:3},{value:"Wrapper for Local Inference",id:"wrapper-for-local-inference",level:3},{value:"Usage",id:"usage",level:3}],c={toc:i},m="wrapper";function u(e){let{components:n,...r}=e;return(0,a.kt)(m,(0,t.Z)({},c,r,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"openllm"},"OpenLLM"),(0,a.kt)("p",null,"This page demonstrates how to use ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/bentoml/OpenLLM"},"OpenLLM"),"\nwith LangChain."),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"OpenLLM")," is an open platform for operating large language models (LLMs) in\nproduction. It enables developers to easily run inference with any open-source\nLLMs, deploy to the cloud or on-premises, and build powerful AI apps."),(0,a.kt)("h2",{id:"installation-and-setup"},"Installation and Setup"),(0,a.kt)("p",null,"Install the OpenLLM package via PyPI:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"pip install openllm\n")),(0,a.kt)("h2",{id:"llm"},"LLM"),(0,a.kt)("p",null,"OpenLLM supports a wide range of open-source LLMs as well as serving users' own\nfine-tuned LLMs. Use ",(0,a.kt)("inlineCode",{parentName:"p"},"openllm model")," command to see all available models that\nare pre-optimized for OpenLLM."),(0,a.kt)("h2",{id:"wrappers"},"Wrappers"),(0,a.kt)("p",null,"There is a OpenLLM Wrapper which supports loading LLM in-process or accessing a\nremote OpenLLM server:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "OpenLLM", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.openllm.OpenLLM.html", "title": "OpenLLM"}]--\x3e\nfrom langchain.llms import OpenLLM\n')),(0,a.kt)("h3",{id:"wrapper-for-openllm-server"},"Wrapper for OpenLLM server"),(0,a.kt)("p",null,"This wrapper supports connecting to an OpenLLM server via HTTP or gRPC. The\nOpenLLM server can run either locally or on the cloud."),(0,a.kt)("p",null,"To try it out locally, start an OpenLLM server:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"openllm start flan-t5\n")),(0,a.kt)("p",null,"Wrapper usage:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "OpenLLM", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.openllm.OpenLLM.html", "title": "OpenLLM"}]--\x3e\nfrom langchain.llms import OpenLLM\n\nllm = OpenLLM(server_url=\'http://localhost:3000\')\n\nllm("What is the difference between a duck and a goose? And why there are so many Goose in Canada?")\n')),(0,a.kt)("h3",{id:"wrapper-for-local-inference"},"Wrapper for Local Inference"),(0,a.kt)("p",null,"You can also use the OpenLLM wrapper to load LLM in current Python process for\nrunning inference."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "OpenLLM", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.openllm.OpenLLM.html", "title": "OpenLLM"}]--\x3e\nfrom langchain.llms import OpenLLM\n\nllm = OpenLLM(model_name="dolly-v2", model_id=\'databricks/dolly-v2-7b\')\n\nllm("What is the difference between a duck and a goose? And why there are so many Goose in Canada?")\n')),(0,a.kt)("h3",{id:"usage"},"Usage"),(0,a.kt)("p",null,"For a more detailed walkthrough of the OpenLLM Wrapper, see the\n",(0,a.kt)("a",{parentName:"p",href:"/docs/integrations/llms/openllm.html"},"example notebook")))}u.isMDXComponent=!0}}]);