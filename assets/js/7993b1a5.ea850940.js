"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[37293],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>g});var r=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=r.createContext({}),p=function(e){var n=r.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},c=function(e){var n=p(e.components);return r.createElement(l.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(t),d=a,g=m["".concat(l,".").concat(d)]||m[d]||u[d]||o;return t?r.createElement(g,i(i({ref:n},c),{},{components:t})):r.createElement(g,i({ref:n},c))}));function g(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=d;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[m]="string"==typeof e?e:a,i[1]=s;for(var p=2;p<o;p++)i[p]=t[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},98155:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var r=t(87462),a=(t(67294),t(3905));const o={},i="JSONFormer",s={unversionedId:"integrations/llms/jsonformer_experimental",id:"integrations/llms/jsonformer_experimental",title:"JSONFormer",description:"JSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.",source:"@site/docs/integrations/llms/jsonformer_experimental.md",sourceDirName:"integrations/llms",slug:"/integrations/llms/jsonformer_experimental",permalink:"/langchain/docs/integrations/llms/jsonformer_experimental",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Huggingface TextGen Inference",permalink:"/langchain/docs/integrations/llms/huggingface_textgen_inference"},next:{title:"KoboldAI API",permalink:"/langchain/docs/integrations/llms/koboldai"}},l={},p=[{value:"HuggingFace Baseline",id:"huggingface-baseline",level:3},{value:"JSONFormer LLM Wrapper",id:"jsonformer-llm-wrapper",level:2}],c=(m="CodeOutputBlock",function(e){return console.warn("Component "+m+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var m;const u={toc:p},d="wrapper";function g(e){let{components:n,...t}=e;return(0,a.kt)(d,(0,r.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"jsonformer"},"JSONFormer"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://github.com/1rgs/jsonformer"},"JSONFormer")," is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema."),(0,a.kt)("p",null,"It works by filling in the structure tokens and then sampling the content tokens from the model."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Warning - this module is still experimental")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"pip install --upgrade jsonformer > /dev/null\n")),(0,a.kt)("h3",{id:"huggingface-baseline"},"HuggingFace Baseline"),(0,a.kt)("p",null,"First, let's establish a qualitative baseline by checking the output of the model without structured decoding."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import logging\n\nlogging.basicConfig(level=logging.ERROR)\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "tool", "source": "langchain.tools", "docs": "https://api.python.langchain.com/en/latest/tools/langchain.tools.base.tool.html", "title": "JSONFormer"}]--\x3e\nfrom typing import Optional\nfrom langchain.tools import tool\nimport os\nimport json\nimport requests\n\nHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")\n\n\n@tool\ndef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):\n    """Query the BigCode StarCoder model about coding questions."""\n    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"\n    headers = {\n        "Authorization": f"Bearer {HF_TOKEN}",\n        "content-type": "application/json",\n    }\n    payload = {\n        "inputs": f"{query}\\n\\nAnswer:",\n        "temperature": temperature,\n        "max_new_tokens": int(max_new_tokens),\n    }\n    response = requests.post(url, headers=headers, data=json.dumps(payload))\n    response.raise_for_status()\n    return json.loads(response.content.decode("utf-8"))\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'prompt = """You must respond using JSON format, with a single action and single action input.\nYou may \'ask_star_coder\' for help on coding problems.\n\n{arg_schema}\n\nEXAMPLES\n----\nHuman: "So what\'s all this about a GIL?"\nAI Assistant:{{\n  "action": "ask_star_coder",\n  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"\n}}\nObservation: "The GIL is python\'s Global Interpreter Lock"\nHuman: "Could you please write a calculator program in LISP?"\nAI Assistant:{{\n  "action": "ask_star_coder",\n  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}\n}}\nObservation: "(defun add (x y) (+ x y))\\n(defun sub (x y) (- x y ))"\nHuman: "What\'s the difference between an SVM and an LLM?"\nAI Assistant:{{\n  "action": "ask_star_coder",\n  "action_input": {{"query": "What\'s the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}\n}}\nObservation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."\n\nBEGIN! Answer the Human\'s question as best as you are able.\n------\nHuman: \'What\'s the difference between an iterator and an iterable?\'\nAI Assistant:""".format(\n    arg_schema=ask_star_coder.args\n)\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "HuggingFacePipeline", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.huggingface_pipeline.HuggingFacePipeline.html", "title": "JSONFormer"}]--\x3e\nfrom transformers import pipeline\nfrom langchain.llms import HuggingFacePipeline\n\nhf_model = pipeline(\n    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200\n)\n\noriginal_model = HuggingFacePipeline(pipeline=hf_model)\n\ngenerated = original_model.predict(prompt, stop=["Observation:", "Human:"])\nprint(generated)\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n     'What's the difference between an iterator and an iterable?'\n    \n"))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},(0,a.kt)("em",{parentName:"strong"},"That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder."))),(0,a.kt)("h2",{id:"jsonformer-llm-wrapper"},"JSONFormer LLM Wrapper"),(0,a.kt)("p",null,"Let's try that again, now providing a the Action input's JSON Schema to the model."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'decoder_schema = {\n    "title": "Decoding Schema",\n    "type": "object",\n    "properties": {\n        "action": {"type": "string", "default": ask_star_coder.name},\n        "action_input": {\n            "type": "object",\n            "properties": ask_star_coder.args,\n        },\n    },\n}\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from langchain_experimental.llms import JsonFormer\n\njson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'results = json_former.predict(prompt, stop=["Observation:", "Human:"])\nprint(results)\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'    {"action": "ask_star_coder", "action_input": {"query": "What\'s the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}\n'))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Voila! Free of parsing errors.")))}g.isMDXComponent=!0}}]);