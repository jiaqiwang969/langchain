"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[70316],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>d});var i=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function p(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},l=Object.keys(e);for(i=0;i<l.length;i++)n=l[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(i=0;i<l.length;i++)n=l[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var o=i.createContext({}),s=function(e){var t=i.useContext(o),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=s(e.components);return i.createElement(o.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},h=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,l=e.originalType,o=e.parentName,c=p(e,["components","mdxType","originalType","parentName"]),u=s(n),h=a,d=u["".concat(o,".").concat(h)]||u[h]||m[h]||l;return n?i.createElement(d,r(r({ref:t},c),{},{components:n})):i.createElement(d,r({ref:t},c))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var l=n.length,r=new Array(l);r[0]=h;var p={};for(var o in t)hasOwnProperty.call(t,o)&&(p[o]=t[o]);p.originalType=e,p[u]="string"==typeof e?e:a,r[1]=p;for(var s=2;s<l;s++)r[s]=n[s];return i.createElement.apply(null,r)}return i.createElement.apply(null,n)}h.displayName="MDXCreateElement"},91793:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>r,default:()=>m,frontMatter:()=>l,metadata:()=>p,toc:()=>s});var i=n(87462),a=(n(67294),n(3905));const l={},r="PipelineAI",p={unversionedId:"integrations/llms/pipelineai",id:"integrations/llms/pipelineai",title:"PipelineAI",description:"PipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.",source:"@site/docs/integrations/llms/pipelineai.md",sourceDirName:"integrations/llms",slug:"/integrations/llms/pipelineai",permalink:"/langchain/docs/integrations/llms/pipelineai",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Petals",permalink:"/langchain/docs/integrations/llms/petals"},next:{title:"Predibase",permalink:"/langchain/docs/integrations/llms/predibase"}},o={},s=[{value:"PipelineAI example",id:"pipelineai-example",level:2},{value:"Setup",id:"setup",level:2},{value:"Example",id:"example",level:2},{value:"Imports",id:"imports",level:3},{value:"Set the Environment API Key",id:"set-the-environment-api-key",level:3},{value:"Create the PipelineAI instance",id:"create-the-pipelineai-instance",level:2},{value:"Create a Prompt Template",id:"create-a-prompt-template",level:3},{value:"Initiate the LLMChain",id:"initiate-the-llmchain",level:3},{value:"Run the LLMChain",id:"run-the-llmchain",level:3}],c={toc:s},u="wrapper";function m(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,i.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"pipelineai"},"PipelineAI"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},(0,a.kt)("a",{parentName:"p",href:"https://pipeline.ai"},"PipelineAI")," allows you to run your ML models at scale in the cloud. It also provides API access to ",(0,a.kt)("a",{parentName:"p",href:"https://pipeline.ai"},"several LLM models"),".")),(0,a.kt)("p",null,"This notebook goes over how to use Langchain with ",(0,a.kt)("a",{parentName:"p",href:"https://docs.pipeline.ai/docs"},"PipelineAI"),"."),(0,a.kt)("h2",{id:"pipelineai-example"},"PipelineAI example"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://docs.pipeline.ai/docs/langchain"},"This example shows how PipelineAI integrated with LangChain")," and it is created by PipelineAI."),(0,a.kt)("h2",{id:"setup"},"Setup"),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"pipeline-ai")," library is required to use the ",(0,a.kt)("inlineCode",{parentName:"p"},"PipelineAI")," API, AKA ",(0,a.kt)("inlineCode",{parentName:"p"},"Pipeline Cloud"),". Install ",(0,a.kt)("inlineCode",{parentName:"p"},"pipeline-ai")," using ",(0,a.kt)("inlineCode",{parentName:"p"},"pip install pipeline-ai"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"# Install the package\npip install pipeline-ai\n")),(0,a.kt)("h2",{id:"example"},"Example"),(0,a.kt)("h3",{id:"imports"},"Imports"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "PipelineAI", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.pipelineai.PipelineAI.html", "title": "PipelineAI"}]--\x3e\nimport os\nfrom langchain.llms import PipelineAI\nfrom langchain import PromptTemplate, LLMChain\n')),(0,a.kt)("h3",{id:"set-the-environment-api-key"},"Set the Environment API Key"),(0,a.kt)("p",null,"Make sure to get your API key from PipelineAI. Check out the ",(0,a.kt)("a",{parentName:"p",href:"https://docs.pipeline.ai/docs/cloud-quickstart"},"cloud quickstart guide"),". You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE"\n')),(0,a.kt)("h2",{id:"create-the-pipelineai-instance"},"Create the PipelineAI instance"),(0,a.kt)("p",null,"When instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. ",(0,a.kt)("inlineCode",{parentName:"p"},'pipeline_key = "public/gpt-j:base"'),". You then have the option of passing additional pipeline-specific keyword arguments:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...})\n')),(0,a.kt)("h3",{id:"create-a-prompt-template"},"Create a Prompt Template"),(0,a.kt)("p",null,"We will create a prompt template for Question and Answer."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'template = """Question: {question}\n\nAnswer: Let\'s think step by step."""\n\nprompt = PromptTemplate(template=template, input_variables=["question"])\n')),(0,a.kt)("h3",{id:"initiate-the-llmchain"},"Initiate the LLMChain"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"llm_chain = LLMChain(prompt=prompt, llm=llm)\n")),(0,a.kt)("h3",{id:"run-the-llmchain"},"Run the LLMChain"),(0,a.kt)("p",null,"Provide a question and run the LLMChain."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"\n\nllm_chain.run(question)\n')))}m.isMDXComponent=!0}}]);