"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[71682],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>h});var a=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var l=a.createContext({}),d=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},c=function(e){var n=d(e.components);return a.createElement(l.Provider,{value:n},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(t),m=o,h=p["".concat(l,".").concat(m)]||p[m]||u[m]||i;return t?a.createElement(h,r(r({ref:n},c),{},{components:t})):a.createElement(h,r({ref:n},c))}));function h(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var i=t.length,r=new Array(i);r[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[p]="string"==typeof e?e:o,r[1]=s;for(var d=2;d<i;d++)r[d]=t[d];return a.createElement.apply(null,r)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},7494:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>m,contentTitle:()=>p,default:()=>y,frontMatter:()=>c,metadata:()=>u,toc:()=>h});var a=t(87462),o=(t(67294),t(3905));const i=(r="CodeOutputBlock",function(e){return console.warn("Component "+r+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var r;const s={toc:[{value:"Using MathPix",id:"using-mathpix",level:2},{value:"Using Unstructured",id:"using-unstructured",level:2},{value:"Retain Elements",id:"retain-elements",level:3},{value:"Fetching remote PDFs using Unstructured",id:"fetching-remote-pdfs-using-unstructured",level:3},{value:"Using PyPDFium2",id:"using-pypdfium2",level:2},{value:"Using PDFMiner",id:"using-pdfminer",level:2},{value:"Using PDFMiner to generate HTML text",id:"using-pdfminer-to-generate-html-text",level:3},{value:"Using PyMuPDF",id:"using-pymupdf",level:2},{value:"PyPDF Directory",id:"pypdf-directory",level:2},{value:"Using PDFPlumber",id:"using-pdfplumber",level:2},{value:"Using AmazonTextractPDFParser",id:"using-amazontextractpdfparser",level:2}]},l="wrapper";function d(e){let{components:n,...t}=e;return(0,o.kt)(l,(0,a.Z)({},s,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"using-pypdf"},"Using PyPDF"),(0,o.kt)("p",null,"Load PDF using ",(0,o.kt)("inlineCode",{parentName:"p"},"pypdf")," into array of documents, where each document contains the page content and metadata with ",(0,o.kt)("inlineCode",{parentName:"p"},"page")," number."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install pypdf\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.document_loaders import PyPDFLoader\n\nloader = PyPDFLoader("example_data/layout-parser-paper.pdf")\npages = loader.load_and_split()\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"pages[0]\n")),(0,o.kt)(i,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Document(page_content='LayoutParser : A Uni\\x0ced Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1Allen Institute for AI\\nshannons@allenai.org\\n2Brown University\\nruochen zhang@brown.edu\\n3Harvard University\\nfmelissadell,jacob carlson g@fas.harvard.edu\\n4University of Washington\\nbcgl@cs.washington.edu\\n5University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model con\\x0cgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\ne\\x0borts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser , an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io .\\nKeywords: Document Image Analysis \xb7Deep Learning \xb7Layout Analysis\\n\xb7Character Recognition \xb7Open Source library \xb7Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classi\\x0ccation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021', metadata={'source': 'example_data/layout-parser-paper.pdf', 'page': 0})\n"))),(0,o.kt)("p",null,"An advantage of this approach is that documents can be retrieved with page numbers."),(0,o.kt)("p",null,"We want to use ",(0,o.kt)("inlineCode",{parentName:"p"},"OpenAIEmbeddings")," so we have to get the OpenAI API Key."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport getpass\n\nos.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n")),(0,o.kt)(i,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    OpenAI API Key: \xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.vectorstores import FAISS\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfaiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\ndocs = faiss_index.similarity_search("How will the community be engaged?", k=2)\nfor doc in docs:\n    print(str(doc.metadata["page"]) + ":", doc.page_content[:300])\n')),(0,o.kt)(i,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    9: 10 Z. Shen et al.\n    Fig. 4: Illustration of (a) the original historical Japanese document with layout\n    detection results and (b) a recreated version of the document image that achieves\n    much better character recognition recall. The reorganization algorithm rearranges\n    the tokens based on the their detect\n    3: 4 Z. Shen et al.\n    Efficient Data AnnotationC u s t o m i z e d  M o d e l  T r a i n i n gModel Cust omizationDI A Model HubDI A Pipeline SharingCommunity PlatformLa y out Detection ModelsDocument Images \n    T h e  C o r e  L a y o u t P a r s e r  L i b r a r yOCR ModuleSt or age & VisualizationLa y ou\n"))),(0,o.kt)("h2",{id:"using-mathpix"},"Using MathPix"),(0,o.kt)("p",null,"Inspired by Daniel Gross's ",(0,o.kt)("a",{parentName:"p",href:"https://gist.github.com/danielgross/3ab4104e14faccc12b49200843adab21"},"https://gist.github.com/danielgross/3ab4104e14faccc12b49200843adab21")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders import MathpixPDFLoader\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = MathpixPDFLoader("example_data/layout-parser-paper.pdf")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data = loader.load()\n")),(0,o.kt)("h2",{id:"using-unstructured"},"Using Unstructured"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders import UnstructuredPDFLoader\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = UnstructuredPDFLoader("example_data/layout-parser-paper.pdf")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data = loader.load()\n")),(0,o.kt)("h3",{id:"retain-elements"},"Retain Elements"),(0,o.kt)("p",null,'Under the hood, Unstructured creates different "elements" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying ',(0,o.kt)("inlineCode",{parentName:"p"},'mode="elements"'),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = UnstructuredPDFLoader("example_data/layout-parser-paper.pdf", mode="elements")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data = loader.load()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data[0]\n")),(0,o.kt)(i,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Document(page_content='LayoutParser: A Uni\ufb01ed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\ufffd), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model con\ufb01gurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\ne\ufb00orts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: Document Image Analysis \xb7 Deep Learning \xb7 Layout Analysis\\n\xb7 Character Recognition \xb7 Open Source library \xb7 Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classi\ufb01cation [11,\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\n', lookup_str='', metadata={'file_path': 'example_data/layout-parser-paper.pdf', 'page_number': 1, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': '', 'encryption': None}, lookup_index=0)\n"))),(0,o.kt)("h3",{id:"fetching-remote-pdfs-using-unstructured"},"Fetching remote PDFs using Unstructured"),(0,o.kt)("p",null,"This covers how to load online PDFs into a document format that we can use downstream. This can be used for various online PDF sites such as ",(0,o.kt)("a",{parentName:"p",href:"https://open.umn.edu/opentextbooks/textbooks/"},"https://open.umn.edu/opentextbooks/textbooks/")," and ",(0,o.kt)("a",{parentName:"p",href:"https://arxiv.org/archive/"},"https://arxiv.org/archive/")),(0,o.kt)("p",null,"Note: all other PDF loaders can also be used to fetch remote PDFs, but ",(0,o.kt)("inlineCode",{parentName:"p"},"OnlinePDFLoader")," is a legacy function, and works specifically with ",(0,o.kt)("inlineCode",{parentName:"p"},"UnstructuredPDFLoader"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders import OnlinePDFLoader\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = OnlinePDFLoader("https://arxiv.org/pdf/2302.03803.pdf")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data = loader.load()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"print(data)\n")),(0,o.kt)(i,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    [Document(page_content='A WEAK ( k, k ) -LEFSCHETZ THEOREM FOR PROJECTIVE TORIC ORBIFOLDS\\n\\nWilliam D. Montoya\\n\\nInstituto de Matem\xb4atica, Estat\xb4\u0131stica e Computa\xb8c\u02dcao Cient\xb4\u0131\ufb01ca,\\n\\nIn [3] we proved that, under suitable conditions, on a very general codimension s quasi- smooth intersection subvariety X in a projective toric orbifold P d \u03a3 with d + s = 2 ( k + 1 ) the Hodge conjecture holds, that is, every ( p, p ) -cohomology class, under the Poincar\xb4e duality is a rational linear combination of fundamental classes of algebraic subvarieties of X . The proof of the above-mentioned result relies, for p \u2260 d + 1 \u2212 s , on a Lefschetz\\n\\nKeywords: (1,1)- Lefschetz theorem, Hodge conjecture, toric varieties, complete intersection Email: wmontoya@ime.unicamp.br\\n\\ntheorem ([7]) and the Hard Lefschetz theorem for projective orbifolds ([11]). When p = d + 1 \u2212 s the proof relies on the Cayley trick, a trick which associates to X a quasi-smooth hypersurface Y in a projective vector bundle, and the Cayley Proposition (4.3) which gives an isomorphism of some primitive cohomologies (4.2) of X and Y . The Cayley trick, following the philosophy of Mavlyutov in [7], reduces results known for quasi-smooth hypersurfaces to quasi-smooth intersection subvarieties. The idea in this paper goes the other way around, we translate some results for quasi-smooth intersection subvarieties to\\n\\nAcknowledgement. I thank Prof. Ugo Bruzzo and Tiago Fonseca for useful discus- sions. I also acknowledge support from FAPESP postdoctoral grant No. 2019/23499-7.\\n\\nLet M be a free abelian group of rank d , let N = Hom ( M, Z ) , and N R = N \u2297 Z R .\\n\\nif there exist k linearly independent primitive elements e\\n\\n, . . . , e k \u2208 N such that \u03c3 = { \xb5\\n\\ne\\n\\n+ \u22ef + \xb5 k e k } . \u2022 The generators e i are integral if for every i and any nonnegative rational number \xb5 the product \xb5e i is in N only if \xb5 is an integer. \u2022 Given two rational simplicial cones \u03c3 , \u03c3 \u2032 one says that \u03c3 \u2032 is a face of \u03c3 ( \u03c3 \u2032 < \u03c3 ) if the set of integral generators of \u03c3 \u2032 is a subset of the set of integral generators of \u03c3 . \u2022 A \ufb01nite set \u03a3 = { \u03c3\\n\\n, . . . , \u03c3 t } of rational simplicial cones is called a rational simplicial complete d -dimensional fan if:\\n\\nall faces of cones in \u03a3 are in \u03a3 ;\\n\\nif \u03c3, \u03c3 \u2032 \u2208 \u03a3 then \u03c3 \u2229 \u03c3 \u2032 < \u03c3 and \u03c3 \u2229 \u03c3 \u2032 < \u03c3 \u2032 ;\\n\\nN R = \u03c3\\n\\n\u222a \u22c5 \u22c5 \u22c5 \u222a \u03c3 t .\\n\\nA rational simplicial complete d -dimensional fan \u03a3 de\ufb01nes a d -dimensional toric variety P d \u03a3 having only orbifold singularities which we assume to be projective. Moreover, T \u2236 = N \u2297 Z C \u2217 \u2243 ( C \u2217 ) d is the torus action on P d \u03a3 . We denote by \u03a3 ( i ) the i -dimensional cones\\n\\nFor a cone \u03c3 \u2208 \u03a3, \u02c6 \u03c3 is the set of 1-dimensional cone in \u03a3 that are not contained in \u03c3\\n\\nand x \u02c6 \u03c3 \u2236 = \u220f \u03c1 \u2208 \u02c6 \u03c3 x \u03c1 is the associated monomial in S .\\n\\nDe\ufb01nition 2.2. The irrelevant ideal of P d \u03a3 is the monomial ideal B \u03a3 \u2236 =< x \u02c6 \u03c3 \u2223 \u03c3 \u2208 \u03a3 > and the zero locus Z ( \u03a3 ) \u2236 = V ( B \u03a3 ) in the a\ufb03ne space A d \u2236 = Spec ( S ) is the irrelevant locus.\\n\\nProposition 2.3 (Theorem 5.1.11 [5]) . The toric variety P d \u03a3 is a categorical quotient A d \u2216 Z ( \u03a3 ) by the group Hom ( Cl ( \u03a3 ) , C \u2217 ) and the group action is induced by the Cl ( \u03a3 ) - grading of S .\\n\\nNow we give a brief introduction to complex orbifolds and we mention the needed theorems for the next section. Namely: de Rham theorem and Dolbeault theorem for complex orbifolds.\\n\\nDe\ufb01nition 2.4. A complex orbifold of complex dimension d is a singular complex space whose singularities are locally isomorphic to quotient singularities C d / G , for \ufb01nite sub- groups G \u2282 Gl ( d, C ) .\\n\\nDe\ufb01nition 2.5. A di\ufb00erential form on a complex orbifold Z is de\ufb01ned locally at z \u2208 Z as a G -invariant di\ufb00erential form on C d where G \u2282 Gl ( d, C ) and Z is locally isomorphic to d\\n\\nRoughly speaking the local geometry of orbifolds reduces to local G -invariant geometry.\\n\\nWe have a complex of di\ufb00erential forms ( A \u25cf ( Z ) , d ) and a double complex ( A \u25cf , \u25cf ( Z ) , \u2202, \xaf \u2202 ) of bigraded di\ufb00erential forms which de\ufb01ne the de Rham and the Dolbeault cohomology groups (for a \ufb01xed p \u2208 N ) respectively:\\n\\n(1,1)-Lefschetz theorem for projective toric orbifolds\\n\\nDe\ufb01nition 3.1. A subvariety X \u2282 P d \u03a3 is quasi-smooth if V ( I X ) \u2282 A #\u03a3 ( 1 ) is smooth outside\\n\\nExample 3.2 . Quasi-smooth hypersurfaces or more generally quasi-smooth intersection sub-\\n\\nExample 3.2 . Quasi-smooth hypersurfaces or more generally quasi-smooth intersection sub- varieties are quasi-smooth subvarieties (see [2] or [7] for more details).\\n\\nRemark 3.3 . Quasi-smooth subvarieties are suborbifolds of P d \u03a3 in the sense of Satake in [8]. Intuitively speaking they are subvarieties whose only singularities come from the ambient\\n\\nProof. From the exponential short exact sequence\\n\\nwe have a long exact sequence in cohomology\\n\\nH 1 (O \u2217 X ) \u2192 H 2 ( X, Z ) \u2192 H 2 (O X ) \u2243 H 0 , 2 ( X )\\n\\nwhere the last isomorphisms is due to Steenbrink in [9]. Now, it is enough to prove the commutativity of the next diagram\\n\\nwhere the last isomorphisms is due to Steenbrink in [9]. Now,\\n\\nH 2 ( X, Z ) / / H 2 ( X, O X ) \u2243 Dolbeault H 2 ( X, C ) deRham \u2243 H 2 dR ( X, C ) / / H 0 , 2 \xaf \u2202 ( X )\\n\\nof the proof follows as the ( 1 , 1 ) -Lefschetz theorem in [6].\\n\\nRemark 3.5 . For k = 1 and P d \u03a3 as the projective space, we recover the classical ( 1 , 1 ) - Lefschetz theorem.\\n\\nBy the Hard Lefschetz Theorem for projective orbifolds (see [11] for details) we\\n\\nBy the Hard Lefschetz Theorem for projective orbifolds (see [11] for details) we get an isomorphism of cohomologies :\\n\\ngiven by the Lefschetz morphism and since it is a morphism of Hodge structures, we have:\\n\\nH 1 , 1 ( X, Q ) \u2243 H dim X \u2212 1 , dim X \u2212 1 ( X, Q )\\n\\nCorollary 3.6. If the dimension of X is 1 , 2 or 3 . The Hodge conjecture holds on X\\n\\nProof. If the dim C X = 1 the result is clear by the Hard Lefschetz theorem for projective orbifolds. The dimension 2 and 3 cases are covered by Theorem 3.5 and the Hard Lefschetz.\\n\\nCayley trick and Cayley proposition\\n\\nThe Cayley trick is a way to associate to a quasi-smooth intersection subvariety a quasi- smooth hypersurface. Let L 1 , . . . , L s be line bundles on P d \u03a3 and let \u03c0 \u2236 P ( E ) \u2192 P d \u03a3 be the projective space bundle associated to the vector bundle E = L 1 \u2295 \u22ef \u2295 L s . It is known that P ( E ) is a ( d + s \u2212 1 ) -dimensional simplicial toric variety whose fan depends on the degrees of the line bundles and the fan \u03a3. Furthermore, if the Cox ring, without considering the grading, of P d \u03a3 is C [ x 1 , . . . , x m ] then the Cox ring of P ( E ) is\\n\\nMoreover for X a quasi-smooth intersection subvariety cut o\ufb00 by f 1 , . . . , f s with deg ( f i ) = [ L i ] we relate the hypersurface Y cut o\ufb00 by F = y 1 f 1 + \u22c5 \u22c5 \u22c5 + y s f s which turns out to be quasi-smooth. For more details see Section 2 in [7].\\n\\nWe will denote P ( E ) as P d + s \u2212 1 \u03a3 ,X to keep track of its relation with X and P d \u03a3 .\\n\\nThe following is a key remark.\\n\\nRemark 4.1 . There is a morphism \u03b9 \u2236 X \u2192 Y \u2282 P d + s \u2212 1 \u03a3 ,X . Moreover every point z \u2236 = ( x, y ) \u2208 Y with y \u2260 0 has a preimage. Hence for any subvariety W = V ( I W ) \u2282 X \u2282 P d \u03a3 there exists W \u2032 \u2282 Y \u2282 P d + s \u2212 1 \u03a3 ,X such that \u03c0 ( W \u2032 ) = W , i.e., W \u2032 = { z = ( x, y ) \u2223 x \u2208 W } .\\n\\nFor X \u2282 P d \u03a3 a quasi-smooth intersection variety the morphism in cohomology induced by the inclusion i \u2217 \u2236 H d \u2212 s ( P d \u03a3 , C ) \u2192 H d \u2212 s ( X, C ) is injective by Proposition 1.4 in [7].\\n\\nDe\ufb01nition 4.2. The primitive cohomology of H d \u2212 s prim ( X ) is the quotient H d \u2212 s ( X, C )/ i \u2217 ( H d \u2212 s ( P d \u03a3 , C )) and H d \u2212 s prim ( X, Q ) with rational coe\ufb03cients.\\n\\nH d \u2212 s ( P d \u03a3 , C ) and H d \u2212 s ( X, C ) have pure Hodge structures, and the morphism i \u2217 is com- patible with them, so that H d \u2212 s prim ( X ) gets a pure Hodge structure.\\n\\nThe next Proposition is the Cayley proposition.\\n\\nProposition 4.3. [Proposition 2.3 in [3] ] Let X = X 1 \u2229\u22c5 \u22c5 \u22c5\u2229 X s be a quasi-smooth intersec- tion subvariety in P d \u03a3 cut o\ufb00 by homogeneous polynomials f 1 . . . f s . Then for p \u2260 d + s \u2212 1 2 , d + s \u2212 3 2\\n\\nRemark 4.5 . The above isomorphisms are also true with rational coe\ufb03cients since H \u25cf ( X, C ) = H \u25cf ( X, Q ) \u2297 Q C . See the beginning of Section 7.1 in [10] for more details.\\n\\nTheorem 5.1. Let Y = { F = y 1 f 1 + \u22ef + y k f k = 0 } \u2282 P 2 k + 1 \u03a3 ,X be the quasi-smooth hypersurface associated to the quasi-smooth intersection surface X = X f 1 \u2229 \u22c5 \u22c5 \u22c5 \u2229 X f k \u2282 P k + 2 \u03a3 . Then on Y the Hodge conjecture holds.\\n\\nthe Hodge conjecture holds.\\n\\nProof. If H k,k prim ( X, Q ) = 0 we are done. So let us assume H k,k prim ( X, Q ) \u2260 0. By the Cayley proposition H k,k prim ( Y, Q ) \u2243 H 1 , 1 prim ( X, Q ) and by the ( 1 , 1 ) -Lefschetz theorem for projective\\n\\ntoric orbifolds there is a non-zero algebraic basis \u03bb C 1 , . . . , \u03bb C n with rational coe\ufb03cients of H 1 , 1 prim ( X, Q ) , that is, there are n \u2236 = h 1 , 1 prim ( X, Q ) algebraic curves C 1 , . . . , C n in X such that under the Poincar\xb4e duality the class in homology [ C i ] goes to \u03bb C i , [ C i ] \u21a6 \u03bb C i . Recall that the Cox ring of P k + 2 is contained in the Cox ring of P 2 k + 1 \u03a3 ,X without considering the grading. Considering the grading we have that if \u03b1 \u2208 Cl ( P k + 2 \u03a3 ) then ( \u03b1, 0 ) \u2208 Cl ( P 2 k + 1 \u03a3 ,X ) . So the polynomials de\ufb01ning C i \u2282 P k + 2 \u03a3 can be interpreted in P 2 k + 1 X, \u03a3 but with di\ufb00erent degree. Moreover, by Remark 4.1 each C i is contained in Y = { F = y 1 f 1 + \u22ef + y k f k = 0 } and\\n\\nfurthermore it has codimension k .\\n\\nClaim: { C i } ni = 1 is a basis of prim ( ) . It is enough to prove that \u03bb C i is di\ufb00erent from zero in H k,k prim ( Y, Q ) or equivalently that the cohomology classes { \u03bb C i } ni = 1 do not come from the ambient space. By contradiction, let us assume that there exists a j and C \u2282 P 2 k + 1 \u03a3 ,X such that \u03bb C \u2208 H k,k ( P 2 k + 1 \u03a3 ,X , Q ) with i \u2217 ( \u03bb C ) = \u03bb C j or in terms of homology there exists a ( k + 2 ) -dimensional algebraic subvariety V \u2282 P 2 k + 1 \u03a3 ,X such that V \u2229 Y = C j so they are equal as a homology class of P 2 k + 1 \u03a3 ,X ,i.e., [ V \u2229 Y ] = [ C j ] . It is easy to check that \u03c0 ( V ) \u2229 X = C j as a subvariety of P k + 2 \u03a3 where \u03c0 \u2236 ( x, y ) \u21a6 x . Hence [ \u03c0 ( V ) \u2229 X ] = [ C j ] which is equivalent to say that \u03bb C j comes from P k + 2 \u03a3 which contradicts the choice of [ C j ] .\\n\\nRemark 5.2 . Into the proof of the previous theorem, the key fact was that on X the Hodge conjecture holds and we translate it to Y by contradiction. So, using an analogous argument we have:\\n\\nargument we have:\\n\\nProposition 5.3. Let Y = { F = y 1 f s +\u22ef+ y s f s = 0 } \u2282 P 2 k + 1 \u03a3 ,X be the quasi-smooth hypersurface associated to a quasi-smooth intersection subvariety X = X f 1 \u2229 \u22c5 \u22c5 \u22c5 \u2229 X f s \u2282 P d \u03a3 such that d + s = 2 ( k + 1 ) . If the Hodge conjecture holds on X then it holds as well on Y .\\n\\nCorollary 5.4. If the dimension of Y is 2 s \u2212 1 , 2 s or 2 s + 1 then the Hodge conjecture holds on Y .\\n\\nProof. By Proposition 5.3 and Corollary 3.6.\\n\\n[\\n\\n] Angella, D. Cohomologies of certain orbifolds. Journal of Geometry and Physics\\n\\n(\\n\\n),\\n\\n\u2013\\n\\n[\\n\\n] Batyrev, V. V., and Cox, D. A. On the Hodge structure of projective hypersur- faces in toric varieties. Duke Mathematical Journal\\n\\n,\\n\\n(Aug\\n\\n). [\\n\\n] Bruzzo, U., and Montoya, W. On the Hodge conjecture for quasi-smooth in- tersections in toric varieties. S\u02dcao Paulo J. Math. Sci. Special Section: Geometry in Algebra and Algebra in Geometry (\\n\\n). [\\n\\n] Caramello Jr, F. C. Introduction to orbifolds. a\\n\\niv:\\n\\nv\\n\\n(\\n\\n). [\\n\\n] Cox, D., Little, J., and Schenck, H. Toric varieties, vol.\\n\\nAmerican Math- ematical Soc.,\\n\\n[\\n\\n] Griffiths, P., and Harris, J. Principles of Algebraic Geometry. John Wiley & Sons, Ltd,\\n\\n[\\n\\n] Mavlyutov, A. R. Cohomology of complete intersections in toric varieties. Pub- lished in Paci\ufb01c J. of Math.\\n\\nNo.\\n\\n(\\n\\n),\\n\\n\u2013\\n\\n[\\n\\n] Satake, I. On a Generalization of the Notion of Manifold. Proceedings of the National Academy of Sciences of the United States of America\\n\\n,\\n\\n(\\n\\n),\\n\\n\u2013\\n\\n[\\n\\n] Steenbrink, J. H. M. Intersection form for quasi-homogeneous singularities. Com- positio Mathematica\\n\\n,\\n\\n(\\n\\n),\\n\\n\u2013\\n\\n[\\n\\n] Voisin, C. Hodge Theory and Complex Algebraic Geometry I, vol.\\n\\nof Cambridge Studies in Advanced Mathematics . Cambridge University Press,\\n\\n[\\n\\n] Wang, Z. Z., and Zaffran, D. A remark on the Hard Lefschetz theorem for K\xa8ahler orbifolds. Proceedings of the American Mathematical Society\\n\\n,\\n\\n(Aug\\n\\n).\\n\\n[2] Batyrev, V. V., and Cox, D. A. On the Hodge structure of projective hypersur- faces in toric varieties. Duke Mathematical Journal 75, 2 (Aug 1994).\\n\\n[\\n\\n] Bruzzo, U., and Montoya, W. On the Hodge conjecture for quasi-smooth in- tersections in toric varieties. S\u02dcao Paulo J. Math. Sci. Special Section: Geometry in Algebra and Algebra in Geometry (\\n\\n).\\n\\n[3] Bruzzo, U., and Montoya, W. On the Hodge conjecture for quasi-smooth in- tersections in toric varieties. S\u02dcao Paulo J. Math. Sci. Special Section: Geometry in Algebra and Algebra in Geometry (2021).\\n\\nA. R. Cohomology of complete intersections in toric varieties. Pub-', lookup_str='', metadata={'source': '/var/folders/ph/hhm7_zyx4l13k3v8z02dwp1w0000gn/T/tmpgq0ckaja/online_file.pdf'}, lookup_index=0)]\n"))),(0,o.kt)("h2",{id:"using-pypdfium2"},"Using PyPDFium2"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders import PyPDFium2Loader\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = PyPDFium2Loader("example_data/layout-parser-paper.pdf")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data = loader.load()\n")),(0,o.kt)("h2",{id:"using-pdfminer"},"Using PDFMiner"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders import PDFMinerLoader\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = PDFMinerLoader("example_data/layout-parser-paper.pdf")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data = loader.load()\n")),(0,o.kt)("h3",{id:"using-pdfminer-to-generate-html-text"},"Using PDFMiner to generate HTML text"),(0,o.kt)("p",null,"This can be helpful for chunking texts semantically into sections as the output html content can be parsed via ",(0,o.kt)("inlineCode",{parentName:"p"},"BeautifulSoup")," to get more structured and rich information about font size, page numbers, PDF headers/footers, etc."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders import PDFMinerPDFasHTMLLoader\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = PDFMinerPDFasHTMLLoader("example_data/layout-parser-paper.pdf")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data = loader.load()[0]   # entire PDF is loaded as a single Document\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from bs4 import BeautifulSoup\nsoup = BeautifulSoup(data.page_content,'html.parser')\ncontent = soup.find_all('div')\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import re\ncur_fs = None\ncur_text = ''\nsnippets = []   # first collect all snippets that have the same font size\nfor c in content:\n    sp = c.find('span')\n    if not sp:\n        continue\n    st = sp.get('style')\n    if not st:\n        continue\n    fs = re.findall('font-size:(\\d+)px',st)\n    if not fs:\n        continue\n    fs = int(fs[0])\n    if not cur_fs:\n        cur_fs = fs\n    if fs == cur_fs:\n        cur_text += c.text\n    else:\n        snippets.append((cur_text,cur_fs))\n        cur_fs = fs\n        cur_text = c.text\nsnippets.append((cur_text,cur_fs))\n# Note: The above logic is very straightforward. One can also add more strategies such as removing duplicate snippets (as\n# headers/footers in a PDF appear on multiple pages so if we find duplicates it's safe to assume that it is redundant info)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.docstore.document import Document\ncur_idx = -1\nsemantic_snippets = []\n# Assumption: headings have higher font size than their respective content\nfor s in snippets:\n    # if current snippet's font size > previous section's heading => it is a new heading\n    if not semantic_snippets or s[1] > semantic_snippets[cur_idx].metadata['heading_font']:\n        metadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]}\n        metadata.update(data.metadata)\n        semantic_snippets.append(Document(page_content='',metadata=metadata))\n        cur_idx += 1\n        continue\n    \n    # if current snippet's font size <= previous section's content => content belongs to the same section (one can also create\n    # a tree like structure for sub sections if needed but that may require some more thinking and may be data specific)\n    if not semantic_snippets[cur_idx].metadata['content_font'] or s[1] <= semantic_snippets[cur_idx].metadata['content_font']:\n        semantic_snippets[cur_idx].page_content += s[0]\n        semantic_snippets[cur_idx].metadata['content_font'] = max(s[1], semantic_snippets[cur_idx].metadata['content_font'])\n        continue\n    \n    # if current snippet's font size > previous section's content but less than previous section's heading than also make a new \n    # section (e.g. title of a PDF will have the highest font size but we don't want it to subsume all sections)\n    metadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]}\n    metadata.update(data.metadata)\n    semantic_snippets.append(Document(page_content='',metadata=metadata))\n    cur_idx += 1\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"semantic_snippets[4]\n")),(0,o.kt)(i,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Document(page_content='Recently, various DL models and datasets have been developed for layout analysis\\ntasks. The dhSegment [22] utilizes fully convolutional networks [20] for segmen-\\ntation tasks on historical documents. Object detection-based methods like Faster\\nR-CNN [28] and Mask R-CNN [12] are used for identifying document elements [38]\\nand detecting tables [30, 26]. Most recently, Graph Neural Networks [29] have also\\nbeen used in table detection [27]. However, these models are usually implemented\\nindividually and there is no uni\ufb01ed framework to load and use such models.\\nThere has been a surge of interest in creating open-source tools for document\\nimage processing: a search of document image analysis in Github leads to 5M\\nrelevant code pieces 6; yet most of them rely on traditional rule-based methods\\nor provide limited functionalities. The closest prior research to our work is the\\nOCR-D project7, which also tries to build a complete toolkit for DIA. However,\\nsimilar to the platform developed by Neudecker et al. [21], it is designed for\\nanalyzing historical documents, and provides no supports for recent DL models.\\nThe DocumentLayoutAnalysis project8 focuses on processing born-digital PDF\\ndocuments via analyzing the stored PDF data. Repositories like DeepLayout9\\nand Detectron2-PubLayNet10 are individual deep learning models trained on\\nlayout analysis datasets without support for the full DIA pipeline. The Document\\nAnalysis and Exploitation (DAE) platform [15] and the DeepDIVA project [2]\\naim to improve the reproducibility of DIA methods (or DL models), yet they\\nare not actively maintained. OCR engines like Tesseract [14], easyOCR11 and\\npaddleOCR12 usually do not come with comprehensive functionalities for other\\nDIA tasks like layout analysis.\\nRecent years have also seen numerous e\ufb00orts to create libraries for promoting\\nreproducibility and reusability in the \ufb01eld of DL. Libraries like Dectectron2 [35],\\n6 The number shown is obtained by specifying the search type as \u2018code\u2019.\\n7 https://ocr-d.de/en/about\\n8 https://github.com/BobLd/DocumentLayoutAnalysis\\n9 https://github.com/leonlulu/DeepLayout\\n10 https://github.com/hpanwar08/detectron2\\n11 https://github.com/JaidedAI/EasyOCR\\n12 https://github.com/PaddlePaddle/PaddleOCR\\n4\\nZ. Shen et al.\\nFig. 1: The overall architecture of LayoutParser. For an input document image,\\nthe core LayoutParser library provides a set of o\ufb00-the-shelf tools for layout\\ndetection, OCR, visualization, and storage, backed by a carefully designed layout\\ndata structure. LayoutParser also supports high level customization via e\ufb03cient\\nlayout annotation and model training functions. These improve model accuracy\\non the target samples. The community platform enables the easy sharing of DIA\\nmodels and whole digitization pipelines to promote reusability and reproducibility.\\nA collection of detailed documentation, tutorials and exemplar projects make\\nLayoutParser easy to learn and use.\\nAllenNLP [8] and transformers [34] have provided the community with complete\\nDL-based support for developing and deploying models for general computer\\nvision and natural language processing problems. LayoutParser, on the other\\nhand, specializes speci\ufb01cally in DIA tasks. LayoutParser is also equipped with a\\ncommunity platform inspired by established model hubs such as Torch Hub [23]\\nand TensorFlow Hub [1]. It enables the sharing of pretrained models as well as\\nfull document processing pipelines that are unique to DIA tasks.\\nThere have been a variety of document data collections to facilitate the\\ndevelopment of DL models. Some examples include PRImA [3](magazine layouts),\\nPubLayNet [38](academic paper layouts), Table Bank [18](tables in academic\\npapers), Newspaper Navigator Dataset [16, 17](newspaper \ufb01gure layouts) and\\nHJDataset [31](historical Japanese document layouts). A spectrum of models\\ntrained on these datasets are currently available in the LayoutParser model zoo\\nto support di\ufb00erent use cases.\\n', metadata={'heading': '2 Related Work\\n', 'content_font': 9, 'heading_font': 11, 'source': 'example_data/layout-parser-paper.pdf'})\n"))),(0,o.kt)("h2",{id:"using-pymupdf"},"Using PyMuPDF"),(0,o.kt)("p",null,"This is the fastest of the PDF parsing options, and contains detailed metadata about the PDF and its pages, as well as returns one document per page."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders import PyMuPDFLoader\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = PyMuPDFLoader("example_data/layout-parser-paper.pdf")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data = loader.load()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data[0]\n")),(0,o.kt)(i,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Document(page_content='LayoutParser: A Uni\ufb01ed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\ufffd), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model con\ufb01gurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\ne\ufb00orts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: Document Image Analysis \xb7 Deep Learning \xb7 Layout Analysis\\n\xb7 Character Recognition \xb7 Open Source library \xb7 Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classi\ufb01cation [11,\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\n', lookup_str='', metadata={'file_path': 'example_data/layout-parser-paper.pdf', 'page_number': 1, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': '', 'encryption': None}, lookup_index=0)\n"))),(0,o.kt)("p",null,"Additionally, you can pass along any of the options from the ",(0,o.kt)("a",{parentName:"p",href:"https://pymupdf.readthedocs.io/en/latest/app1.html#plain-text/"},"PyMuPDF documentation")," as keyword arguments in the ",(0,o.kt)("inlineCode",{parentName:"p"},"load")," call, and it will be pass along to the ",(0,o.kt)("inlineCode",{parentName:"p"},"get_text()")," call."),(0,o.kt)("h2",{id:"pypdf-directory"},"PyPDF Directory"),(0,o.kt)("p",null,"Load PDFs from directory"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders import PyPDFDirectoryLoader\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = PyPDFDirectoryLoader("example_data/")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"docs = loader.load()\n")),(0,o.kt)("h2",{id:"using-pdfplumber"},"Using PDFPlumber"),(0,o.kt)("p",null,"Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders import PDFPlumberLoader\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = PDFPlumberLoader("example_data/layout-parser-paper.pdf")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data = loader.load()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"data[0]\n")),(0,o.kt)(i,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Document(page_content='LayoutParser: A Unified Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 ((cid:0)), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\n1202 shannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\nnuJ {melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n12 5 University of Waterloo\\nw422li@uwaterloo.ca\\n]VC.sc[\\nAbstract. Recentadvancesindocumentimageanalysis(DIA)havebeen\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomescouldbeeasilydeployedinproductionandextendedforfurther\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model configurations complicate the easy reuse of im-\\n2v84351.3012:viXra portantinnovationsbyawideaudience.Thoughtherehavebeenon-going\\nefforts to improve reusability and simplify deep learning (DL) model\\ndevelopmentindisciplineslikenaturallanguageprocessingandcomputer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademicresearchacross awiderangeof disciplinesinthesocialsciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitiveinterfacesforapplyingandcustomizingDLmodelsforlayoutde-\\ntection,characterrecognition,andmanyotherdocumentprocessingtasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: DocumentImageAnalysis\xb7DeepLearning\xb7LayoutAnalysis\\n\xb7 Character Recognition \xb7 Open Source library \xb7 Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocumentimageanalysis(DIA)tasksincludingdocumentimageclassification[11,', metadata={'source': 'example_data/layout-parser-paper.pdf', 'file_path': 'example_data/layout-parser-paper.pdf', 'page': 1, 'total_pages': 16, 'Author': '', 'CreationDate': 'D:20210622012710Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210622012710Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'})\n"))),(0,o.kt)("h2",{id:"using-amazontextractpdfparser"},"Using AmazonTextractPDFParser"),(0,o.kt)("p",null,"The AmazonTextractPDFLoader calls the ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/textract/"},"Amazon Textract Service")," to convert PDFs into a Document structure. The loader does pure OCR at the moment, with more features like layout support planned, depending on demand.  Single and multi-page documents are supported with up to 3000 pages and 512 MB of size."),(0,o.kt)("p",null,"For the call to be successful an AWS account is required, similar to the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html"},"AWS CLI")," requirements."),(0,o.kt)("p",null,"Besides the AWS configuration, it is very similar to the other PDF loaders, while also supporting JPEG, PNG and TIFF and non-native PDF formats."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.document_loaders import AmazonTextractPDFLoader\nloader = AmazonTextractPDFLoader("example_data/alejandro_rosalez_sample-small.jpeg")\ndocuments = loader.load()\n')))}d.isMDXComponent=!0;const c={},p="PDF",u={unversionedId:"modules/data_connection/document_loaders/pdf",id:"modules/data_connection/document_loaders/pdf",title:"PDF",description:"Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.",source:"@site/docs/modules/data_connection/document_loaders/pdf.mdx",sourceDirName:"modules/data_connection/document_loaders",slug:"/modules/data_connection/document_loaders/pdf",permalink:"/langchain/docs/modules/data_connection/document_loaders/pdf",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Markdown",permalink:"/langchain/docs/modules/data_connection/document_loaders/markdown"},next:{title:"Document transformers",permalink:"/langchain/docs/modules/data_connection/document_transformers/"}},m={},h=[],g={toc:h},f="wrapper";function y(e){let{components:n,...t}=e;return(0,o.kt)(f,(0,a.Z)({},g,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"pdf"},"PDF"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/PDF"},"Portable Document Format (PDF)"),", standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.")),(0,o.kt)("p",null,"This covers how to load ",(0,o.kt)("inlineCode",{parentName:"p"},"PDF")," documents into the Document format that we use downstream."),(0,o.kt)(d,{mdxType:"Example"}))}y.isMDXComponent=!0}}]);