"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[55965],{3905:(e,n,t)=>{t.d(n,{Zo:()=>s,kt:()=>h});var a=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var g=a.createContext({}),p=function(e){var n=a.useContext(g),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},s=function(e){var n=p(e.components);return a.createElement(g.Provider,{value:n},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,o=e.mdxType,r=e.originalType,g=e.parentName,s=i(e,["components","mdxType","originalType","parentName"]),c=p(t),u=o,h=c["".concat(g,".").concat(u)]||c[u]||m[u]||r;return t?a.createElement(h,l(l({ref:n},s),{},{components:t})):a.createElement(h,l({ref:n},s))}));function h(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var r=t.length,l=new Array(r);l[0]=u;var i={};for(var g in n)hasOwnProperty.call(n,g)&&(i[g]=n[g]);i.originalType=e,i[c]="string"==typeof e?e:o,l[1]=i;for(var p=2;p<r;p++)l[p]=t[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},2819:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>g,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>p});var a=t(87462),o=(t(67294),t(3905));const r={},l="Log10",i={unversionedId:"integrations/providers/log10",id:"integrations/providers/log10",title:"Log10",description:"This page covers how to use the Log10 within LangChain.",source:"@site/docs/integrations/providers/log10.mdx",sourceDirName:"integrations/providers",slug:"/integrations/providers/log10",permalink:"/langchain/docs/integrations/providers/log10",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Llama.cpp",permalink:"/langchain/docs/integrations/providers/llamacpp"},next:{title:"Marqo",permalink:"/langchain/docs/integrations/providers/marqo"}},g={},p=[{value:"What is Log10?",id:"what-is-log10",level:2},{value:"Quick start",id:"quick-start",level:2},{value:"How to enable Log10 data management for Langchain",id:"how-to-enable-log10-data-management-for-langchain",level:2},{value:"How to use tags with Log10",id:"how-to-use-tags-with-log10",level:2},{value:"How to debug Langchain calls",id:"how-to-debug-langchain-calls",level:2}],s={toc:p},c="wrapper";function m(e){let{components:n,...t}=e;return(0,o.kt)(c,(0,a.Z)({},s,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"log10"},"Log10"),(0,o.kt)("p",null,"This page covers how to use the ",(0,o.kt)("a",{parentName:"p",href:"https://log10.io"},"Log10")," within LangChain."),(0,o.kt)("h2",{id:"what-is-log10"},"What is Log10?"),(0,o.kt)("p",null,"Log10 is an ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/log10-io/log10"},"open source")," proxiless LLM data management and application development platform that lets you log, debug and tag your Langchain calls."),(0,o.kt)("h2",{id:"quick-start"},"Quick start"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Create your free account at ",(0,o.kt)("a",{parentName:"li",href:"https://log10.io"},"log10.io")),(0,o.kt)("li",{parentName:"ol"},"Add your ",(0,o.kt)("inlineCode",{parentName:"li"},"LOG10_TOKEN")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"LOG10_ORG_ID")," from the Settings and Organization tabs respectively as environment variables."),(0,o.kt)("li",{parentName:"ol"},"Also add ",(0,o.kt)("inlineCode",{parentName:"li"},"LOG10_URL=https://log10.io")," and your usual LLM API key: for e.g. ",(0,o.kt)("inlineCode",{parentName:"li"},"OPENAI_API_KEY")," or ",(0,o.kt)("inlineCode",{parentName:"li"},"ANTHROPIC_API_KEY")," to your environment")),(0,o.kt)("h2",{id:"how-to-enable-log10-data-management-for-langchain"},"How to enable Log10 data management for Langchain"),(0,o.kt)("p",null,"Integration with log10 is a simple one-line ",(0,o.kt)("inlineCode",{parentName:"p"},"log10_callback")," integration as shown below:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nfrom log10.langchain import Log10Callback\nfrom log10.llm import Log10Config\n\nlog10_callback = Log10Callback(log10_config=Log10Config())\n\nmessages = [\n    HumanMessage(content="You are a ping pong machine"),\n    HumanMessage(content="Ping?"),\n]\n\nllm = ChatOpenAI(model_name="gpt-3.5-turbo", callbacks=[log10_callback])\n')),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/log10-io/log10/blob/main/logging.md#langchain-logger"},"Log10 + Langchain + Logs docs")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://log10.io/docs/logs"},"More details + screenshots")," including instructions for self-hosting logs"),(0,o.kt)("h2",{id:"how-to-use-tags-with-log10"},"How to use tags with Log10"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain import OpenAI\nfrom langchain.chat_models import ChatAnthropic\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nfrom log10.langchain import Log10Callback\nfrom log10.llm import Log10Config\n\nlog10_callback = Log10Callback(log10_config=Log10Config())\n\nmessages = [\n    HumanMessage(content="You are a ping pong machine"),\n    HumanMessage(content="Ping?"),\n]\n\nllm = ChatOpenAI(model_name="gpt-3.5-turbo", callbacks=[log10_callback], temperature=0.5, tags=["test"])\ncompletion = llm.predict_messages(messages, tags=["foobar"])\nprint(completion)\n\nllm = ChatAnthropic(model="claude-2", callbacks=[log10_callback], temperature=0.7, tags=["baz"])\nllm.predict_messages(messages)\nprint(completion)\n\nllm = OpenAI(model_name="text-davinci-003", callbacks=[log10_callback], temperature=0.5)\ncompletion = llm.predict("You are a ping pong machine.\\nPing?\\n")\nprint(completion)\n')),(0,o.kt)("p",null,"You can also intermix direct OpenAI calls and Langchain LLM calls:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import os\nfrom log10.load import log10, log10_session\nimport openai\nfrom langchain import OpenAI\n\nlog10(openai)\n\nwith log10_session(tags=["foo", "bar"]):\n    # Log a direct OpenAI call\n    response = openai.Completion.create(\n        model="text-ada-001",\n        prompt="Where is the Eiffel Tower?",\n        temperature=0,\n        max_tokens=1024,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n    print(response)\n\n    # Log a call via Langchain\n    llm = OpenAI(model_name="text-ada-001", temperature=0.5)\n    response = llm.predict("You are a ping pong machine.\\nPing?\\n")\n    print(response)\n')),(0,o.kt)("h2",{id:"how-to-debug-langchain-calls"},"How to debug Langchain calls"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://log10.io/docs/prompt_chain_debugging"},"Example of debugging")),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/log10-io/log10/tree/main/examples#langchain"},"More Langchain examples")))}m.isMDXComponent=!0}}]);