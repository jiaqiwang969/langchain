"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[92602],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=o.createContext({}),l=function(e){var t=o.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=l(e.components);return o.createElement(c.Provider,{value:t},e.children)},h="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),h=l(n),u=a,d=h["".concat(c,".").concat(u)]||h[u]||m[u]||r;return n?o.createElement(d,i(i({ref:t},p),{},{components:n})):o.createElement(d,i({ref:t},p))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=u;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[h]="string"==typeof e?e:a,i[1]=s;for(var l=2;l<r;l++)i[l]=n[l];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}u.displayName="MDXCreateElement"},51252:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var o=n(87462),a=(n(67294),n(3905));const r={},i="Vectara Text Generation",s={unversionedId:"integrations/providers/vectara/vectara_text_generation",id:"integrations/providers/vectara/vectara_text_generation",title:"Vectara Text Generation",description:"This notebook is based on text generation notebook and adapted to Vectara.",source:"@site/docs/integrations/providers/vectara/vectara_text_generation.md",sourceDirName:"integrations/providers/vectara",slug:"/integrations/providers/vectara/vectara_text_generation",permalink:"/langchain/docs/integrations/providers/vectara/vectara_text_generation",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Chat Over Documents with Vectara",permalink:"/langchain/docs/integrations/providers/vectara/vectara_chat"},next:{title:"Vespa",permalink:"/langchain/docs/integrations/providers/vespa"}},c={},l=[{value:"Prepare Data",id:"prepare-data",level:2},{value:"Set Up Vector DB",id:"set-up-vector-db",level:2},{value:"Set Up LLM Chain with Custom Prompt",id:"set-up-llm-chain-with-custom-prompt",level:2},{value:"Generate Text",id:"generate-text",level:2}],p=(h="CodeOutputBlock",function(e){return console.warn("Component "+h+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var h;const m={toc:l},u="wrapper";function d(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,o.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"vectara-text-generation"},"Vectara Text Generation"),(0,a.kt)("p",null,"This notebook is based on ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/hwchase17/langchain/blob/master/docs/modules/chains/index_examples/vector_db_text_generation.ipynb"},"text generation")," notebook and adapted to Vectara."),(0,a.kt)("h2",{id:"prepare-data"},"Prepare Data"),(0,a.kt)("p",null,"First, we prepare the data. For this example, we fetch a documentation site that consists of markdown files hosted on Github and split them into small enough Documents."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "OpenAI", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html", "title": "Vectara Text Generation"}, {"imported": "Document", "source": "langchain.docstore.document", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.document.Document.html", "title": "Vectara Text Generation"}, {"imported": "Vectara", "source": "langchain.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.vectara.Vectara.html", "title": "Vectara Text Generation"}, {"imported": "CharacterTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html", "title": "Vectara Text Generation"}, {"imported": "PromptTemplate", "source": "langchain.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html", "title": "Vectara Text Generation"}]--\x3e\nimport os\nfrom langchain.llms import OpenAI\nfrom langchain.docstore.document import Document\nimport requests\nfrom langchain.vectorstores import Vectara\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nimport pathlib\nimport subprocess\nimport tempfile\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'def get_github_docs(repo_owner, repo_name):\n    with tempfile.TemporaryDirectory() as d:\n        subprocess.check_call(\n            f"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .",\n            cwd=d,\n            shell=True,\n        )\n        git_sha = (\n            subprocess.check_output("git rev-parse HEAD", shell=True, cwd=d)\n            .decode("utf-8")\n            .strip()\n        )\n        repo_path = pathlib.Path(d)\n        markdown_files = list(repo_path.glob("*/*.md")) + list(\n            repo_path.glob("*/*.mdx")\n        )\n        for markdown_file in markdown_files:\n            with open(markdown_file, "r") as f:\n                relative_path = markdown_file.relative_to(repo_path)\n                github_url = f"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}"\n                yield Document(page_content=f.read(), metadata={"source": github_url})\n\n\nsources = get_github_docs("yirenlu92", "deno-manual-forked")\n\nsource_chunks = []\nsplitter = CharacterTextSplitter(separator=" ", chunk_size=1024, chunk_overlap=0)\nfor source in sources:\n    for chunk in splitter.split_text(source.page_content):\n        source_chunks.append(chunk)\n')),(0,a.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    Cloning into '.'...\n"))),(0,a.kt)("h2",{id:"set-up-vector-db"},"Set Up Vector DB"),(0,a.kt)("p",null,"Now that we have the documentation content in chunks, let's put all this information in a vector index for easy retrieval."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"search_index = Vectara.from_texts(source_chunks, embedding=None)\n")),(0,a.kt)("h2",{id:"set-up-llm-chain-with-custom-prompt"},"Set Up LLM Chain with Custom Prompt"),(0,a.kt)("p",null,"Next, let's set up a simple LLM chain but give it a custom prompt for blog post generation. Note that the custom prompt is parameterized and takes two inputs: ",(0,a.kt)("inlineCode",{parentName:"p"},"context"),", which will be the documents fetched from the vector search, and ",(0,a.kt)("inlineCode",{parentName:"p"},"topic"),", which is given by the user."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "Vectara Text Generation"}]--\x3e\nfrom langchain.chains import LLMChain\n\nprompt_template = """Use the context below to write a 400 word blog post about the topic below:\n    Context: {context}\n    Topic: {topic}\n    Blog post:"""\n\nPROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "topic"])\n\nllm = OpenAI(openai_api_key=os.environ["OPENAI_API_KEY"], temperature=0)\n\nchain = LLMChain(llm=llm, prompt=PROMPT)\n')),(0,a.kt)("h2",{id:"generate-text"},"Generate Text"),(0,a.kt)("p",null,"Finally, we write a function to apply our inputs to the chain. The function takes an input parameter ",(0,a.kt)("inlineCode",{parentName:"p"},"topic"),". We find the documents in the vector index that correspond to that ",(0,a.kt)("inlineCode",{parentName:"p"},"topic"),", and use them as additional context in our simple LLM chain."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'def generate_blog_post(topic):\n    docs = search_index.similarity_search(topic, k=4)\n    inputs = [{"context": doc.page_content, "topic": topic} for doc in docs]\n    print(chain.apply(inputs))\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'generate_blog_post("environment variables")\n')),(0,a.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'    [{\'text\': \'\\n\\nWhen it comes to running Deno CLI tasks, environment variables can be a powerful tool for customizing the behavior of your tasks. With the Deno Task Definition interface, you can easily configure environment variables to be set when executing your tasks.\\n\\nThe Deno Task Definition interface is configured in a `tasks.json` within your workspace. It includes a `env` field, which allows you to specify any environment variables that should be set when executing the task. For example, if you wanted to set the `NODE_ENV` environment variable to `production` when running a Deno task, you could add the following to your `tasks.json`:\\n\\n```json\\n{\\n "version": "2.0.0",\\n "tasks": [\\n {\\n "type": "deno",\\n "command": "run",\\n "args": [\\n "mod.ts"\\n ],\\n "env": {\\n "NODE_ENV": "production"\\n },\\n "problemMatcher": [\\n "$deno"\\n ],\\n "label": "deno: run"\\n }\\n ]\\n}\\n```\\n\\nThe Deno language server and this extension also\'}, {\'text\': \'\\n\\nEnvironment variables are a great way to store and access data in your applications. They are especially useful when you need to store sensitive information such as API keys, passwords, and other credentials.\\n\\nDeno.env is a library that provides getter and setter methods for environment variables. This makes it easy to store and retrieve data from environment variables. For example, you can use the setter method to set a variable like this:\\n\\n```ts\\nDeno.env.set("FIREBASE_API_KEY", "examplekey123");\\nDeno.env.set("FIREBASE_AUTH_DOMAIN", "firebasedomain.com");\\n```\\n\\nAnd then you can use the getter method to retrieve the data like this:\\n\\n```ts\\nconsole.log(Deno.env.get("FIREBASE_API_KEY")); // examplekey123\\nconsole.log(Deno.env.get("FIREBASE_AUTH_DOMAIN")); // firebasedomain.com\\n```\\n\\nYou can also store environment variables in a `.env` file and retrieve them using `dotenv` in the standard\'}, {\'text\': \'\\n\\nEnvironment variables are a powerful tool for developers, allowing them to store and access data without hard-coding it into their applications. Deno, the secure JavaScript and TypeScript runtime, offers built-in support for environment variables with the `Deno.env` API.\\n\\nUsing `Deno.env` is simple. It has getter and setter methods that allow you to easily set and retrieve environment variables. For example, you can set the `FIREBASE_API_KEY` and `FIREBASE_AUTH_DOMAIN` environment variables like this:\\n\\n```ts\\nDeno.env.set("FIREBASE_API_KEY", "examplekey123");\\nDeno.env.set("FIREBASE_AUTH_DOMAIN", "firebasedomain.com");\\n```\\n\\nAnd then you can retrieve them like this:\\n\\n```ts\\nconsole.log(Deno.env.get("FIREBASE_API_KEY")); // examplekey123\\nconsole.log(Deno.env.get("FIREBASE_AUTH_DOMAIN")); // firebasedomain.com\\n```\'}, {\'text\': \'\\n\\nEnvironment variables are an important part of any programming language, and Deno is no exception. Environment variables are used to store information about the environment in which a program is running, such as the operating system, user preferences, and other settings. In Deno, environment variables are used to set up proxies, control the output of colors, and more.\\n\\nThe `NO_PROXY` environment variable is a de facto standard in Deno that indicates which hosts should bypass the proxy set in other environment variables. This is useful for developers who want to access certain resources without having to go through a proxy. For more information on this standard, you can check out the website no-color.org.\\n\\nThe `Deno.noColor` environment variable is another important environment variable in Deno. This variable is used to control the output of colors in the Deno terminal. By setting this variable to true, you can disable the output of colors in the terminal. This can be useful for developers who want to focus on the output of their code without being distracted by the colors.\\n\\nFinally, the `Deno.env` environment variable is used to access the environment variables set in the Deno runtime. This variable is useful for developers who want\'}]\n'))))}d.isMDXComponent=!0}}]);