"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[80709],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var r=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=r.createContext({}),l=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,c=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=l(n),m=o,h=p["".concat(c,".").concat(m)]||p[m]||d[m]||a;return n?r.createElement(h,i(i({ref:t},u),{},{components:n})):r.createElement(h,i({ref:t},u))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=m;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[p]="string"==typeof e?e:o,i[1]=s;for(var l=2;l<a;l++)i[l]=n[l];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},85831:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var r=n(87462),o=(n(67294),n(3905));const a={},i="MultiVector Retriever",s={unversionedId:"modules/data_connection/retrievers/multi_vector",id:"modules/data_connection/retrievers/multi_vector",title:"MultiVector Retriever",description:"It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base MultiVectorRetriever which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the MultiVectorRetriever.",source:"@site/docs/modules/data_connection/retrievers/multi_vector.md",sourceDirName:"modules/data_connection/retrievers",slug:"/modules/data_connection/retrievers/multi_vector",permalink:"/langchain/docs/modules/data_connection/retrievers/multi_vector",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Ensemble Retriever",permalink:"/langchain/docs/modules/data_connection/retrievers/ensemble"},next:{title:"Parent Document Retriever",permalink:"/langchain/docs/modules/data_connection/retrievers/parent_document_retriever"}},c={},l=[{value:"Smaller chunks",id:"smaller-chunks",level:2},{value:"Summary",id:"summary",level:2},{value:"Hypothetical Queries",id:"hypothetical-queries",level:2}],u=(p="CodeOutputBlock",function(e){return console.warn("Component "+p+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var p;const d={toc:l},m="wrapper";function h(e){let{components:t,...n}=e;return(0,o.kt)(m,(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"multivector-retriever"},"MultiVector Retriever"),(0,o.kt)("p",null,"It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base ",(0,o.kt)("inlineCode",{parentName:"p"},"MultiVectorRetriever")," which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the ",(0,o.kt)("inlineCode",{parentName:"p"},"MultiVectorRetriever"),"."),(0,o.kt)("p",null,"The methods to create multiple vectors per document include:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever)."),(0,o.kt)("li",{parentName:"ul"},"Summary: create a summary for each document, embed that along with (or instead of) the document."),(0,o.kt)("li",{parentName:"ul"},"Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.")),(0,o.kt)("p",null,"Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "MultiVectorRetriever", "source": "langchain.retrievers.multi_vector", "docs": "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html", "title": "MultiVector Retriever"}]--\x3e\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "Chroma", "source": "langchain.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html", "title": "MultiVector Retriever"}, {"imported": "OpenAIEmbeddings", "source": "langchain.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html", "title": "MultiVector Retriever"}, {"imported": "RecursiveCharacterTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html", "title": "MultiVector Retriever"}, {"imported": "InMemoryStore", "source": "langchain.storage", "docs": "https://api.python.langchain.com/en/latest/storage/langchain.storage.in_memory.InMemoryStore.html", "title": "MultiVector Retriever"}, {"imported": "TextLoader", "source": "langchain.document_loaders", "docs": "https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html", "title": "MultiVector Retriever"}]--\x3e\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.document_loaders import TextLoader\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"loaders = [\n    TextLoader('../../paul_graham_essay.txt'),\n    TextLoader('../../state_of_the_union.txt'),\n]\ndocs = []\nfor l in loaders:\n    docs.extend(l.load())\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\ndocs = text_splitter.split_documents(docs)\n")),(0,o.kt)("h2",{id:"smaller-chunks"},"Smaller chunks"),(0,o.kt)("p",null,"Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks. This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the ",(0,o.kt)("inlineCode",{parentName:"p"},"ParentDocumentRetriever")," does. Here we show what is going on under the hood."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# The vectorstore to use to index the child chunks\nvectorstore = Chroma(\n    collection_name="full_documents",\n    embedding_function=OpenAIEmbeddings()\n)\n# The storage layer for the parent documents\nstore = InMemoryStore()\nid_key = "doc_id"\n# The retriever (empty to start)\nretriever = MultiVectorRetriever(\n    vectorstore=vectorstore, \n    docstore=store, \n    id_key=id_key,\n)\nimport uuid\ndoc_ids = [str(uuid.uuid4()) for _ in docs]\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# The splitter to use to create smaller chunks\nchild_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"sub_docs = []\nfor i, doc in enumerate(docs):\n    _id = doc_ids[i]\n    _sub_docs = child_text_splitter.split_documents([doc])\n    for _doc in _sub_docs:\n        _doc.metadata[id_key] = _id\n    sub_docs.extend(_sub_docs)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"retriever.vectorstore.add_documents(sub_docs)\nretriever.docstore.mset(list(zip(doc_ids, docs)))\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# Vectorstore alone retrieves the small chunks\nretriever.vectorstore.similarity_search("justice breyer")[0]\n')),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Document(page_content='Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.', metadata={'doc_id': '10e9cbc0-4ba5-4d79-a09b-c033d1ba7b01', 'source': '../../state_of_the_union.txt'})\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# Retriever returns larger chunks\nlen(retriever.get_relevant_documents("justice breyer")[0].page_content)\n')),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    9874\n"))),(0,o.kt)("h2",{id:"summary"},"Summary"),(0,o.kt)("p",null,"Oftentimes a summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html", "title": "MultiVector Retriever"}, {"imported": "ChatPromptTemplate", "source": "langchain.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html", "title": "MultiVector Retriever"}, {"imported": "StrOutputParser", "source": "langchain.schema.output_parser", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.output_parser.StrOutputParser.html", "title": "MultiVector Retriever"}, {"imported": "Document", "source": "langchain.schema.document", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.document.Document.html", "title": "MultiVector Retriever"}]--\x3e\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nimport uuid\nfrom langchain.schema.document import Document\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'chain = (\n    {"doc": lambda x: x.page_content}\n    | ChatPromptTemplate.from_template("Summarize the following document:\\n\\n{doc}")\n    | ChatOpenAI(max_retries=0)\n    | StrOutputParser()\n)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'summaries = chain.batch(docs, {"max_concurrency": 5})\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# The vectorstore to use to index the child chunks\nvectorstore = Chroma(\n    collection_name="summaries",\n    embedding_function=OpenAIEmbeddings()\n)\n# The storage layer for the parent documents\nstore = InMemoryStore()\nid_key = "doc_id"\n# The retriever (empty to start)\nretriever = MultiVectorRetriever(\n    vectorstore=vectorstore, \n    docstore=store, \n    id_key=id_key,\n)\ndoc_ids = [str(uuid.uuid4()) for _ in docs]\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"summary_docs = [Document(page_content=s,metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries)]\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"retriever.vectorstore.add_documents(summary_docs)\nretriever.docstore.mset(list(zip(doc_ids, docs)))\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# # We can also add the original chunks to the vectorstore if we so want\n# for i, doc in enumerate(docs):\n#     doc.metadata[id_key] = doc_ids[i]\n# retriever.vectorstore.add_documents(docs)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'sub_docs = vectorstore.similarity_search("justice breyer")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"sub_docs[0]\n")),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Document(page_content=\"The document is a transcript of a speech given by the President of the United States. The President discusses several important issues and initiatives, including the nomination of a Supreme Court Justice, border security and immigration reform, protecting women's rights, advancing LGBTQ+ equality, bipartisan legislation, addressing the opioid epidemic and mental health, supporting veterans, investigating the health effects of burn pits on military personnel, ending cancer, and the strength and resilience of the American people.\", metadata={'doc_id': '79fa2e9f-28d9-4372-8af3-2caf4f1de312'})\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'retrieved_docs = retriever.get_relevant_documents("justice breyer")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"len(retrieved_docs[0].page_content)\n")),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    9194\n"))),(0,o.kt)("h2",{id:"hypothetical-queries"},"Hypothetical Queries"),(0,o.kt)("p",null,"An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document. These questions can then be embedded"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'functions = [\n    {\n      "name": "hypothetical_questions",\n      "description": "Generate hypothetical questions",\n      "parameters": {\n        "type": "object",\n        "properties": {\n          "questions": {\n            "type": "array",\n            "items": {\n                "type": "string"\n              },\n          },\n        },\n        "required": ["questions"]\n      }\n    }\n  ]\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "JsonKeyOutputFunctionsParser", "source": "langchain.output_parsers.openai_functions", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.openai_functions.JsonKeyOutputFunctionsParser.html", "title": "MultiVector Retriever"}]--\x3e\nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\nchain = (\n    {"doc": lambda x: x.page_content}\n    # Only asking for 3 hypothetical questions, but this could be adjusted\n    | ChatPromptTemplate.from_template("Generate a list of 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}")\n    | ChatOpenAI(max_retries=0, model="gpt-4").bind(functions=functions, function_call={"name": "hypothetical_questions"})\n    | JsonKeyOutputFunctionsParser(key_name="questions")\n)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"chain.invoke(docs[0])\n")),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    [\"What was the author's initial impression of philosophy as a field of study, and how did it change when they got to college?\",\n     'Why did the author decide to switch their focus to Artificial Intelligence (AI)?',\n     \"What led to the author's disillusionment with the field of AI as it was practiced at the time?\"]\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'hypothetical_questions = chain.batch(docs, {"max_concurrency": 5})\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# The vectorstore to use to index the child chunks\nvectorstore = Chroma(\n    collection_name="hypo-questions",\n    embedding_function=OpenAIEmbeddings()\n)\n# The storage layer for the parent documents\nstore = InMemoryStore()\nid_key = "doc_id"\n# The retriever (empty to start)\nretriever = MultiVectorRetriever(\n    vectorstore=vectorstore, \n    docstore=store, \n    id_key=id_key,\n)\ndoc_ids = [str(uuid.uuid4()) for _ in docs]\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"question_docs = []\nfor i, question_list in enumerate(hypothetical_questions):\n    question_docs.extend([Document(page_content=s,metadata={id_key: doc_ids[i]}) for s in question_list])\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"retriever.vectorstore.add_documents(question_docs)\nretriever.docstore.mset(list(zip(doc_ids, docs)))\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'sub_docs = vectorstore.similarity_search("justice breyer")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"sub_docs\n")),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    [Document(page_content=\"What is the President's stance on immigration reform?\", metadata={'doc_id': '505d73e3-8350-46ec-a58e-3af032f04ab3'}),\n     Document(page_content=\"What is the President's stance on immigration reform?\", metadata={'doc_id': '1c9618f0-7660-4b4f-a37c-509cbbbf6dba'}),\n     Document(page_content=\"What is the President's stance on immigration reform?\", metadata={'doc_id': '82c08209-b904-46a8-9532-edd2380950b7'}),\n     Document(page_content='What measures is the President proposing to protect the rights of LGBTQ+ Americans?', metadata={'doc_id': '82c08209-b904-46a8-9532-edd2380950b7'})]\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'retrieved_docs = retriever.get_relevant_documents("justice breyer")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"len(retrieved_docs[0].page_content)\n")),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    9194\n"))))}h.isMDXComponent=!0}}]);