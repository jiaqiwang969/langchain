"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[56600],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=o.createContext({}),p=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},c=function(e){var t=p(e.components);return o.createElement(s.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},m=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),d=p(n),m=a,h=d["".concat(s,".").concat(m)]||d[m]||u[m]||r;return n?o.createElement(h,l(l({ref:t},c),{},{components:n})):o.createElement(h,l({ref:t},c))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,l=new Array(r);l[0]=m;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[d]="string"==typeof e?e:a,l[1]=i;for(var p=2;p<r;p++)l[p]=n[p];return o.createElement.apply(null,l)}return o.createElement.apply(null,n)}m.displayName="MDXCreateElement"},57922:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>m,contentTitle:()=>d,default:()=>y,frontMatter:()=>c,metadata:()=>u,toc:()=>h});var o=n(87462),a=(n(67294),n(3905));const r=(l="CodeOutputBlock",function(e){return console.warn("Component "+l+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var l;const i={toc:[{value:"Setup",id:"setup",level:3},{value:"<code>__call__</code>: string in -&gt; string out",id:"__call__-string-in---string-out",level:3},{value:"<code>generate</code>: batch calls, richer outputs",id:"generate-batch-calls-richer-outputs",level:3}]},s="wrapper";function p(e){let{components:t,...n}=e;return(0,a.kt)(s,(0,o.Z)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h3",{id:"setup"},"Setup"),(0,a.kt)("p",null,"To start we'll need to install the OpenAI Python package:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"pip install openai\n")),(0,a.kt)("p",null,"Accessing the API requires an API key, which you can get by creating an account and heading ",(0,a.kt)("a",{parentName:"p",href:"https://platform.openai.com/account/api-keys"},"here"),". Once we have a key we'll want to set it as an environment variable by running:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'export OPENAI_API_KEY="..."\n')),(0,a.kt)("p",null,"If you'd prefer not to set an environment variable you can pass the key in directly via the ",(0,a.kt)("inlineCode",{parentName:"p"},"openai_api_key")," named parameter when initiating the OpenAI LLM class:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.llms import OpenAI\n\nllm = OpenAI(openai_api_key="...")\n')),(0,a.kt)("p",null,"otherwise you can initialize without any params:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.llms import OpenAI\n\nllm = OpenAI()\n")),(0,a.kt)("h3",{id:"__call__-string-in---string-out"},(0,a.kt)("inlineCode",{parentName:"h3"},"__call__"),": string in -> string out"),(0,a.kt)("p",null,"The simplest way to use an LLM is a callable: pass in a string, get a string completion."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'llm("Tell me a joke")\n')),(0,a.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    'Why did the chicken cross the road?\\n\\nTo get to the other side.'\n"))),(0,a.kt)("h3",{id:"generate-batch-calls-richer-outputs"},(0,a.kt)("inlineCode",{parentName:"h3"},"generate"),": batch calls, richer outputs"),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"generate")," lets you call the model with a list of strings, getting back a more complete response than just the text. This complete response can include things like multiple top responses and other LLM provider-specific information:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'llm_result = llm.generate(["Tell me a joke", "Tell me a poem"]*15)\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"len(llm_result.generations)\n")),(0,a.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    30\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"llm_result.generations[0]\n")),(0,a.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!'),\n     Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.')]\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"llm_result.generations[-1]\n")),(0,a.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    [Generation(text=\"\\n\\nWhat if love neverspeech\\n\\nWhat if love never ended\\n\\nWhat if love was only a feeling\\n\\nI'll never know this love\\n\\nIt's not a feeling\\n\\nBut it's what we have for each other\\n\\nWe just know that love is something strong\\n\\nAnd we can't help but be happy\\n\\nWe just feel what love is for us\\n\\nAnd we love each other with all our heart\\n\\nWe just don't know how\\n\\nHow it will go\\n\\nBut we know that love is something strong\\n\\nAnd we'll always have each other\\n\\nIn our lives.\"),\n     Generation(text='\\n\\nOnce upon a time\\n\\nThere was a love so pure and true\\n\\nIt lasted for centuries\\n\\nAnd never became stale or dry\\n\\nIt was moving and alive\\n\\nAnd the heart of the love-ick\\n\\nIs still beating strong and true.')]\n"))),(0,a.kt)("p",null,"You can also access provider specific information that is returned. This information is ",(0,a.kt)("strong",{parentName:"p"},"not")," standardized across providers."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"llm_result.llm_output\n")),(0,a.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    {'token_usage': {'completion_tokens': 3903,\n      'total_tokens': 4023,\n      'prompt_tokens': 120}}\n"))))}p.isMDXComponent=!0;const c={sidebar_position:0},d="LLMs",u={unversionedId:"modules/model_io/models/llms/index",id:"modules/model_io/models/llms/index",title:"LLMs",description:"Head to Integrations for documentation on built-in integrations with LLM providers.",source:"@site/docs/modules/model_io/models/llms/index.mdx",sourceDirName:"modules/model_io/models/llms",slug:"/modules/model_io/models/llms/",permalink:"/langchain/docs/modules/model_io/models/llms/",draft:!1,tags:[],version:"current",sidebarPosition:0,frontMatter:{sidebar_position:0},sidebar:"docs",previous:{title:"Language models",permalink:"/langchain/docs/modules/model_io/models/"},next:{title:"Async API",permalink:"/langchain/docs/modules/model_io/models/llms/async_llm"}},m={},h=[{value:"Get started",id:"get-started",level:2}],g={toc:h},k="wrapper";function y(e){let{components:t,...n}=e;return(0,a.kt)(k,(0,o.Z)({},g,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"llms"},"LLMs"),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"Head to ",(0,a.kt)("a",{parentName:"p",href:"/docs/integrations/llms/"},"Integrations")," for documentation on built-in integrations with LLM providers.")),(0,a.kt)("p",null,"Large Language Models (LLMs) are a core component of LangChain.\nLangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs."),(0,a.kt)("h2",{id:"get-started"},"Get started"),(0,a.kt)("p",null,"There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the ",(0,a.kt)("inlineCode",{parentName:"p"},"LLM")," class is designed to provide a standard interface for all of them."),(0,a.kt)("p",null,"In this walkthrough we'll work with an OpenAI LLM wrapper, although the functionalities highlighted are generic for all LLM types."),(0,a.kt)(p,{mdxType:"LLMGetStarted"}))}y.isMDXComponent=!0}}]);