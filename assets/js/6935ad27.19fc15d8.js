"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[62312],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>g});var a=n(67294);function p(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){p(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,p=function(e,t){if(null==e)return{};var n,a,p={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(p[n]=e[n]);return p}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(p[n]=e[n])}return p}var l=a.createContext({}),s=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},m=function(e){var t=s(e.components);return a.createElement(l.Provider,{value:t},e.children)},c="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,p=e.mdxType,o=e.originalType,l=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),c=s(n),u=p,g=c["".concat(l,".").concat(u)]||c[u]||h[u]||o;return n?a.createElement(g,r(r({ref:t},m),{},{components:n})):a.createElement(g,r({ref:t},m))}));function g(e,t){var n=arguments,p=t&&t.mdxType;if("string"==typeof e||p){var o=n.length,r=new Array(o);r[0]=u;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[c]="string"==typeof e?e:p,r[1]=i;for(var s=2;s<o;s++)r[s]=n[s];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},81350:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>i,toc:()=>s});var a=n(87462),p=(n(67294),n(3905));const o={},r="Prompt pipelining",i={unversionedId:"modules/model_io/prompts/prompt_templates/prompts_pipelining",id:"modules/model_io/prompts/prompt_templates/prompts_pipelining",title:"Prompt pipelining",description:"The idea behind prompt pipelining is to provide a user friendly interface for composing different parts of prompts together. You can do this with either string prompts or chat prompts. Constructing prompts this way allows for easy reuse of components.",source:"@site/docs/modules/model_io/prompts/prompt_templates/prompts_pipelining.md",sourceDirName:"modules/model_io/prompts/prompt_templates",slug:"/modules/model_io/prompts/prompt_templates/prompts_pipelining",permalink:"/langchain/docs/modules/model_io/prompts/prompt_templates/prompts_pipelining",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Serialization",permalink:"/langchain/docs/modules/model_io/prompts/prompt_templates/prompt_serialization"},next:{title:"Validate template",permalink:"/langchain/docs/modules/model_io/prompts/prompt_templates/validate"}},l={},s=[{value:"String prompt pipelining",id:"string-prompt-pipelining",level:2},{value:"Chat prompt pipelining",id:"chat-prompt-pipelining",level:2}],m=(c="CodeOutputBlock",function(e){return console.warn("Component "+c+" was not imported, exported, or provided by MDXProvider as global scope"),(0,p.kt)("div",e)});var c;const h={toc:s},u="wrapper";function g(e){let{components:t,...n}=e;return(0,p.kt)(u,(0,a.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,p.kt)("h1",{id:"prompt-pipelining"},"Prompt pipelining"),(0,p.kt)("p",null,"The idea behind prompt pipelining is to provide a user friendly interface for composing different parts of prompts together. You can do this with either string prompts or chat prompts. Constructing prompts this way allows for easy reuse of components."),(0,p.kt)("h2",{id:"string-prompt-pipelining"},"String prompt pipelining"),(0,p.kt)("p",null,"When working with string prompts, each template is joined togther. You can work with either prompts directly or strings (the first element in the list needs to be a prompt)."),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html", "title": "Prompt pipelining"}]--\x3e\nfrom langchain.prompts import PromptTemplate\n')),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'prompt = (\n    PromptTemplate.from_template("Tell me a joke about {topic}")\n    + ", make it funny"\n    + "\\n\\nand in {language}"\n)\n')),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},"prompt\n")),(0,p.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre"},"    PromptTemplate(input_variables=['language', 'topic'], output_parser=None, partial_variables={}, template='Tell me a joke about {topic}, make it funny\\n\\nand in {language}', template_format='f-string', validate_template=True)\n"))),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'prompt.format(topic="sports", language="spanish")\n')),(0,p.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre"},"    'Tell me a joke about sports, make it funny\\n\\nand in spanish'\n"))),(0,p.kt)("p",null,"You can also use it in an LLMChain, just like before."),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html", "title": "Prompt pipelining"}, {"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "Prompt pipelining"}]--\x3e\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\n')),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},"model = ChatOpenAI()\n")),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},"chain = LLMChain(llm=model, prompt=prompt)\n")),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'chain.run(topic="sports", language="spanish")\n')),(0,p.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre"},"    '\xbfPor qu\xe9 el futbolista llevaba un paraguas al partido?\\n\\nPorque pronosticaban lluvia de goles.'\n"))),(0,p.kt)("h2",{id:"chat-prompt-pipelining"},"Chat prompt pipelining"),(0,p.kt)("p",null,"A chat prompt is made up a of a list of messages. Purely for developer experience, we've added a convinient way to create these prompts. In this pipeline, each new element is a new message in the final prompt."),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html", "title": "Prompt pipelining"}, {"imported": "HumanMessagePromptTemplate", "source": "langchain.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.HumanMessagePromptTemplate.html", "title": "Prompt pipelining"}, {"imported": "HumanMessage", "source": "langchain.schema", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.HumanMessage.html", "title": "Prompt pipelining"}, {"imported": "AIMessage", "source": "langchain.schema", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.AIMessage.html", "title": "Prompt pipelining"}, {"imported": "SystemMessage", "source": "langchain.schema", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.SystemMessage.html", "title": "Prompt pipelining"}]--\x3e\nfrom langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\nfrom langchain.schema import HumanMessage, AIMessage, SystemMessage\n')),(0,p.kt)("p",null,"First, let's initialize the base ChatPromptTemplate with a system message. It doesn't have to start with a system, but it's often good practice"),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'prompt = SystemMessage(content="You are a nice pirate")\n')),(0,p.kt)("p",null,"You can then easily create a pipeline combining it with other messages ",(0,p.kt)("em",{parentName:"p"},"or")," message templates.\nUse a ",(0,p.kt)("inlineCode",{parentName:"p"},"Message")," when there is no variables to be formatted, use a ",(0,p.kt)("inlineCode",{parentName:"p"},"MessageTemplate")," when there are variables to be formatted. You can also use just a string (note: this will automatically get inferred as a HumanMessagePromptTemplate.)"),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'new_prompt = (\n    prompt\n    + HumanMessage(content="hi")\n    + AIMessage(content="what?")\n    + "{input}"\n)\n')),(0,p.kt)("p",null,"Under the hood, this creates an instance of the ChatPromptTemplate class, so you can use it just as you did before!"),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'new_prompt.format_messages(input="i said hi")\n')),(0,p.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre"},"    [SystemMessage(content='You are a nice pirate', additional_kwargs={}),\n     HumanMessage(content='hi', additional_kwargs={}, example=False),\n     AIMessage(content='what?', additional_kwargs={}, example=False),\n     HumanMessage(content='i said hi', additional_kwargs={}, example=False)]\n"))),(0,p.kt)("p",null,"You can also use it in an LLMChain, just like before."),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html", "title": "Prompt pipelining"}, {"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "Prompt pipelining"}]--\x3e\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\n')),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},"model = ChatOpenAI()\n")),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},"chain = LLMChain(llm=model, prompt=new_prompt)\n")),(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre",className:"language-python"},'chain.run("i said hi")\n')),(0,p.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,p.kt)("pre",null,(0,p.kt)("code",{parentName:"pre"},"    'Oh, hello! How can I assist you today?'\n"))))}g.isMDXComponent=!0}}]);