"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[25301],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>g});var a=t(67294);function l(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){l(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,l=function(e,n){if(null==e)return{};var t,a,l={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(l[t]=e[t]);return l}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(l[t]=e[t])}return l}var s=a.createContext({}),c=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(s.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,l=e.mdxType,r=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),m=c(t),d=l,g=m["".concat(s,".").concat(d)]||m[d]||u[d]||r;return t?a.createElement(g,o(o({ref:n},p),{},{components:t})):a.createElement(g,o({ref:n},p))}));function g(e,n){var t=arguments,l=n&&n.mdxType;if("string"==typeof e||l){var r=t.length,o=new Array(r);o[0]=d;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[m]="string"==typeof e?e:l,o[1]=i;for(var c=2;c<r;c++)o[c]=t[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},69894:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>g,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var a=t(87462),l=(t(67294),t(3905));const r={},o="Streaming final agent output",i={unversionedId:"modules/agents/how_to/streaming_stdout_final_only",id:"modules/agents/how_to/streaming_stdout_final_only",title:"Streaming final agent output",description:"If you only want the final output of an agent to be streamed, you can use the callback `FinalStreamingStdOutCallbackHandler`.",source:"@site/docs/modules/agents/how_to/streaming_stdout_final_only.md",sourceDirName:"modules/agents/how_to",slug:"/modules/agents/how_to/streaming_stdout_final_only",permalink:"/langchain/docs/modules/agents/how_to/streaming_stdout_final_only",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Shared memory across agents and tools",permalink:"/langchain/docs/modules/agents/how_to/sharedmemory_for_tools"},next:{title:"Use ToolKits with OpenAI Functions",permalink:"/langchain/docs/modules/agents/how_to/use_toolkits_with_openai_functions"}},s={},c=[{value:"Handling custom answer prefixes",id:"handling-custom-answer-prefixes",level:3},{value:"Also streaming the answer prefixes",id:"also-streaming-the-answer-prefixes",level:3}],p=(m="CodeOutputBlock",function(e){return console.warn("Component "+m+" was not imported, exported, or provided by MDXProvider as global scope"),(0,l.kt)("div",e)});var m;const u={toc:c},d="wrapper";function g(e){let{components:n,...t}=e;return(0,l.kt)(d,(0,a.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"streaming-final-agent-output"},"Streaming final agent output"),(0,l.kt)("p",null,"If you only want the final output of an agent to be streamed, you can use the callback ",(0,l.kt)("inlineCode",{parentName:"p"},"FinalStreamingStdOutCallbackHandler"),".\nFor this, the underlying LLM has to support streaming as well."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "load_tools", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html", "title": "Streaming final agent output"}, {"imported": "initialize_agent", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/agents/langchain.agents.initialize.initialize_agent.html", "title": "Streaming final agent output"}, {"imported": "AgentType", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html", "title": "Streaming final agent output"}, {"imported": "FinalStreamingStdOutCallbackHandler", "source": "langchain.callbacks.streaming_stdout_final_only", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler.html", "title": "Streaming final agent output"}, {"imported": "OpenAI", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html", "title": "Streaming final agent output"}]--\x3e\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.callbacks.streaming_stdout_final_only import (\n    FinalStreamingStdOutCallbackHandler,\n)\nfrom langchain.llms import OpenAI\n')),(0,l.kt)("p",null,"Let's create the underlying LLM with ",(0,l.kt)("inlineCode",{parentName:"p"},"streaming = True")," and pass a new instance of ",(0,l.kt)("inlineCode",{parentName:"p"},"FinalStreamingStdOutCallbackHandler"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"llm = OpenAI(\n    streaming=True, callbacks=[FinalStreamingStdOutCallbackHandler()], temperature=0\n)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'tools = load_tools(["wikipedia", "llm-math"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False\n)\nagent.run(\n    "It\'s 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany."\n)\n')),(0,l.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"     Konrad Adenauer became Chancellor of Germany in 1949, 74 years ago in 2023.\n\n\n\n\n    'Konrad Adenauer became Chancellor of Germany in 1949, 74 years ago in 2023.'\n"))),(0,l.kt)("h3",{id:"handling-custom-answer-prefixes"},"Handling custom answer prefixes"),(0,l.kt)("p",null,"By default, we assume that the token sequence ",(0,l.kt)("inlineCode",{parentName:"p"},'"Final", "Answer", ":"')," indicates that the agent has reached an answers. We can, however, also pass a custom sequence to use as answer prefix."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'llm = OpenAI(\n    streaming=True,\n    callbacks=[\n        FinalStreamingStdOutCallbackHandler(answer_prefix_tokens=["The", "answer", ":"])\n    ],\n    temperature=0,\n)\n')),(0,l.kt)("p",null,"For convenience, the callback automatically strips whitespaces and new line characters when comparing to ",(0,l.kt)("inlineCode",{parentName:"p"},"answer_prefix_tokens"),". I.e., if ",(0,l.kt)("inlineCode",{parentName:"p"},'answer_prefix_tokens = ["The", " answer", ":"]')," then both ",(0,l.kt)("inlineCode",{parentName:"p"},'["\\nThe", " answer", ":"]')," and ",(0,l.kt)("inlineCode",{parentName:"p"},'["The", " answer", ":"]')," would be recognized a the answer prefix."),(0,l.kt)("p",null,"If you don't know the tokenized version of your answer prefix, you can determine it with the following code:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "BaseCallbackHandler", "source": "langchain.callbacks.base", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.base.BaseCallbackHandler.html", "title": "Streaming final agent output"}]--\x3e\nfrom langchain.callbacks.base import BaseCallbackHandler\n\n\nclass MyCallbackHandler(BaseCallbackHandler):\n    def on_llm_new_token(self, token, **kwargs) -> None:\n        # print every token on a new line\n        print(f"#{token}#")\n\n\nllm = OpenAI(streaming=True, callbacks=[MyCallbackHandler()])\ntools = load_tools(["wikipedia", "llm-math"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False\n)\nagent.run(\n    "It\'s 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany."\n)\n')),(0,l.kt)("h3",{id:"also-streaming-the-answer-prefixes"},"Also streaming the answer prefixes"),(0,l.kt)("p",null,"When the parameter ",(0,l.kt)("inlineCode",{parentName:"p"},"stream_prefix = True")," is set, the answer prefix itself will also be streamed. This can be useful when the answer prefix itself is part of the answer. For example, when your answer is a JSON like"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},'{\n    "action": "Final answer",\n    "action_input": "Konrad Adenauer became Chancellor 74 years ago."\n}')),(0,l.kt)("p",null,"and you don't only want the ",(0,l.kt)("inlineCode",{parentName:"p"},"action_input")," to be streamed, but the entire JSON."))}g.isMDXComponent=!0}}]);