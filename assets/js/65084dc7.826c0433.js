"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[67545,27918],{85162:(e,t,a)=>{a.d(t,{Z:()=>r});var n=a(67294),o=a(86010);const s={tabItem:"tabItem_Ymn6"};function r(e){let{children:t,hidden:a,className:r}=e;return n.createElement("div",{role:"tabpanel",className:(0,o.Z)(s.tabItem,r),hidden:a},t)}},74866:(e,t,a)=>{a.d(t,{Z:()=>w});var n=a(87462),o=a(67294),s=a(86010),r=a(12466),l=a(16550),i=a(91980),p=a(67392),u=a(50012);function m(e){return function(e){return o.Children.map(e,(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:o}}=e;return{value:t,label:a,attributes:n,default:o}}))}function c(e){const{values:t,children:a}=e;return(0,o.useMemo)((()=>{const e=t??m(a);return function(e){const t=(0,p.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function d(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function h(e){let{queryString:t=!1,groupId:a}=e;const n=(0,l.k6)(),s=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,i._X)(s),(0,o.useCallback)((e=>{if(!s)return;const t=new URLSearchParams(n.location.search);t.set(s,e),n.replace({...n.location,search:t.toString()})}),[s,n])]}function g(e){const{defaultValue:t,queryString:a=!1,groupId:n}=e,s=c(e),[r,l]=(0,o.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!d({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:s}))),[i,p]=h({queryString:a,groupId:n}),[m,g]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[n,s]=(0,u.Nk)(a);return[n,(0,o.useCallback)((e=>{a&&s.set(e)}),[a,s])]}({groupId:n}),f=(()=>{const e=i??m;return d({value:e,tabValues:s})?e:null})();(0,o.useLayoutEffect)((()=>{f&&l(f)}),[f]);return{selectedValue:r,selectValue:(0,o.useCallback)((e=>{if(!d({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),g(e)}),[p,g,s]),tabValues:s}}var f=a(72389);const k={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function y(e){let{className:t,block:a,selectedValue:l,selectValue:i,tabValues:p}=e;const u=[],{blockElementScrollPositionUntilNextRender:m}=(0,r.o5)(),c=e=>{const t=e.currentTarget,a=u.indexOf(t),n=p[a].value;n!==l&&(m(t),i(n))},d=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const a=u.indexOf(e.currentTarget)+1;t=u[a]??u[0];break}case"ArrowLeft":{const a=u.indexOf(e.currentTarget)-1;t=u[a]??u[u.length-1];break}}t?.focus()};return o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.Z)("tabs",{"tabs--block":a},t)},p.map((e=>{let{value:t,label:a,attributes:r}=e;return o.createElement("li",(0,n.Z)({role:"tab",tabIndex:l===t?0:-1,"aria-selected":l===t,key:t,ref:e=>u.push(e),onKeyDown:d,onClick:c},r,{className:(0,s.Z)("tabs__item",k.tabItem,r?.className,{"tabs__item--active":l===t})}),a??t)})))}function b(e){let{lazy:t,children:a,selectedValue:n}=e;const s=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=s.find((e=>e.props.value===n));return e?(0,o.cloneElement)(e,{className:"margin-top--md"}):null}return o.createElement("div",{className:"margin-top--md"},s.map(((e,t)=>(0,o.cloneElement)(e,{key:t,hidden:e.props.value!==n}))))}function v(e){const t=g(e);return o.createElement("div",{className:(0,s.Z)("tabs-container",k.tabList)},o.createElement(y,(0,n.Z)({},e,t)),o.createElement(b,(0,n.Z)({},e,t)))}function w(e){const t=(0,f.Z)();return o.createElement(v,(0,n.Z)({key:String(t)},e))}},18573:(e,t,a)=>{a.d(t,{Z:()=>r});var n=a(67294),o=a(90814);function s(e){let{imports:t}=e;return n.createElement("div",{style:{paddingTop:"1.3rem",background:"var(--prism-background-color)",color:"var(--prism-color)",marginTop:"calc(-1 * var(--ifm-leading) - 5px)",marginBottom:"var(--ifm-leading)",boxShadow:"var(--ifm-global-shadow-lw)",borderBottomLeftRadius:"var(--ifm-code-border-radius)",borderBottomRightRadius:"var(--ifm-code-border-radius)"}},n.createElement("h4",{style:{paddingLeft:"0.65rem",marginBottom:"0.45rem"}},"API Reference:"),n.createElement("ul",{style:{paddingBottom:"1rem"}},t.map((e=>{let{imported:t,source:a,docs:o}=e;return n.createElement("li",{key:t},n.createElement("a",{href:o},n.createElement("span",null,t)))}))))}function r(e){let{children:t,...a}=e,r=[];if("string"==typeof t){const e=/<!--IMPORTS:(.*?)-->\n/.exec(t);e&&(r=JSON.parse(e[1]),t=t.replace(e[0],""))}else t.imports&&(r=t.imports);return n.createElement(n.Fragment,null,n.createElement(o.Z,a,t),r.length>0&&n.createElement(s,{imports:r}))}},9028:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>X,contentTitle:()=>Z,default:()=>j,frontMatter:()=>D,metadata:()=>q,toc:()=>V});var n=a(87462),o=(a(67294),a(3905)),s=a(74866),r=a(85162),l=a(18573);const i={toc:[]},p="wrapper";function u(e){let{components:t,...a}=e;return(0,o.kt)(p,(0,n.Z)({},i,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)(s.Z,{mdxType:"Tabs"},(0,o.kt)(r.Z,{value:"pip",label:"Pip",default:!0,mdxType:"TabItem"},(0,o.kt)(l.Z,{language:"bash",mdxType:"CodeBlock"},"pip install langchain")),(0,o.kt)(r.Z,{value:"conda",label:"Conda",mdxType:"TabItem"},(0,o.kt)(l.Z,{language:"bash",mdxType:"CodeBlock"},"conda install langchain -c conda-forge"))))}u.isMDXComponent=!0;const m={toc:[]},c="wrapper";function d(e){let{components:t,...a}=e;return(0,o.kt)(c,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"First we'll need to install their Python package:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install openai\n")),(0,o.kt)("p",null,"Accessing the API requires an API key, which you can get by creating an account and heading ",(0,o.kt)("a",{parentName:"p",href:"https://platform.openai.com/account/api-keys"},"here"),". Once we have a key we'll want to set it as an environment variable by running:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'export OPENAI_API_KEY="..."\n')),(0,o.kt)("p",null,"If you'd prefer not to set an environment variable you can pass the key in directly via the ",(0,o.kt)("inlineCode",{parentName:"p"},"openai_api_key")," named parameter when initiating the OpenAI LLM class:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.llms import OpenAI\n\nllm = OpenAI(openai_api_key="...")\n')))}d.isMDXComponent=!0;const h={toc:[]},g="wrapper";function f(e){let{components:t,...a}=e;return(0,o.kt)(g,(0,n.Z)({},h,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\n\nllm = OpenAI()\nchat_model = ChatOpenAI()\n\nllm.predict("hi!")\n>>> "Hi"\n\nchat_model.predict("hi!")\n>>> "Hi"\n')))}f.isMDXComponent=!0;const k={toc:[]},y="wrapper";function b(e){let{components:t,...a}=e;return(0,o.kt)(y,(0,n.Z)({},k,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'text = "What would be a good company name for a company that makes colorful socks?"\n\nllm.predict(text)\n# >> Feetful of Fun\n\nchat_model.predict(text)\n# >> Socks O\'Color\n')))}b.isMDXComponent=!0;const v={toc:[]},w="wrapper";function C(e){let{components:t,...a}=e;return(0,o.kt)(w,(0,n.Z)({},v,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.schema import HumanMessage\n\ntext = "What would be a good company name for a company that makes colorful socks?"\nmessages = [HumanMessage(content=text)]\n\nllm.predict_messages(messages)\n# >> Feetful of Fun\n\nchat_model.predict_messages(messages)\n# >> Socks O\'Color\n')))}C.isMDXComponent=!0;const L={toc:[]},N="wrapper";function M(e){let{components:t,...a}=e;return(0,o.kt)(N,(0,n.Z)({},L,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")\nprompt.format(product="colorful socks")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},"What is a good name for a company that makes colorful socks?\n")))}M.isMDXComponent=!0;const T={toc:[]},_="wrapper";function x(e){let{components:t,...a}=e;return(0,o.kt)(_,(0,n.Z)({},T,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\ntemplate = "You are a helpful assistant that translates {input_language} to {output_language}."\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = "{text}"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n\nchat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-pycon"},'[\n    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),\n    HumanMessage(content="I love programming.")\n]\n')))}x.isMDXComponent=!0;const I={toc:[]},P="wrapper";function E(e){let{components:t,...a}=e;return(0,o.kt)(P,(0,n.Z)({},I,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.schema import BaseOutputParser\n\nclass CommaSeparatedListOutputParser(BaseOutputParser):\n    """Parse the output of an LLM call to a comma-separated list."""\n\n\n    def parse(self, text: str):\n        """Parse the output of an LLM call."""\n        return text.strip().split(", ")\n\nCommaSeparatedListOutputParser().parse("hi, bye")\n# >> [\'hi\', \'bye\']\n')))}E.isMDXComponent=!0;const O={toc:[]},A="wrapper";function S(e){let{components:t,...a}=e;return(0,o.kt)(A,(0,n.Z)({},O,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.chains import LLMChain\nfrom langchain.schema import BaseOutputParser\n\nclass CommaSeparatedListOutputParser(BaseOutputParser):\n    """Parse the output of an LLM call to a comma-separated list."""\n\n\n    def parse(self, text: str):\n        """Parse the output of an LLM call."""\n        return text.strip().split(", ")\n\ntemplate = """You are a helpful assistant who generates comma separated lists.\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\nONLY return a comma separated list, and nothing more."""\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = "{text}"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\nchain = LLMChain(\n    llm=ChatOpenAI(),\n    prompt=chat_prompt,\n    output_parser=CommaSeparatedListOutputParser()\n)\nchain.run("colors")\n# >> [\'red\', \'blue\', \'green\', \'yellow\', \'orange\']\n')))}S.isMDXComponent=!0;const D={},Z="Quickstart",q={unversionedId:"get_started/quickstart",id:"get_started/quickstart",title:"Quickstart",description:"Installation",source:"@site/docs/get_started/quickstart.mdx",sourceDirName:"get_started",slug:"/get_started/quickstart",permalink:"/langchain/docs/get_started/quickstart",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Installation",permalink:"/langchain/docs/get_started/installation"},next:{title:"Modules",permalink:"/langchain/docs/modules/"}},X={},V=[{value:"Installation",id:"installation",level:2},{value:"Environment setup",id:"environment-setup",level:2},{value:"Building an application",id:"building-an-application",level:2},{value:"LLMs",id:"llms",level:2},{value:"Prompt templates",id:"prompt-templates",level:2},{value:"Output parsers",id:"output-parsers",level:2},{value:"LLMChain",id:"llmchain",level:2},{value:"Next steps",id:"next-steps",level:2}],F={toc:V},B="wrapper";function j(e){let{components:t,...a}=e;return(0,o.kt)(B,(0,n.Z)({},F,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"quickstart"},"Quickstart"),(0,o.kt)("h2",{id:"installation"},"Installation"),(0,o.kt)("p",null,"To install LangChain run:"),(0,o.kt)(u,{mdxType:"Install"}),(0,o.kt)("p",null,"For more details, see our ",(0,o.kt)("a",{parentName:"p",href:"/docs/get_started/installation.html"},"Installation guide"),"."),(0,o.kt)("h2",{id:"environment-setup"},"Environment setup"),(0,o.kt)("p",null,"Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs."),(0,o.kt)(d,{mdxType:"OpenAISetup"}),(0,o.kt)("h2",{id:"building-an-application"},"Building an application"),(0,o.kt)("p",null,"Now we can start building our language model application. LangChain provides many modules that can be used to build language model applications.\nModules can be used as stand-alones in simple applications and they can be combined for more complex use cases."),(0,o.kt)("p",null,"The core building block of LangChain applications is the LLMChain.\nThis combines three things:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"LLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them."),(0,o.kt)("li",{parentName:"ul"},"Prompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial."),(0,o.kt)("li",{parentName:"ul"},"Output Parsers: These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream.")),(0,o.kt)("p",null,"In this getting started guide we will cover those three components by themselves, and then cover the LLMChain which combines all of them.\nUnderstanding these concepts will set you up well for being able to use and customize LangChain applications.\nMost LangChain applications allow you to configure the LLM and/or the prompt used, so knowing how to take advantage of this will be a big enabler."),(0,o.kt)("h2",{id:"llms"},"LLMs"),(0,o.kt)("p",null,"There are two types of language models, which in LangChain are called:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"LLMs: this is a language model which takes a string as input and returns a string"),(0,o.kt)("li",{parentName:"ul"},"ChatModels: this is a language model which takes a list of messages as input and returns a message")),(0,o.kt)("p",null,"The input/output for LLMs is simple and easy to understand - a string.\nBut what about ChatModels? The input there is a list of ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatMessage"),"s, and the output is a single ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatMessage"),".\nA ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatMessage")," has two required components:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"content"),": This is the content of the message."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"role"),": This is the role of the entity from which the ",(0,o.kt)("inlineCode",{parentName:"li"},"ChatMessage")," is coming from.")),(0,o.kt)("p",null,"LangChain provides several objects to easily distinguish between different roles:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"HumanMessage"),": A ",(0,o.kt)("inlineCode",{parentName:"li"},"ChatMessage")," coming from a human/user."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"AIMessage"),": A ",(0,o.kt)("inlineCode",{parentName:"li"},"ChatMessage")," coming from an AI/assistant."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SystemMessage"),": A ",(0,o.kt)("inlineCode",{parentName:"li"},"ChatMessage")," coming from the system."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"FunctionMessage"),": A ",(0,o.kt)("inlineCode",{parentName:"li"},"ChatMessage")," coming from a function call.")),(0,o.kt)("p",null,"If none of those roles sound right, there is also a ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatMessage")," class where you can specify the role manually.\nFor more information on how to use these different messages most effectively, see our prompting guide."),(0,o.kt)("p",null,"LangChain provides a standard interface for both, but it's useful to understand this difference in order to construct prompts for a given language model.\nThe standard interface that LangChain provides has two methods:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"predict"),": Takes in a string, returns a string"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"predict_messages"),": Takes in a list of messages, returns a message.")),(0,o.kt)("p",null,"Let's see how to work with these different types of models and these different types of inputs.\nFirst, let's import an LLM and a ChatModel."),(0,o.kt)(f,{mdxType:"ImportLLMs"}),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"OpenAI")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatOpenAI")," objects are basically just configuration objects.\nYou can initialize them with parameters like ",(0,o.kt)("inlineCode",{parentName:"p"},"temperature")," and others, and pass them around."),(0,o.kt)("p",null,"Next, let's use the ",(0,o.kt)("inlineCode",{parentName:"p"},"predict")," method to run over a string input."),(0,o.kt)(b,{mdxType:"InputString"}),(0,o.kt)("p",null,"Finally, let's use the ",(0,o.kt)("inlineCode",{parentName:"p"},"predict_messages")," method to run over a list of messages."),(0,o.kt)(C,{mdxType:"InputMessages"}),(0,o.kt)("p",null,"For both these methods, you can also pass in parameters as key word arguments.\nFor example, you could pass in ",(0,o.kt)("inlineCode",{parentName:"p"},"temperature=0")," to adjust the temperature that is used from what the object was configured with.\nWhatever values are passed in during run time will always override what the object was configured with."),(0,o.kt)("h2",{id:"prompt-templates"},"Prompt templates"),(0,o.kt)("p",null,"Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand."),(0,o.kt)("p",null,"In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions."),(0,o.kt)("p",null,"PromptTemplates help with exactly this!\nThey bundle up all the logic for going from user input into a fully formatted prompt.\nThis can start off very simple - for example, a prompt to produce the above string would just be:"),(0,o.kt)(M,{mdxType:"PromptTemplateLLM"}),(0,o.kt)("p",null,'However, the advantages of using these over raw string formatting are several.\nYou can "partial" out variables - e.g. you can format only some of the variables at a time.\nYou can compose them together, easily combining different templates into a single prompt.\nFor explanations of these functionalities, see the ',(0,o.kt)("a",{parentName:"p",href:"/docs/modules/model_io/prompts"},"section on prompts")," for more detail."),(0,o.kt)("p",null,"PromptTemplates can also be used to produce a list of messages.\nIn this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc)\nHere, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates.\nEach ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content.\nLet's take a look at this below:"),(0,o.kt)(x,{mdxType:"PromptTemplateChatModel"}),(0,o.kt)("p",null,"ChatPromptTemplates can also include other things besides ChatMessageTemplates - see the ",(0,o.kt)("a",{parentName:"p",href:"/docs/modules/model_io/prompts"},"section on prompts")," for more detail."),(0,o.kt)("h2",{id:"output-parsers"},"Output parsers"),(0,o.kt)("p",null,"OutputParsers convert the raw output of an LLM into a format that can be used downstream.\nThere are few main type of OutputParsers, including:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Convert text from LLM -> structured information (e.g. JSON)"),(0,o.kt)("li",{parentName:"ul"},"Convert a ChatMessage into just a string"),(0,o.kt)("li",{parentName:"ul"},"Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.")),(0,o.kt)("p",null,"For full information on this, see the ",(0,o.kt)("a",{parentName:"p",href:"/docs/modules/model_io/output_parsers"},"section on output parsers")),(0,o.kt)("p",null,"In this getting started guide, we will write our own output parser - one that converts a comma separated list into a list."),(0,o.kt)(E,{mdxType:"OutputParser"}),(0,o.kt)("h2",{id:"llmchain"},"LLMChain"),(0,o.kt)("p",null,"We can now combine all these into one chain.\nThis chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to an LLM, and then pass the output through an (optional) output parser.\nThis is a convenient way to bundle up a modular piece of logic.\nLet's see it in action!"),(0,o.kt)(S,{mdxType:"LLMChain"}),(0,o.kt)("h2",{id:"next-steps"},"Next steps"),(0,o.kt)("p",null,"This is it!\nWe've now gone over how to create the core building block of LangChain applications - the LLMChains.\nThere is a lot more nuance in all these components (LLMs, prompts, output parsers) and a lot more different components to learn about as well.\nTo continue on your journey:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/modules/model_io"},"Dive deeper")," into LLMs, prompts, and output parsers"),(0,o.kt)("li",{parentName:"ul"},"Learn the other ",(0,o.kt)("a",{parentName:"li",href:"/docs/modules"},"key components")),(0,o.kt)("li",{parentName:"ul"},"Check out our ",(0,o.kt)("a",{parentName:"li",href:"/docs/guides"},"helpful guides")," for detailed walkthroughs on particular topics"),(0,o.kt)("li",{parentName:"ul"},"Explore ",(0,o.kt)("a",{parentName:"li",href:"/docs/use_cases"},"end-to-end use cases"))))}j.isMDXComponent=!0}}]);