"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[40046],{3905:(e,t,r)=>{r.d(t,{Zo:()=>c,kt:()=>h});var n=r(67294);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function l(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,n,o=function(e,t){if(null==e)return{};var r,n,o={},a=Object.keys(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||(o[r]=e[r]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var i=n.createContext({}),u=function(e){var t=n.useContext(i),r=t;return e&&(r="function"==typeof e?e(t):l(l({},t),e)),r},c=function(e){var t=u(e.components);return n.createElement(i.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var r=e.components,o=e.mdxType,a=e.originalType,i=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=u(r),m=o,h=d["".concat(i,".").concat(m)]||d[m]||p[m]||a;return r?n.createElement(h,l(l({ref:t},c),{},{components:r})):n.createElement(h,l({ref:t},c))}));function h(e,t){var r=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=r.length,l=new Array(a);l[0]=m;var s={};for(var i in t)hasOwnProperty.call(t,i)&&(s[i]=t[i]);s.originalType=e,s[d]="string"==typeof e?e:o,l[1]=s;for(var u=2;u<a;u++)l[u]=r[u];return n.createElement.apply(null,l)}return n.createElement.apply(null,r)}m.displayName="MDXCreateElement"},37501:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>i,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>u});var n=r(87462),o=(r(67294),r(3905));const a={},l="Recursive URL Loader",s={unversionedId:"integrations/document_loaders/recursive_url_loader",id:"integrations/document_loaders/recursive_url_loader",title:"Recursive URL Loader",description:"We may want to process load all URLs under a root directory.",source:"@site/docs/integrations/document_loaders/recursive_url_loader.md",sourceDirName:"integrations/document_loaders",slug:"/integrations/document_loaders/recursive_url_loader",permalink:"/langchain/docs/integrations/document_loaders/recursive_url_loader",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"ReadTheDocs Documentation",permalink:"/langchain/docs/integrations/document_loaders/readthedocs_documentation"},next:{title:"Reddit",permalink:"/langchain/docs/integrations/document_loaders/reddit"}},i={},u=[],c=(d="CodeOutputBlock",function(e){return console.warn("Component "+d+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var d;const p={toc:u},m="wrapper";function h(e){let{components:t,...r}=e;return(0,o.kt)(m,(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"recursive-url-loader"},"Recursive URL Loader"),(0,o.kt)("p",null,"We may want to process load all URLs under a root directory."),(0,o.kt)("p",null,"For example, let's look at the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.python.org/3.9/"},"Python 3.9 Document"),"."),(0,o.kt)("p",null,"This has many interesting child pages that we may want to read in bulk."),(0,o.kt)("p",null,"Of course, the ",(0,o.kt)("inlineCode",{parentName:"p"},"WebBaseLoader")," can load a list of pages. "),(0,o.kt)("p",null,"But, the challenge is traversing the tree of child pages and actually assembling that list!"),(0,o.kt)("p",null,"We do this using the ",(0,o.kt)("inlineCode",{parentName:"p"},"RecursiveUrlLoader"),"."),(0,o.kt)("p",null,"This also gives us the flexibility to exclude some children, customize the extractor, and more."),(0,o.kt)("h1",{id:"parameters"},"Parameters"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"url: str, the target url to crawl."),(0,o.kt)("li",{parentName:"ul"},"exclude_dirs: Optional","[str]",", webpage directories to exclude."),(0,o.kt)("li",{parentName:"ul"},"use_async: Optional","[bool]",", wether to use async requests, using async requests is usually faster in large tasks. However, async will disable the lazy loading feature(the function still works, but it is not lazy). By default, it is set to False."),(0,o.kt)("li",{parentName:"ul"},"extractor: Optional[Callable[","[str]",", str]], a function to extract the text of the document from the webpage, by default it returns the page as it is. It is recommended to use tools like goose3 and beautifulsoup to extract the text. By default, it just returns the page as it is."),(0,o.kt)("li",{parentName:"ul"},"max_depth: Optional","[int]"," = None, the maximum depth to crawl. By default, it is set to 2. If you need to crawl the whole website, set it to a number that is large enough would simply do the job."),(0,o.kt)("li",{parentName:"ul"},"timeout: Optional","[int]"," = None, the timeout for each request, in the unit of seconds. By default, it is set to 10."),(0,o.kt)("li",{parentName:"ul"},"prevent_outside: Optional","[bool]"," = None, whether to prevent crawling outside the root url. By default, it is set to True.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "RecursiveUrlLoader", "source": "langchain.document_loaders.recursive_url_loader", "docs": "https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.recursive_url_loader.RecursiveUrlLoader.html", "title": "Recursive URL Loader"}]--\x3e\nfrom langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n')),(0,o.kt)("p",null,"Let's try a simple example."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from bs4 import BeautifulSoup as Soup\n\nurl = "https://docs.python.org/3.9/"\nloader = RecursiveUrlLoader(url=url, max_depth=2, extractor=lambda x: Soup(x, "html.parser").text)\ndocs = loader.load()\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"docs[0].page_content[:50]\n")),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    '\\n\\n\\n\\n\\nPython Frequently Asked Questions \u2014 Python 3.'\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"docs[-1].metadata\n")),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    {'source': 'https://docs.python.org/3.9/library/index.html',\n     'title': 'The Python Standard Library \u2014 Python 3.9.17 documentation',\n     'language': None}\n"))),(0,o.kt)("p",null,"However, since it's hard to perform a perfect filter, you may still see some irrelevant results in the results. You can perform a filter on the returned documents by yourself, if it's needed. Most of the time, the returned results are good enough."),(0,o.kt)("p",null,"Testing on LangChain docs."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'url = "https://js.langchain.com/docs/modules/memory/integrations/"\nloader = RecursiveUrlLoader(url=url)\ndocs = loader.load()\nlen(docs)\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    8\n"))))}h.isMDXComponent=!0}}]);