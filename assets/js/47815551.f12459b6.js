"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[11564],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>g});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=o.createContext({}),p=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=p(e.components);return o.createElement(s.Provider,{value:t},e.children)},h="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},m=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),h=p(n),m=a,g=h["".concat(s,".").concat(m)]||h[m]||c[m]||r;return n?o.createElement(g,l(l({ref:t},u),{},{components:n})):o.createElement(g,l({ref:t},u))}));function g(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,l=new Array(r);l[0]=m;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[h]="string"==typeof e?e:a,l[1]=i;for(var p=2;p<r;p++)l[p]=n[p];return o.createElement.apply(null,l)}return o.createElement.apply(null,n)}m.displayName="MDXCreateElement"},86465:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>m,contentTitle:()=>h,default:()=>f,frontMatter:()=>u,metadata:()=>c,toc:()=>g});var o=n(87462),a=(n(67294),n(3905));const r=(l="CodeOutputBlock",function(e){return console.warn("Component "+l+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var l;const i={toc:[{value:"Set up environment",id:"set-up-environment",level:2},{value:"Set up tools",id:"set-up-tools",level:2},{value:"Prompt template",id:"prompt-template",level:2},{value:"Output parser",id:"output-parser",level:2},{value:"Set up LLM",id:"set-up-llm",level:2},{value:"Define the stop sequence",id:"define-the-stop-sequence",level:2},{value:"Set up the Agent",id:"set-up-the-agent",level:2},{value:"Use the Agent",id:"use-the-agent",level:2}]},s="wrapper";function p(e){let{components:t,...n}=e;return(0,a.kt)(s,(0,o.Z)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"The LLM Agent is used in an ",(0,a.kt)("inlineCode",{parentName:"p"},"AgentExecutor"),". This ",(0,a.kt)("inlineCode",{parentName:"p"},"AgentExecutor")," can largely be thought of as a loop that:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Passes user input and any previous steps to the Agent (in this case, the LLM Agent)"),(0,a.kt)("li",{parentName:"ol"},"If the Agent returns an ",(0,a.kt)("inlineCode",{parentName:"li"},"AgentFinish"),", then return that directly to the user"),(0,a.kt)("li",{parentName:"ol"},"If the Agent returns an ",(0,a.kt)("inlineCode",{parentName:"li"},"AgentAction"),", then use that to call a tool and get an ",(0,a.kt)("inlineCode",{parentName:"li"},"Observation")),(0,a.kt)("li",{parentName:"ol"},"Repeat, passing the ",(0,a.kt)("inlineCode",{parentName:"li"},"AgentAction")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"Observation")," back to the Agent until an ",(0,a.kt)("inlineCode",{parentName:"li"},"AgentFinish")," is emitted.")),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"AgentAction")," is a response that consists of ",(0,a.kt)("inlineCode",{parentName:"p"},"action")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"action_input"),". ",(0,a.kt)("inlineCode",{parentName:"p"},"action")," refers to which tool to use, and ",(0,a.kt)("inlineCode",{parentName:"p"},"action_input")," refers to the input to that tool. ",(0,a.kt)("inlineCode",{parentName:"p"},"log")," can also be provided as more context (that can be used for logging, tracing, etc)."),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"AgentFinish")," is a response that contains the final message to be sent back to the user. This should be used to end an agent run."),(0,a.kt)("p",null,"In this notebook we walk through how to create a custom LLM agent."),(0,a.kt)("h2",{id:"set-up-environment"},"Set up environment"),(0,a.kt)("p",null,"Do necessary imports, etc."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"pip install langchain\npip install google-search-results\npip install openai\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\nfrom langchain.prompts import BaseChatPromptTemplate\nfrom langchain import SerpAPIWrapper, LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom typing import List, Union\nfrom langchain.schema import AgentAction, AgentFinish, HumanMessage\nimport re\nfrom getpass import getpass\n")),(0,a.kt)("h2",{id:"set-up-tools"},"Set up tools"),(0,a.kt)("p",null,"Set up any tools the agent may want to use. This may be necessary to put in the prompt (so that the agent knows to use these tools)."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"SERPAPI_API_KEY = getpass()\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# Define which tools the agent can use to answer user queries\nsearch = SerpAPIWrapper(serpapi_api_key=SERPAPI_API_KEY)\ntools = [\n    Tool(\n        name = "Search",\n        func=search.run,\n        description="useful for when you need to answer questions about current events"\n    )\n]\n')),(0,a.kt)("h2",{id:"prompt-template"},"Prompt template"),(0,a.kt)("p",null,"This instructs the agent on what to do. Generally, the template should incorporate:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"tools"),": which tools the agent has access and how and when to call them."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"intermediate_steps"),": These are tuples of previous (",(0,a.kt)("inlineCode",{parentName:"li"},"AgentAction"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"Observation"),") pairs. These are generally not passed directly to the model, but the prompt template formats them in a specific way."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"input"),": generic user input")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# Set up the base template\ntemplate = """Complete the objective as best you can. You have access to the following tools:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nThese were previous tasks you completed:\n\n\n\nBegin!\n\nQuestion: {input}\n{agent_scratchpad}"""\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# Set up a prompt template\nclass CustomPromptTemplate(BaseChatPromptTemplate):\n    # The template to use\n    template: str\n    # The list of tools available\n    tools: List[Tool]\n    \n    def format_messages(self, **kwargs) -> str:\n        # Get the intermediate steps (AgentAction, Observation tuples)\n        # Format them in a particular way\n        intermediate_steps = kwargs.pop("intermediate_steps")\n        thoughts = ""\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f"\\nObservation: {observation}\\nThought: "\n        # Set the agent_scratchpad variable to that value\n        kwargs["agent_scratchpad"] = thoughts\n        # Create a tools variable from the list of tools provided\n        kwargs["tools"] = "\\n".join([f"{tool.name}: {tool.description}" for tool in self.tools])\n        # Create a list of tool names for the tools provided\n        kwargs["tool_names"] = ", ".join([tool.name for tool in self.tools])\n        formatted = self.template.format(**kwargs)\n        return [HumanMessage(content=formatted)]\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'prompt = CustomPromptTemplate(\n    template=template,\n    tools=tools,\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n    # This includes the `intermediate_steps` variable because that is needed\n    input_variables=["input", "intermediate_steps"]\n)\n')),(0,a.kt)("h2",{id:"output-parser"},"Output parser"),(0,a.kt)("p",null,"The output parser is responsible for parsing the LLM output into ",(0,a.kt)("inlineCode",{parentName:"p"},"AgentAction")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"AgentFinish"),". This usually depends heavily on the prompt used."),(0,a.kt)("p",null,"This is where you can change the parsing to do retries, handle whitespace, etc."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'class CustomOutputParser(AgentOutputParser):\n    \n    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n        # Check if agent should finish\n        if "Final Answer:" in llm_output:\n            return AgentFinish(\n                # Return values is generally always a dictionary with a single `output` key\n                # It is not recommended to try anything else at the moment :)\n                return_values={"output": llm_output.split("Final Answer:")[-1].strip()},\n                log=llm_output,\n            )\n        # Parse out the action and action input\n        regex = r"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)"\n        match = re.search(regex, llm_output, re.DOTALL)\n        if not match:\n            raise ValueError(f"Could not parse LLM output: `{llm_output}`")\n        action = match.group(1).strip()\n        action_input = match.group(2)\n        # Return the action and action input\n        return AgentAction(tool=action, tool_input=action_input.strip(" ").strip(\'"\'), log=llm_output)\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"output_parser = CustomOutputParser()\n")),(0,a.kt)("h2",{id:"set-up-llm"},"Set up LLM"),(0,a.kt)("p",null,"Choose the LLM you want to use!"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"OPENAI_API_KEY = getpass()\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0)\n")),(0,a.kt)("h2",{id:"define-the-stop-sequence"},"Define the stop sequence"),(0,a.kt)("p",null,"This is important because it tells the LLM when to stop generation."),(0,a.kt)("p",null,"This depends heavily on the prompt and model you are using. Generally, you want this to be whatever token you use in the prompt to denote the start of an ",(0,a.kt)("inlineCode",{parentName:"p"},"Observation")," (otherwise, the LLM may hallucinate an observation for you)."),(0,a.kt)("h2",{id:"set-up-the-agent"},"Set up the Agent"),(0,a.kt)("p",null,"We can now combine everything to set up our agent:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# LLM chain consisting of the LLM and a prompt\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'tool_names = [tool.name for tool in tools]\nagent = LLMSingleActionAgent(\n    llm_chain=llm_chain, \n    output_parser=output_parser,\n    stop=["\\nObservation:"], \n    allowed_tools=tool_names\n)\n')),(0,a.kt)("h2",{id:"use-the-agent"},"Use the Agent"),(0,a.kt)("p",null,"Now we can use it!"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'agent_executor.run("Search for Leo DiCaprio\'s girlfriend on the internet.")\n')),(0,a.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'    \n    \n    > Entering new AgentExecutor chain...\n    Thought: I should use a reliable search engine to get accurate information.\n    Action: Search\n    Action Input: "Leo DiCaprio girlfriend"\n    \n    Observation:He went on to date Gisele B\xfcndchen, Bar Refaeli, Blake Lively, Toni Garrn and Nina Agdal, among others, before finally settling down with current girlfriend Camila Morrone, who is 23 years his junior.\n    I have found the answer to the question.\n    Final Answer: Leo DiCaprio\'s current girlfriend is Camila Morrone.\n    \n    > Finished chain.\n\n\n\n\n\n    "Leo DiCaprio\'s current girlfriend is Camila Morrone."\n'))))}p.isMDXComponent=!0;const u={},h="Custom LLM Agent (with a ChatModel)",c={unversionedId:"modules/agents/how_to/custom_llm_chat_agent",id:"modules/agents/how_to/custom_llm_chat_agent",title:"Custom LLM Agent (with a ChatModel)",description:"This notebook goes through how to create your own custom agent based on a chat model.",source:"@site/docs/modules/agents/how_to/custom_llm_chat_agent.mdx",sourceDirName:"modules/agents/how_to",slug:"/modules/agents/how_to/custom_llm_chat_agent",permalink:"/langchain/docs/modules/agents/how_to/custom_llm_chat_agent",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Custom LLM agent",permalink:"/langchain/docs/modules/agents/how_to/custom_llm_agent"},next:{title:"Custom MRKL agent",permalink:"/langchain/docs/modules/agents/how_to/custom_mrkl_agent"}},m={},g=[],d={toc:g},k="wrapper";function f(e){let{components:t,...n}=e;return(0,a.kt)(k,(0,o.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"custom-llm-agent-with-a-chatmodel"},"Custom LLM Agent (with a ChatModel)"),(0,a.kt)("p",null,"This notebook goes through how to create your own custom agent based on a chat model."),(0,a.kt)("p",null,"An LLM chat agent consists of three parts:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"PromptTemplate"),": This is the prompt template that can be used to instruct the language model on what to do"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"ChatModel"),": This is the language model that powers the agent"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"stop")," sequence: Instructs the LLM to stop generating as soon as this string is found"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"OutputParser"),": This determines how to parse the LLM output into an ",(0,a.kt)("inlineCode",{parentName:"li"},"AgentAction")," or ",(0,a.kt)("inlineCode",{parentName:"li"},"AgentFinish")," object")),(0,a.kt)(p,{mdxType:"Example"}))}f.isMDXComponent=!0}}]);