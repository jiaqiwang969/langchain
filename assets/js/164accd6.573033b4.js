"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[61598],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>h});var a=t(67294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=a.createContext({}),d=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=d(e.components);return a.createElement(l.Provider,{value:n},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=d(t),m=i,h=u["".concat(l,".").concat(m)]||u[m]||c[m]||r;return t?a.createElement(h,o(o({ref:n},p),{},{components:t})):a.createElement(h,o({ref:n},p))}));function h(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[u]="string"==typeof e?e:i,o[1]=s;for(var d=2;d<r;d++)o[d]=t[d];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},95825:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});var a=t(87462),i=(t(67294),t(3905));const r={},o="Wikipedia",s={unversionedId:"integrations/retrievers/wikipedia",id:"integrations/retrievers/wikipedia",title:"Wikipedia",description:"Wikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.",source:"@site/docs/integrations/retrievers/wikipedia.md",sourceDirName:"integrations/retrievers",slug:"/integrations/retrievers/wikipedia",permalink:"/langchain/docs/integrations/retrievers/wikipedia",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Weaviate Hybrid Search",permalink:"/langchain/docs/integrations/retrievers/weaviate-hybrid"},next:{title:"Zep",permalink:"/langchain/docs/integrations/retrievers/zep_memorystore"}},l={},d=[{value:"Installation",id:"installation",level:2},{value:"Examples",id:"examples",level:2},{value:"Running retriever",id:"running-retriever",level:3},{value:"Question Answering on facts",id:"question-answering-on-facts",level:3}],p=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,i.kt)("div",e)});var u;const c={toc:d},m="wrapper";function h(e){let{components:n,...t}=e;return(0,i.kt)(m,(0,a.Z)({},c,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"wikipedia"},"Wikipedia"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("a",{parentName:"p",href:"https://wikipedia.org/"},"Wikipedia")," is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. ",(0,i.kt)("inlineCode",{parentName:"p"},"Wikipedia")," is the largest and most-read reference work in history.")),(0,i.kt)("p",null,"This notebook shows how to retrieve wiki pages from ",(0,i.kt)("inlineCode",{parentName:"p"},"wikipedia.org")," into the Document format that is used downstream."),(0,i.kt)("h2",{id:"installation"},"Installation"),(0,i.kt)("p",null,"First, you need to install ",(0,i.kt)("inlineCode",{parentName:"p"},"wikipedia")," python package."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"#!pip install wikipedia\n")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"WikipediaRetriever")," has these arguments:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"optional ",(0,i.kt)("inlineCode",{parentName:"li"},"lang"),': default="en". Use it to search in a specific language part of Wikipedia'),(0,i.kt)("li",{parentName:"ul"},"optional ",(0,i.kt)("inlineCode",{parentName:"li"},"load_max_docs"),": default=100. Use it to limit number of downloaded documents. It takes time to download all 100 documents, so use a small number for experiments. There is a hard limit of 300 for now."),(0,i.kt)("li",{parentName:"ul"},"optional ",(0,i.kt)("inlineCode",{parentName:"li"},"load_all_available_meta"),": default=False. By default only the most important fields downloaded: ",(0,i.kt)("inlineCode",{parentName:"li"},"Published")," (date when document was published/last updated), ",(0,i.kt)("inlineCode",{parentName:"li"},"title"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"Summary"),". If True, other fields also downloaded.")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"get_relevant_documents()")," has one argument, ",(0,i.kt)("inlineCode",{parentName:"p"},"query"),": free text which used to find documents in Wikipedia"),(0,i.kt)("h2",{id:"examples"},"Examples"),(0,i.kt)("h3",{id:"running-retriever"},"Running retriever"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "WikipediaRetriever", "source": "langchain.retrievers", "docs": "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.wikipedia.WikipediaRetriever.html", "title": "Wikipedia"}]--\x3e\nfrom langchain.retrievers import WikipediaRetriever\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"retriever = WikipediaRetriever()\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'docs = retriever.get_relevant_documents(query="HUNTER X HUNTER")\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"docs[0].metadata  # meta-information of the Document\n")),(0,i.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"    {'title': 'Hunter \xd7 Hunter',\n     'summary': 'Hunter \xd7 Hunter (stylized as HUNTER\xd7HUNTER and pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s sh\u014dnen manga magazine Weekly Sh\u014dnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 37 tank\u014dbon volumes as of November 2022. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\\nHunter \xd7 Hunter was adapted into a 62-episode anime television series produced by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter \xd7 Hunter.\\nThe manga has been translated into English and released in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim\\'s Toonami programming block from April 2016 to June 2019.\\nHunter \xd7 Hunter has been a huge critical and financial success and has become one of the best-selling manga series of all time, having over 84 million copies in circulation by July 2022.\\n\\n'}\n"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"docs[0].page_content[:400]  # a content of the Document\n")),(0,i.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"    'Hunter \xd7 Hunter (stylized as HUNTER\xd7HUNTER and pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s sh\u014dnen manga magazine Weekly Sh\u014dnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 37 tank\u014dbon volumes as of November 2022. The sto'\n"))),(0,i.kt)("h3",{id:"question-answering-on-facts"},"Question Answering on facts"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# get a token: https://platform.openai.com/account/api-keys\n\nfrom getpass import getpass\n\nOPENAI_API_KEY = getpass()\n")),(0,i.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"     \xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\n"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nos.environ["OPENAI_API_KEY"] = OPENAI_API_KEY\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html", "title": "Wikipedia"}, {"imported": "ConversationalRetrievalChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html", "title": "Wikipedia"}]--\x3e\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\n\nmodel = ChatOpenAI(model_name="gpt-3.5-turbo")  # switch to \'gpt-4\'\nqa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'questions = [\n    "What is Apify?",\n    "When the Monument to the Martyrs of the 1830 Revolution was created?",\n    "What is the Abhayagiri Vih\u0101ra?",\n    # "How big is Wikip\xe9dia en fran\xe7ais?",\n]\nchat_history = []\n\nfor question in questions:\n    result = qa({"question": question, "chat_history": chat_history})\n    chat_history.append((question, result["answer"]))\n    print(f"-> **Question**: {question} \\n")\n    print(f"**Answer**: {result[\'answer\']} \\n")\n')),(0,i.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"    -> **Question**: What is Apify? \n    \n    **Answer**: Apify is a platform that allows you to easily automate web scraping, data extraction and web automation. It provides a cloud-based infrastructure for running web crawlers and other automation tasks, as well as a web-based tool for building and managing your crawlers. Additionally, Apify offers a marketplace for buying and selling pre-built crawlers and related services. \n    \n    -> **Question**: When the Monument to the Martyrs of the 1830 Revolution was created? \n    \n    **Answer**: Apify is a web scraping and automation platform that enables you to extract data from websites, turn unstructured data into structured data, and automate repetitive tasks. It provides a user-friendly interface for creating web scraping scripts without any coding knowledge. Apify can be used for various web scraping tasks such as data extraction, web monitoring, content aggregation, and much more. Additionally, it offers various features such as proxy support, scheduling, and integration with other tools to make web scraping and automation tasks easier and more efficient. \n    \n    -> **Question**: What is the Abhayagiri Vih\u0101ra? \n    \n    **Answer**: Abhayagiri Vih\u0101ra was a major monastery site of Theravada Buddhism that was located in Anuradhapura, Sri Lanka. It was founded in the 2nd century BCE and is considered to be one of the most important monastic complexes in Sri Lanka. \n    \n"))))}h.isMDXComponent=!0}}]);