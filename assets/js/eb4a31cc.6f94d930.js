"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[64731],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>k});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function p(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),i=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},c=function(e){var t=i(e.components);return r.createElement(s.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,c=p(e,["components","mdxType","originalType","parentName"]),u=i(n),d=a,k=u["".concat(s,".").concat(d)]||u[d]||m[d]||o;return n?r.createElement(k,l(l({ref:t},c),{},{components:n})):r.createElement(k,l({ref:t},c))}));function k(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,l=new Array(o);l[0]=d;var p={};for(var s in t)hasOwnProperty.call(t,s)&&(p[s]=t[s]);p.originalType=e,p[u]="string"==typeof e?e:a,l[1]=p;for(var i=2;i<o;i++)l[i]=n[i];return r.createElement.apply(null,l)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},20538:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>k,frontMatter:()=>o,metadata:()=>p,toc:()=>i});var r=n(87462),a=(n(67294),n(3905));const o={},l="Databricks",p={unversionedId:"integrations/llms/databricks",id:"integrations/llms/databricks",title:"Databricks",description:"The Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.",source:"@site/docs/integrations/llms/databricks.md",sourceDirName:"integrations/llms",slug:"/integrations/llms/databricks",permalink:"/langchain/docs/integrations/llms/databricks",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"C Transformers",permalink:"/langchain/docs/integrations/llms/ctransformers"},next:{title:"DeepInfra",permalink:"/langchain/docs/integrations/llms/deepinfra"}},s={},i=[{value:"Wrapping a serving endpoint",id:"wrapping-a-serving-endpoint",level:2},{value:"Wrapping a cluster driver proxy app",id:"wrapping-a-cluster-driver-proxy-app",level:2}],c=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var u;const m={toc:i},d="wrapper";function k(e){let{components:t,...n}=e;return(0,a.kt)(d,(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"databricks"},"Databricks"),(0,a.kt)("p",null,"The ",(0,a.kt)("a",{parentName:"p",href:"https://www.databricks.com/"},"Databricks")," Lakehouse Platform unifies data, analytics, and AI on one platform."),(0,a.kt)("p",null,"This example notebook shows how to wrap Databricks endpoints as LLMs in LangChain.\nIt supports two endpoint types:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Serving endpoint, recommended for production and development,"),(0,a.kt)("li",{parentName:"ul"},"Cluster driver proxy app, recommended for iteractive development.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "Databricks", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.databricks.Databricks.html", "title": "Databricks"}]--\x3e\nfrom langchain.llms import Databricks\n')),(0,a.kt)("h2",{id:"wrapping-a-serving-endpoint"},"Wrapping a serving endpoint"),(0,a.kt)("p",null,"Prerequisites:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"An LLM was registered and deployed to ",(0,a.kt)("a",{parentName:"li",href:"https://docs.databricks.com/machine-learning/model-serving/index.html"},"a Databricks serving endpoint"),"."),(0,a.kt)("li",{parentName:"ul"},"You have ",(0,a.kt)("a",{parentName:"li",href:"https://docs.databricks.com/security/auth-authz/access-control/serving-endpoint-acl.html"},'"Can Query" permission')," to the endpoint.")),(0,a.kt)("p",null,"The expected MLflow model signature is:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"inputs: ",(0,a.kt)("inlineCode",{parentName:"li"},'[{"name": "prompt", "type": "string"}, {"name": "stop", "type": "list[string]"}]')),(0,a.kt)("li",{parentName:"ul"},"outputs: ",(0,a.kt)("inlineCode",{parentName:"li"},'[{"type": "string"}]'))),(0,a.kt)("p",null,"If the model signature is incompatible or you want to insert extra configs, you can set ",(0,a.kt)("inlineCode",{parentName:"p"},"transform_input_fn")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"transform_output_fn")," accordingly."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# If running a Databricks notebook attached to an interactive cluster in "single user"\n# or "no isolation shared" mode, you only need to specify the endpoint name to create\n# a `Databricks` instance to query a serving endpoint in the same workspace.\nllm = Databricks(endpoint_name="dolly")\n\nllm("How are you?")\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    'I am happy to hear that you are in good health and as always, you are appreciated.'\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'llm("How are you?", stop=["."])\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    'Good'\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# Otherwise, you can manually specify the Databricks workspace hostname and personal access token\n# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.\n# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens\n# We strongly recommend not exposing the API token explicitly inside a notebook.\n# You can use Databricks secret manager to store your API token securely.\n# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecrets\n\nimport os\n\nos.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("myworkspace", "api_token")\n\nllm = Databricks(host="myworkspace.cloud.databricks.com", endpoint_name="dolly")\n\nllm("How are you?")\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    'I am fine. Thank you!'\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# If the serving endpoint accepts extra parameters like `temperature`,\n# you can set them in `model_kwargs`.\nllm = Databricks(endpoint_name="dolly", model_kwargs={"temperature": 0.1})\n\nllm("How are you?")\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    'I am fine.'\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# Use `transform_input_fn` and `transform_output_fn` if the serving endpoint\n# expects a different input schema and does not return a JSON string,\n# respectively, or you want to apply a prompt template on top.\n\n\ndef transform_input(**request):\n    full_prompt = f"""{request["prompt"]}\n    Be Concise.\n    """\n    request["prompt"] = full_prompt\n    return request\n\n\nllm = Databricks(endpoint_name="dolly", transform_input_fn=transform_input)\n\nllm("How are you?")\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    'I\u2019m Excellent. You?'\n"))),(0,a.kt)("h2",{id:"wrapping-a-cluster-driver-proxy-app"},"Wrapping a cluster driver proxy app"),(0,a.kt)("p",null,"Prerequisites:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},'An LLM loaded on a Databricks interactive cluster in "single user" or "no isolation shared" mode.'),(0,a.kt)("li",{parentName:"ul"},"A local HTTP server running on the driver node to serve the model at ",(0,a.kt)("inlineCode",{parentName:"li"},'"/"')," using HTTP POST with JSON input/output."),(0,a.kt)("li",{parentName:"ul"},"It uses a port number between ",(0,a.kt)("inlineCode",{parentName:"li"},"[3000, 8000]")," and listens to the driver IP address or simply ",(0,a.kt)("inlineCode",{parentName:"li"},"0.0.0.0")," instead of localhost only."),(0,a.kt)("li",{parentName:"ul"},'You have "Can Attach To" permission to the cluster.')),(0,a.kt)("p",null,"The expected server schema (using JSON schema) is:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"inputs:",(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-json"},'{"type": "object",\n "properties": {\n    "prompt": {"type": "string"},\n     "stop": {"type": "array", "items": {"type": "string"}}},\n  "required": ["prompt"]}\n'))),(0,a.kt)("li",{parentName:"ul"},"outputs: ",(0,a.kt)("inlineCode",{parentName:"li"},'{"type": "string"}'))),(0,a.kt)("p",null,"If the server schema is incompatible or you want to insert extra configs, you can use ",(0,a.kt)("inlineCode",{parentName:"p"},"transform_input_fn")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"transform_output_fn")," accordingly."),(0,a.kt)("p",null,"The following is a minimal example for running a driver proxy app to serve an LLM:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from flask import Flask, request, jsonify\nimport torch\nfrom transformers import pipeline, AutoTokenizer, StoppingCriteria\n\nmodel = "databricks/dolly-v2-3b"\ntokenizer = AutoTokenizer.from_pretrained(model, padding_side="left")\ndolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map="auto")\ndevice = dolly.device\n\nclass CheckStop(StoppingCriteria):\n    def __init__(self, stop=None):\n        super().__init__()\n        self.stop = stop or []\n        self.matched = ""\n        self.stop_ids = [tokenizer.encode(s, return_tensors=\'pt\').to(device) for s in self.stop]\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):\n        for i, s in enumerate(self.stop_ids):\n            if torch.all((s == input_ids[0][-s.shape[1]:])).item():\n                self.matched = self.stop[i]\n                return True\n        return False\n\ndef llm(prompt, stop=None, **kwargs):\n  check_stop = CheckStop(stop)\n  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)\n  return result[0]["generated_text"].rstrip(check_stop.matched)\n\napp = Flask("dolly")\n\n@app.route(\'/\', methods=[\'POST\'])\ndef serve_llm():\n  resp = llm(**request.json)\n  return jsonify(resp)\n\napp.run(host="0.0.0.0", port="7777")\n')),(0,a.kt)("p",null,"Once the server is running, you can create a ",(0,a.kt)("inlineCode",{parentName:"p"},"Databricks")," instance to wrap it as an LLM."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# If running a Databricks notebook attached to the same cluster that runs the app,\n# you only need to specify the driver port to create a `Databricks` instance.\nllm = Databricks(cluster_driver_port="7777")\n\nllm("How are you?")\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    'Hello, thank you for asking. It is wonderful to hear that you are well.'\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# Otherwise, you can manually specify the cluster ID to use,\n# as well as Databricks workspace hostname and personal access token.\n\nllm = Databricks(cluster_id="0000-000000-xxxxxxxx", cluster_driver_port="7777")\n\nllm("How are you?")\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    'I am well. You?'\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# If the app accepts extra parameters like `temperature`,\n# you can set them in `model_kwargs`.\nllm = Databricks(cluster_driver_port="7777", model_kwargs={"temperature": 0.1})\n\nllm("How are you?")\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    'I am very well. It is a pleasure to meet you.'\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# Use `transform_input_fn` and `transform_output_fn` if the app\n# expects a different input schema and does not return a JSON string,\n# respectively, or you want to apply a prompt template on top.\n\n\ndef transform_input(**request):\n    full_prompt = f"""{request["prompt"]}\n    Be Concise.\n    """\n    request["prompt"] = full_prompt\n    return request\n\n\ndef transform_output(response):\n    return response.upper()\n\n\nllm = Databricks(\n    cluster_driver_port="7777",\n    transform_input_fn=transform_input,\n    transform_output_fn=transform_output,\n)\n\nllm("How are you?")\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    'I AM DOING GREAT THANK YOU.'\n"))))}k.isMDXComponent=!0}}]);