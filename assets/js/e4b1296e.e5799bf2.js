"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[60043],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>h});var r=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,o=function(e,n){if(null==e)return{};var t,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var p=r.createContext({}),i=function(e){var n=r.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},u=function(e){var n=i(e.components);return r.createElement(p.Provider,{value:n},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef((function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,p=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=i(t),d=o,h=m["".concat(p,".").concat(d)]||m[d]||c[d]||a;return t?r.createElement(h,l(l({ref:n},u),{},{components:t})):r.createElement(h,l({ref:n},u))}));function h(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,l=new Array(a);l[0]=d;var s={};for(var p in n)hasOwnProperty.call(n,p)&&(s[p]=n[p]);s.originalType=e,s[m]="string"==typeof e?e:o,l[1]=s;for(var i=2;i<a;i++)l[i]=t[i];return r.createElement.apply(null,l)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},40731:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>i});var r=t(87462),o=(t(67294),t(3905));const a={},l="Runhouse",s={unversionedId:"integrations/llms/runhouse",id:"integrations/llms/runhouse",title:"Runhouse",description:"The Runhouse allows remote compute and data across environments and users. See the Runhouse docs.",source:"@site/docs/integrations/llms/runhouse.md",sourceDirName:"integrations/llms",slug:"/integrations/llms/runhouse",permalink:"/langchain/docs/integrations/llms/runhouse",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Replicate",permalink:"/langchain/docs/integrations/llms/replicate"},next:{title:"SageMakerEndpoint",permalink:"/langchain/docs/integrations/llms/sagemaker"}},p={},i=[],u=(m="CodeOutputBlock",function(e){return console.warn("Component "+m+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var m;const c={toc:i},d="wrapper";function h(e){let{components:n,...t}=e;return(0,o.kt)(d,(0,r.Z)({},c,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"runhouse"},"Runhouse"),(0,o.kt)("p",null,"The ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/run-house/runhouse"},"Runhouse")," allows remote compute and data across environments and users. See the ",(0,o.kt)("a",{parentName:"p",href:"https://runhouse-docs.readthedocs-hosted.com/en/latest/"},"Runhouse docs"),"."),(0,o.kt)("p",null,"This example goes over how to use LangChain and ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/run-house/runhouse"},"Runhouse")," to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note"),": Code uses ",(0,o.kt)("inlineCode",{parentName:"p"},"SelfHosted")," name instead of the ",(0,o.kt)("inlineCode",{parentName:"p"},"Runhouse"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install runhouse\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "SelfHostedPipeline", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.self_hosted.SelfHostedPipeline.html", "title": "Runhouse"}, {"imported": "SelfHostedHuggingFaceLLM", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.self_hosted_hugging_face.SelfHostedHuggingFaceLLM.html", "title": "Runhouse"}]--\x3e\nfrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\nfrom langchain import PromptTemplate, LLMChain\nimport runhouse as rh\n')),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# For an on-demand A100 with GCP, Azure, or Lambda\ngpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\", use_spot=False)\n\n# For an on-demand A10G with AWS (no single A100s on AWS)\n# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')\n\n# For an existing cluster\n# gpu = rh.cluster(ips=['<ip of the cluster>'],\n#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},\n#                  name='rh-a10x')\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'template = """Question: {question}\n\nAnswer: Let\'s think step by step."""\n\nprompt = PromptTemplate(template=template, input_variables=["question"])\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'llm = SelfHostedHuggingFaceLLM(\n    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"]\n)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"llm_chain = LLMChain(prompt=prompt, llm=llm)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"\n\nllm_chain.run(question)\n')),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC\n    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds\n\n\n\n\n\n    \"\\n\\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber\"\n"))),(0,o.kt)("p",null,"You can also load more custom models through the SelfHostedHuggingFaceLLM interface:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'llm = SelfHostedHuggingFaceLLM(\n    model_id="google/flan-t5-small",\n    task="text2text-generation",\n    hardware=gpu,\n)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'llm("What is the capital of Germany?")\n')),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC\n    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds\n\n\n\n\n\n    'berlin'\n"))),(0,o.kt)("p",null,"Using a custom load function, we can load a custom pipeline directly on the remote hardware:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'def load_pipeline():\n    from transformers import (\n        AutoModelForCausalLM,\n        AutoTokenizer,\n        pipeline,\n    )  # Need to be inside the fn in notebooks\n\n    model_id = "gpt2"\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(model_id)\n    pipe = pipeline(\n        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10\n    )\n    return pipe\n\n\ndef inference_fn(pipeline, prompt, stop=None):\n    return pipeline(prompt)[0]["generated_text"][len(prompt) :]\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"llm = SelfHostedHuggingFaceLLM(\n    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn\n)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'llm("Who is the current US president?")\n')),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC\n    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds\n\n\n\n\n\n    'john w. bush'\n"))),(0,o.kt)("p",null,"You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"pipeline = load_pipeline()\nllm = SelfHostedPipeline.from_pipeline(\n    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs\n)\n")),(0,o.kt)("p",null,"Instead, we can also send it to the hardware's filesystem, which will be much faster."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(\n    gpu, path="models"\n)\n\nllm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu)\n')))}h.isMDXComponent=!0}}]);