"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[73572],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>f});var r=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},m=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),m=a,f=u["".concat(s,".").concat(m)]||u[m]||d[m]||i;return t?r.createElement(f,o(o({ref:n},p),{},{components:t})):r.createElement(f,o({ref:n},p))}));function f(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=m;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[u]="string"==typeof e?e:a,o[1]=l;for(var c=2;c<i;c++)o[c]=t[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}m.displayName="MDXCreateElement"},20790:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var r=t(87462),a=(t(67294),t(3905));const i={},o="Xorbits Inference (Xinference)",l={unversionedId:"integrations/providers/xinference",id:"integrations/providers/xinference",title:"Xorbits Inference (Xinference)",description:"This page demonstrates how to use Xinference",source:"@site/docs/integrations/providers/xinference.mdx",sourceDirName:"integrations/providers",slug:"/integrations/providers/xinference",permalink:"/langchain/docs/integrations/providers/xinference",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Xata",permalink:"/langchain/docs/integrations/providers/xata"},next:{title:"Yeager.ai",permalink:"/langchain/docs/integrations/providers/yeagerai"}},s={},c=[{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"LLM",id:"llm",level:2},{value:"Wrapper for Xinference",id:"wrapper-for-xinference",level:3},{value:"Usage",id:"usage",level:3},{value:"Embeddings",id:"embeddings",level:3}],p={toc:c},u="wrapper";function d(e){let{components:n,...t}=e;return(0,a.kt)(u,(0,r.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"xorbits-inference-xinference"},"Xorbits Inference (Xinference)"),(0,a.kt)("p",null,"This page demonstrates how to use ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/xorbitsai/inference"},"Xinference"),"\nwith LangChain."),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"Xinference")," is a powerful and versatile library designed to serve LLMs,\nspeech recognition models, and multimodal models, even on your laptop.\nWith Xorbits Inference, you can effortlessly deploy and serve your or\nstate-of-the-art built-in models using just a single command."),(0,a.kt)("h2",{id:"installation-and-setup"},"Installation and Setup"),(0,a.kt)("p",null,"Xinference can be installed via pip from PyPI: "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'pip install "xinference[all]"\n')),(0,a.kt)("h2",{id:"llm"},"LLM"),(0,a.kt)("p",null,"Xinference supports various models compatible with GGML, including chatglm, baichuan, whisper,\nvicuna, and orca. To view the builtin models, run the command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"xinference list --all\n")),(0,a.kt)("h3",{id:"wrapper-for-xinference"},"Wrapper for Xinference"),(0,a.kt)("p",null,"You can start a local instance of Xinference by running:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"xinference\n")),(0,a.kt)("p",null,"You can also deploy Xinference in a distributed cluster. To do so, first start an Xinference supervisor\non the server you want to run it:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'xinference-supervisor -H "${supervisor_host}"\n')),(0,a.kt)("p",null,"Then, start the Xinference workers on each of the other servers where you want to run them on:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'xinference-worker -e "http://${supervisor_host}:9997"\n')),(0,a.kt)("p",null,"You can also start a local instance of Xinference by running:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"xinference\n")),(0,a.kt)("p",null,"Once Xinference is running, an endpoint will be accessible for model management via CLI or\nXinference client. "),(0,a.kt)("p",null,"For local deployment, the endpoint will be http://localhost:9997. "),(0,a.kt)("p",null,"For cluster deployment, the endpoint will be http://${supervisor_host}:9997."),(0,a.kt)("p",null,"Then, you need to launch a model. You can specify the model names and other attributes\nincluding model_size_in_billions and quantization. You can use command line interface (CLI) to\ndo it. For example, "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"xinference launch -n orca -s 3 -q q4_0\n")),(0,a.kt)("p",null,"A model uid will be returned."),(0,a.kt)("p",null,"Example usage:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "Xinference", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.xinference.Xinference.html", "title": "Xorbits Inference (Xinference)"}]--\x3e\nfrom langchain.llms import Xinference\n\nllm = Xinference(\n    server_url="http://0.0.0.0:9997",\n    model_uid = {model_uid} # replace model_uid with the model UID return from launching the model\n)\n\nllm(\n    prompt="Q: where can we visit in the capital of France? A:",\n    generate_config={"max_tokens": 1024, "stream": True},\n)\n\n')),(0,a.kt)("h3",{id:"usage"},"Usage"),(0,a.kt)("p",null,"For more information and detailed examples, refer to the\n",(0,a.kt)("a",{parentName:"p",href:"../modules/models/llms/integrations/xinference.ipynb"},"example notebook for xinference")),(0,a.kt)("h3",{id:"embeddings"},"Embeddings"),(0,a.kt)("p",null,"Xinference also supports embedding queries and documents. See\n",(0,a.kt)("a",{parentName:"p",href:"../modules/data_connection/text_embedding/integrations/xinference.ipynb"},"example notebook for xinference embeddings"),"\nfor a more detailed demo."))}d.isMDXComponent=!0}}]);