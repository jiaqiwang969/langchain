"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[51749],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),h=r,m=u["".concat(l,".").concat(h)]||u[h]||d[h]||o;return n?a.createElement(m,i(i({ref:t},c),{},{components:n})):a.createElement(m,i({ref:t},c))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},22975:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(87462),r=(n(67294),n(3905));const o={},i="Comparing Chain Outputs",s={unversionedId:"guides/evaluation/examples/comparisons",id:"guides/evaluation/examples/comparisons",title:"Comparing Chain Outputs",description:'Suppose you have two different prompts (or LLMs). How do you know which will generate "better" results?',source:"@site/docs/guides/evaluation/examples/comparisons.md",sourceDirName:"guides/evaluation/examples",slug:"/guides/evaluation/examples/comparisons",permalink:"/langchain/docs/guides/evaluation/examples/comparisons",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Examples",permalink:"/langchain/docs/guides/evaluation/examples/"},next:{title:"Fallbacks",permalink:"/langchain/docs/guides/fallbacks"}},l={},p=[{value:"Step 1. Create the Evaluator",id:"step-1-create-the-evaluator",level:3},{value:"Step 2. Select Dataset",id:"step-2-select-dataset",level:3},{value:"Step 3. Define Models to Compare",id:"step-3-define-models-to-compare",level:3},{value:"Step 4. Generate Responses",id:"step-4-generate-responses",level:3},{value:"Step 5. Evaluate Pairs",id:"step-5-evaluate-pairs",level:2},{value:"Estimate Confidence Intervals",id:"estimate-confidence-intervals",level:3}],c=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var u;const d={toc:p},h="wrapper";function m(e){let{components:t,...n}=e;return(0,r.kt)(h,(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"comparing-chain-outputs"},"Comparing Chain Outputs"),(0,r.kt)("p",null,'Suppose you have two different prompts (or LLMs). How do you know which will generate "better" results?'),(0,r.kt)("p",null,"One automated way to predict the preferred configuration is to use a ",(0,r.kt)("inlineCode",{parentName:"p"},"PairwiseStringEvaluator")," like the ",(0,r.kt)("inlineCode",{parentName:"p"},"PairwiseStringEvalChain"),(0,r.kt)("a",{name:"cite_ref-1"}),(0,r.kt)("a",{parentName:"p",href:"#cite_note-1"},(0,r.kt)("sup",null,"[1]")),". This chain prompts an LLM to select which output is preferred, given a specific input."),(0,r.kt)("p",null,"For this evaluation, we will need 3 things:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"An evaluator"),(0,r.kt)("li",{parentName:"ol"},"A dataset of inputs"),(0,r.kt)("li",{parentName:"ol"},"2 (or more) LLMs, Chains, or Agents to compare")),(0,r.kt)("p",null,"Then we will aggregate the restults to determine the preferred model."),(0,r.kt)("h3",{id:"step-1-create-the-evaluator"},"Step 1. Create the Evaluator"),(0,r.kt)("p",null,"In this example, you will use gpt-4 to select which output is preferred."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "load_evaluator", "source": "langchain.evaluation", "docs": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.loading.load_evaluator.html", "title": "Comparing Chain Outputs"}]--\x3e\nfrom langchain.evaluation import load_evaluator\n\neval_chain = load_evaluator("pairwise_string")\n')),(0,r.kt)("h3",{id:"step-2-select-dataset"},"Step 2. Select Dataset"),(0,r.kt)("p",null,"If you already have real usage data for your LLM, you can use a representative sample. More examples\nprovide more reliable results. We will use some example queries someone might have about how to use langchain here."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "load_dataset", "source": "langchain.evaluation.loading", "docs": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.loading.load_dataset.html", "title": "Comparing Chain Outputs"}]--\x3e\nfrom langchain.evaluation.loading import load_dataset\n\ndataset = load_dataset("langchain-howto-queries")\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    Found cached dataset parquet (/Users/wfh/.cache/huggingface/datasets/LangChainDatasets___parquet/LangChainDatasets--langchain-howto-queries-bbb748bbee7e77aa/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n\n\n\n      0%|          | 0/1 [00:00<?, ?it/s]\n"))),(0,r.kt)("h3",{id:"step-3-define-models-to-compare"},"Step 3. Define Models to Compare"),(0,r.kt)("p",null,"We will be comparing two agents in this case."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "initialize_agent", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/agents/langchain.agents.initialize.initialize_agent.html", "title": "Comparing Chain Outputs"}, {"imported": "Tool", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/tools/langchain.tools.base.Tool.html", "title": "Comparing Chain Outputs"}, {"imported": "AgentType", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html", "title": "Comparing Chain Outputs"}, {"imported": "ChatOpenAI", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html", "title": "Comparing Chain Outputs"}]--\x3e\nfrom langchain import SerpAPIWrapper\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.agents import AgentType\nfrom langchain.chat_models import ChatOpenAI\n\n\n# Initialize the language model\n# You can add your own OpenAI API key by adding openai_api_key="<your_api_key>"\nllm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613")\n\n# Initialize the SerpAPIWrapper for search functionality\n# Replace <your_api_key> in openai_api_key="<your_api_key>" with your actual SerpAPI key.\nsearch = SerpAPIWrapper()\n\n# Define a list of tools offered by the agent\ntools = [\n    Tool(\n        name="Search",\n        func=search.run,\n        coroutine=search.arun,\n        description="Useful when you need to answer questions about current events. You should ask targeted questions.",\n    ),\n]\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"functions_agent = initialize_agent(\n    tools, llm, agent=AgentType.OPENAI_MULTI_FUNCTIONS, verbose=False\n)\nconversations_agent = initialize_agent(\n    tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=False\n)\n")),(0,r.kt)("h3",{id:"step-4-generate-responses"},"Step 4. Generate Responses"),(0,r.kt)("p",null,"We will generate outputs for each of the models before evaluating them."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from tqdm.notebook import tqdm\nimport asyncio\n\nresults = []\nagents = [functions_agent, conversations_agent]\nconcurrency_level = 6  # How many concurrent agents to run. May need to decrease if OpenAI is rate limiting.\n\n# We will only run the first 20 examples of this dataset to speed things up\n# This will lead to larger confidence intervals downstream.\nbatch = []\nfor example in tqdm(dataset[:20]):\n    batch.extend([agent.acall(example["inputs"]) for agent in agents])\n    if len(batch) >= concurrency_level:\n        batch_results = await asyncio.gather(*batch, return_exceptions=True)\n        results.extend(list(zip(*[iter(batch_results)] * 2)))\n        batch = []\nif batch:\n    batch_results = await asyncio.gather(*batch, return_exceptions=True)\n    results.extend(list(zip(*[iter(batch_results)] * 2)))\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"      0%|          | 0/20 [00:00<?, ?it/s]\n"))),(0,r.kt)("h2",{id:"step-5-evaluate-pairs"},"Step 5. Evaluate Pairs"),(0,r.kt)("p",null,"Now it's time to evaluate the results. For each agent response, run the evaluation chain to select which output is preferred (or return a tie)."),(0,r.kt)("p",null,"Randomly select the input order to reduce the likelihood that one model will be preferred just because it is presented first."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import random\n\n\ndef predict_preferences(dataset, results) -> list:\n    preferences = []\n\n    for example, (res_a, res_b) in zip(dataset, results):\n        input_ = example["inputs"]\n        # Flip a coin to reduce persistent position bias\n        if random.random() < 0.5:\n            pred_a, pred_b = res_a, res_b\n            a, b = "a", "b"\n        else:\n            pred_a, pred_b = res_b, res_a\n            a, b = "b", "a"\n        eval_res = eval_chain.evaluate_string_pairs(\n            prediction=pred_a["output"] if isinstance(pred_a, dict) else str(pred_a),\n            prediction_b=pred_b["output"] if isinstance(pred_b, dict) else str(pred_b),\n            input=input_,\n        )\n        if eval_res["value"] == "A":\n            preferences.append(a)\n        elif eval_res["value"] == "B":\n            preferences.append(b)\n        else:\n            preferences.append(None)  # No preference\n    return preferences\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"preferences = predict_preferences(dataset, results)\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Print out the ratio of preferences.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from collections import Counter\n\nname_map = {\n    "a": "OpenAI Functions Agent",\n    "b": "Structured Chat Agent",\n}\ncounts = Counter(preferences)\npref_ratios = {k: v / len(preferences) for k, v in counts.items()}\nfor k, v in pref_ratios.items():\n    print(f"{name_map.get(k)}: {v:.2%}")\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    OpenAI Functions Agent: 95.00%\n    None: 5.00%\n"))),(0,r.kt)("h3",{id:"estimate-confidence-intervals"},"Estimate Confidence Intervals"),(0,r.kt)("p",null,'The results seem pretty clear, but if you want to have a better sense of how confident we are, that model "A" (the OpenAI Functions Agent) is the preferred model, we can calculate confidence intervals. '),(0,r.kt)("p",null,"Below, use the Wilson score to estimate the confidence interval."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from math import sqrt\n\n\ndef wilson_score_interval(\n    preferences: list, which: str = "a", z: float = 1.96\n) -> tuple:\n    """Estimate the confidence interval using the Wilson score.\n\n    See: https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval\n    for more details, including when to use it and when it should not be used.\n    """\n    total_preferences = preferences.count("a") + preferences.count("b")\n    n_s = preferences.count(which)\n\n    if total_preferences == 0:\n        return (0, 0)\n\n    p_hat = n_s / total_preferences\n\n    denominator = 1 + (z**2) / total_preferences\n    adjustment = (z / denominator) * sqrt(\n        p_hat * (1 - p_hat) / total_preferences\n        + (z**2) / (4 * total_preferences * total_preferences)\n    )\n    center = (p_hat + (z**2) / (2 * total_preferences)) / denominator\n    lower_bound = min(max(center - adjustment, 0.0), 1.0)\n    upper_bound = min(max(center + adjustment, 0.0), 1.0)\n\n    return (lower_bound, upper_bound)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"for which_, name in name_map.items():\n    low, high = wilson_score_interval(preferences, which=which_)\n    print(\n        f'The \"{name}\" would be preferred between {low:.2%} and {high:.2%} percent of the time (with 95% confidence).'\n    )\n")),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'    The "OpenAI Functions Agent" would be preferred between 83.18% and 100.00% percent of the time (with 95% confidence).\n    The "Structured Chat Agent" would be preferred between 0.00% and 16.82% percent of the time (with 95% confidence).\n'))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Print out the p-value.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from scipy import stats\n\npreferred_model = max(pref_ratios, key=pref_ratios.get)\nsuccesses = preferences.count(preferred_model)\nn = len(preferences) - preferences.count(None)\np_value = stats.binom_test(successes, n, p=0.5, alternative="two-sided")\nprint(\n    f"""The p-value is {p_value:.5f}. If the null hypothesis is true (i.e., if the selected eval chain actually has no preference between the models),\nthen there is a {p_value:.5%} chance of observing the {name_map.get(preferred_model)} be preferred at least {successes}\ntimes out of {n} trials."""\n)\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    The p-value is 0.00000. If the null hypothesis is true (i.e., if the selected eval chain actually has no preference between the models),\n    then there is a 0.00038% chance of observing the OpenAI Functions Agent be preferred at least 19\n    times out of 19 trials.\n\n\n    /var/folders/gf/6rnp_mbx5914kx7qmmh7xzmw0000gn/T/ipykernel_15978/384907688.py:6: DeprecationWarning: 'binom_test' is deprecated in favour of 'binomtest' from version 1.7.0 and will be removed in Scipy 1.12.0.\n      p_value = stats.binom_test(successes, n, p=0.5, alternative=\"two-sided\")\n"))),(0,r.kt)("a",{name:"cite_note-1"}),'_1. Note: Automated evals are still an open research topic and are best used alongside other evaluation approaches. LLM preferences exhibit biases, including banal ones like the order of outputs. In choosing preferences, "ground truth" may not be taken into account, which may lead to scores that aren\'t grounded in utility._')}m.isMDXComponent=!0}}]);