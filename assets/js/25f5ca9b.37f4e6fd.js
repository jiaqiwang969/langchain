"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[42234],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,p=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=s(n),h=r,m=u["".concat(p,".").concat(h)]||u[h]||d[h]||o;return n?a.createElement(m,i(i({ref:t},c),{},{components:n})):a.createElement(m,i({ref:t},c))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=h;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l[u]="string"==typeof e?e:r,i[1]=l;for(var s=2;s<o;s++)i[s]=n[s];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},48517:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>s});var a=n(87462),r=(n(67294),n(3905));const o={sidebar_position:1,title:"Extraction"},i=void 0,l={unversionedId:"use_cases/extraction",id:"use_cases/extraction",title:"Extraction",description:"Open In Collab",source:"@site/docs/use_cases/extraction.md",sourceDirName:"use_cases",slug:"/use_cases/extraction",permalink:"/langchain/docs/use_cases/extraction",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Extraction"},sidebar:"use_cases",previous:{title:"Code understanding",permalink:"/langchain/docs/use_cases/code_understanding"},next:{title:"Summarization",permalink:"/langchain/docs/use_cases/summarization"}},p={},s=[{value:"Use case",id:"use-case",level:2},{value:"Overview",id:"overview",level:2},{value:"Quickstart",id:"quickstart",level:2},{value:"Option 1: OpenAI functions",id:"option-1-openai-functions",level:2},{value:"Looking under the hood",id:"looking-under-the-hood",level:3},{value:"Multiple entity types",id:"multiple-entity-types",level:3},{value:"Unrelated entities",id:"unrelated-entities",level:3},{value:"Extra information",id:"extra-information",level:3},{value:"Pydantic",id:"pydantic",level:3},{value:"Option 2: Parsing",id:"option-2-parsing",level:2},{value:"Pydantic",id:"pydantic-1",level:3},{value:"Going deeper",id:"going-deeper",level:3}],c=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var u;const d={toc:s},h="wrapper";function m(e){let{components:t,...o}=e;return(0,r.kt)(h,(0,a.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/extras/use_cases/extraction.ipynb"},(0,r.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Collab"}))),(0,r.kt)("h2",{id:"use-case"},"Use case"),(0,r.kt)("p",null,"Getting structured output from raw LLM generations is hard."),(0,r.kt)("p",null,"For example, suppose you need the model output formatted with a specific schema for:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Extracting a structured row to insert into a database "),(0,r.kt)("li",{parentName:"ul"},"Extracting API parameters"),(0,r.kt)("li",{parentName:"ul"},"Extracting different parts of a user query (e.g., for semantic vs keyword search)")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Image description",src:n(27892).Z,width:"1687",height:"629"})),(0,r.kt)("h2",{id:"overview"},"Overview"),(0,r.kt)("p",null,"There are two primary approaches for this:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"Functions"),": Some LLMs can call ",(0,r.kt)("a",{parentName:"p",href:"https://openai.com/blog/function-calling-and-other-api-updates"},"functions")," to extract arbitrary entities from LLM responses.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"Parsing"),": ",(0,r.kt)("a",{parentName:"p",href:"/docs/modules/model_io/output_parsers/"},"Output parsers")," are classes that structure LLM responses. "))),(0,r.kt)("p",null,"Only some LLMs support functions (e.g., OpenAI), and they are more general than parsers. "),(0,r.kt)("p",null,"Parsers extract precisely what is enumerated in a provided schema (e.g., specific attributes of a person)."),(0,r.kt)("p",null,"Functions can infer things beyond of a provided schema (e.g., attributes about a person that you did not ask for)."),(0,r.kt)("h2",{id:"quickstart"},"Quickstart"),(0,r.kt)("p",null,"OpenAI functions are one way to get started with extraction."),(0,r.kt)("p",null,"Define a schema that specifies the properties we want to extract from the LLM output."),(0,r.kt)("p",null,"Then, we can use ",(0,r.kt)("inlineCode",{parentName:"p"},"create_extraction_chain")," to extract our desired schema using an OpenAI function call."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"pip install langchain openai \n\n# Set env var OPENAI_API_KEY or load from a .env file:\n# import dotenv\n# dotenv.load_dotenv()\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chat_models import ChatOpenAI\nfrom langchain.chains import create_extraction_chain\n\n# Schema\nschema = {\n    "properties": {\n        "name": {"type": "string"},\n        "height": {"type": "integer"},\n        "hair_color": {"type": "string"},\n    },\n    "required": ["name", "height"],\n}\n\n# Input \ninp = """Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde."""\n\n# Run chain\nllm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")\nchain = create_extraction_chain(schema, llm)\nchain.run(inp)\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    [{'name': 'Alex', 'height': 5, 'hair_color': 'blonde'},\n     {'name': 'Claudia', 'height': 6, 'hair_color': 'brunette'}]\n"))),(0,r.kt)("h2",{id:"option-1-openai-functions"},"Option 1: OpenAI functions"),(0,r.kt)("h3",{id:"looking-under-the-hood"},"Looking under the hood"),(0,r.kt)("p",null,"Let's dig into what is happening when we call ",(0,r.kt)("inlineCode",{parentName:"p"},"create_extraction_chain"),"."),(0,r.kt)("p",null,"The ",(0,r.kt)("a",{parentName:"p",href:"https://smith.langchain.com/public/72bc3205-7743-4ca6-929a-966a9d4c2a77/r"},"LangSmith trace")," shows that we call the function ",(0,r.kt)("inlineCode",{parentName:"p"},"information_extraction")," on the input string, ",(0,r.kt)("inlineCode",{parentName:"p"},"inp"),"."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Image description",src:n(41092).Z,width:"1591",height:"909"})),(0,r.kt)("p",null,"This ",(0,r.kt)("inlineCode",{parentName:"p"},"information_extraction")," function is defined ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/openai_functions/extraction.py"},"here")," and returns a dict."),(0,r.kt)("p",null,"We can see the ",(0,r.kt)("inlineCode",{parentName:"p"},"dict")," in the model output:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},' {\n      "info": [\n        {\n          "name": "Alex",\n          "height": 5,\n          "hair_color": "blonde"\n        },\n        {\n          "name": "Claudia",\n          "height": 6,\n          "hair_color": "brunette"\n        }\n      ]\n    }\n')),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"create_extraction_chain")," then parses the raw LLM output for us using ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/langchain-ai/langchain/blob/f81e613086d211327b67b0fb591fd4d5f9a85860/libs/langchain/langchain/chains/openai_functions/extraction.py#L62"},(0,r.kt)("inlineCode",{parentName:"a"},"JsonKeyOutputFunctionsParser")),"."),(0,r.kt)("p",null,"This results in the list of JSON objects returned by the chain above:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"[{'name': 'Alex', 'height': 5, 'hair_color': 'blonde'},\n {'name': 'Claudia', 'height': 6, 'hair_color': 'brunette'}]\n")),(0,r.kt)("h3",{id:"multiple-entity-types"},"Multiple entity types"),(0,r.kt)("p",null,"We can extend this further."),(0,r.kt)("p",null,"Let's say we want to differentiate between dogs and people."),(0,r.kt)("p",null,"We can add ",(0,r.kt)("inlineCode",{parentName:"p"},"person_")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"dog_")," prefixes for each property"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'schema = {\n    "properties": {\n        "person_name": {"type": "string"},\n        "person_height": {"type": "integer"},\n        "person_hair_color": {"type": "string"},\n        "dog_name": {"type": "string"},\n        "dog_breed": {"type": "string"},\n    },\n    "required": ["person_name", "person_height"],\n}\n\nchain = create_extraction_chain(schema, llm)\n\ninp = """Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\nAlex\'s dog Frosty is a labrador and likes to play hide and seek."""\n\nchain.run(inp)\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    [{'person_name': 'Alex',\n      'person_height': 5,\n      'person_hair_color': 'blonde',\n      'dog_name': 'Frosty',\n      'dog_breed': 'labrador'},\n     {'person_name': 'Claudia',\n      'person_height': 6,\n      'person_hair_color': 'brunette'}]\n"))),(0,r.kt)("h3",{id:"unrelated-entities"},"Unrelated entities"),(0,r.kt)("p",null,"If we use ",(0,r.kt)("inlineCode",{parentName:"p"},"required: []"),", we allow the model to return ",(0,r.kt)("strong",{parentName:"p"},"only")," person attributes or ",(0,r.kt)("strong",{parentName:"p"},"only")," dog attributes for a single entity (person or dog)."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'schema = {\n    "properties": {\n        "person_name": {"type": "string"},\n        "person_height": {"type": "integer"},\n        "person_hair_color": {"type": "string"},\n        "dog_name": {"type": "string"},\n        "dog_breed": {"type": "string"},\n    },\n    "required": [],\n}\n\nchain = create_extraction_chain(schema, llm)\n\ninp = """Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\nWillow is a German Shepherd that likes to play with other dogs and can always be found playing with Milo, a border collie that lives close by."""\n\nchain.run(inp)\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    [{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde'},\n     {'person_name': 'Claudia',\n      'person_height': 6,\n      'person_hair_color': 'brunette'},\n     {'dog_name': 'Willow', 'dog_breed': 'German Shepherd'},\n     {'dog_name': 'Milo', 'dog_breed': 'border collie'}]\n"))),(0,r.kt)("h3",{id:"extra-information"},"Extra information"),(0,r.kt)("p",null,"The power of functions (relative to using parsers alone) lies in the ability to perform sematic extraction."),(0,r.kt)("p",null,"In particular, ",(0,r.kt)("inlineCode",{parentName:"p"},"we can ask for things that are not explictly enumerated in the schema"),"."),(0,r.kt)("p",null,"Suppose we want unspecified additional information about dogs. "),(0,r.kt)("p",null,"We can use add a placeholder for unstructured extraction, ",(0,r.kt)("inlineCode",{parentName:"p"},"dog_extra_info"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'schema = {\n    "properties": {\n        "person_name": {"type": "string"},\n        "person_height": {"type": "integer"},\n        "person_hair_color": {"type": "string"},\n        "dog_name": {"type": "string"},\n        "dog_breed": {"type": "string"},\n        "dog_extra_info": {"type": "string"},\n    },\n}\n\nchain = create_extraction_chain(schema, llm)\nchain.run(inp)\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    [{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde'},\n     {'person_name': 'Claudia',\n      'person_height': 6,\n      'person_hair_color': 'brunette'},\n     {'dog_name': 'Willow',\n      'dog_breed': 'German Shepherd',\n      'dog_extra_info': 'likes to play with other dogs'},\n     {'dog_name': 'Milo',\n      'dog_breed': 'border collie',\n      'dog_extra_info': 'lives close by'}]\n"))),(0,r.kt)("p",null,"This gives us additional information about the dogs."),(0,r.kt)("h3",{id:"pydantic"},"Pydantic"),(0,r.kt)("p",null,"Pydantic is a data validation and settings management library for Python. "),(0,r.kt)("p",null,"It allows you to create data classes with attributes that are automatically validated when you instantiate an object."),(0,r.kt)("p",null,"Lets define a class with attributes annotated with types."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from typing import Optional, List\nfrom pydantic import BaseModel, Field\nfrom langchain.chains import create_extraction_chain_pydantic\n\n# Pydantic data class\nclass Properties(BaseModel):\n    person_name: str\n    person_height: int\n    person_hair_color: str\n    dog_breed: Optional[str]\n    dog_name: Optional[str]\n        \n# Extraction\nchain = create_extraction_chain_pydantic(pydantic_schema=Properties, llm=llm)\n\n# Run \ninp = """Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde."""\nchain.run(inp)\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    [Properties(person_name='Alex', person_height=5, person_hair_color='blonde', dog_breed=None, dog_name=None),\n     Properties(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)]\n"))),(0,r.kt)("p",null,"As we can see from the ",(0,r.kt)("a",{parentName:"p",href:"https://smith.langchain.com/public/fed50ae6-26bb-4235-a254-e0b7a229d10f/r"},"trace"),", we use the function ",(0,r.kt)("inlineCode",{parentName:"p"},"information_extraction"),", as above, with the Pydantic schema. "),(0,r.kt)("h2",{id:"option-2-parsing"},"Option 2: Parsing"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"/docs/modules/model_io/output_parsers/"},"Output parsers")," are classes that help structure language model responses. "),(0,r.kt)("p",null,"As shown above, they are used to parse the output of the OpenAI function calls in ",(0,r.kt)("inlineCode",{parentName:"p"},"create_extraction_chain"),"."),(0,r.kt)("p",null,"But, they can be used independent of functions."),(0,r.kt)("h3",{id:"pydantic-1"},"Pydantic"),(0,r.kt)("p",null,"Just as a above, let's parse a generation based on a Pydantic data class."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from typing import Sequence\nfrom langchain.prompts import (\n    PromptTemplate,\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.llms import OpenAI\nfrom pydantic import BaseModel, Field, validator\nfrom langchain.output_parsers import PydanticOutputParser\n\nclass Person(BaseModel):\n    person_name: str\n    person_height: int\n    person_hair_color: str\n    dog_breed: Optional[str]\n    dog_name: Optional[str]\n\nclass People(BaseModel):\n    """Identifying information about all people in a text."""\n    people: Sequence[Person]\n\n        \n# Run \nquery = """Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde."""\n\n# Set up a parser + inject instructions into the prompt template.\nparser = PydanticOutputParser(pydantic_object=People)\n\n# Prompt\nprompt = PromptTemplate(\n    template="Answer the user query.\\n{format_instructions}\\n{query}\\n",\n    input_variables=["query"],\n    partial_variables={"format_instructions": parser.get_format_instructions()},\n)\n\n# Run\n_input = prompt.format_prompt(query=query)\nmodel = OpenAI(temperature=0)\noutput = model(_input.to_string())\nparser.parse(output)\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    People(people=[Person(person_name='Alex', person_height=5, person_hair_color='blonde', dog_breed=None, dog_name=None), Person(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)])\n"))),(0,r.kt)("p",null,"We can see from the ",(0,r.kt)("a",{parentName:"p",href:"https://smith.langchain.com/public/8e3aa858-467e-46a5-aa49-5db65f0a2b9a/r"},"LangSmith trace")," that we get the same output as above."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Image description",src:n(14620).Z,width:"2027",height:"969"})),(0,r.kt)("p",null,"We can see that we provide a two-shot prompt in order to instruct the LLM to output in our desired format."),(0,r.kt)("p",null,"And, we need to do a bit more work:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Define a class that holds multiple instances of ",(0,r.kt)("inlineCode",{parentName:"li"},"Person")),(0,r.kt)("li",{parentName:"ul"},"Explicty parse the output of the LLM to the Pydantic class")),(0,r.kt)("p",null,"We can see this for other cases, too."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.prompts import (\n    PromptTemplate,\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.llms import OpenAI\nfrom pydantic import BaseModel, Field, validator\nfrom langchain.output_parsers import PydanticOutputParser\n\n# Define your desired data structure.\nclass Joke(BaseModel):\n    setup: str = Field(description="question to set up a joke")\n    punchline: str = Field(description="answer to resolve the joke")\n\n    # You can add custom validation logic easily with Pydantic.\n    @validator("setup")\n    def question_ends_with_question_mark(cls, field):\n        if field[-1] != "?":\n            raise ValueError("Badly formed question!")\n        return field\n\n# And a query intented to prompt a language model to populate the data structure.\njoke_query = "Tell me a joke."\n\n# Set up a parser + inject instructions into the prompt template.\nparser = PydanticOutputParser(pydantic_object=Joke)\n\n# Prompt\nprompt = PromptTemplate(\n    template="Answer the user query.\\n{format_instructions}\\n{query}\\n",\n    input_variables=["query"],\n    partial_variables={"format_instructions": parser.get_format_instructions()},\n)\n\n# Run\n_input = prompt.format_prompt(query=joke_query)\nmodel = OpenAI(temperature=0)\noutput = model(_input.to_string())\nparser.parse(output)\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!')\n"))),(0,r.kt)("p",null,"As we can see, we get an output of the ",(0,r.kt)("inlineCode",{parentName:"p"},"Joke")," class, which respects our originally desired schema: 'setup' and 'punchline'."),(0,r.kt)("p",null,"We can look at the ",(0,r.kt)("a",{parentName:"p",href:"https://smith.langchain.com/public/69f11d41-41be-4319-93b0-6d0eda66e969/r"},"LangSmith trace")," to see exactly what is going on under the hood."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Image description",src:n(36467).Z,width:"2031",height:"550"})),(0,r.kt)("h3",{id:"going-deeper"},"Going deeper"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"The ",(0,r.kt)("a",{parentName:"li",href:"/docs/modules/model_io/output_parsers/"},"output parser")," documentation includes various parser examples for specific types (e.g., lists, datetimne, enum, etc).  "),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/integrations/llms/jsonformer_experimental"},"JSONFormer")," offers another way for structured decoding of a subset of the JSON Schema."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://eyurtsev.github.io/kor/"},"Kor")," is another library for extraction where schema and examples can be provided to the LLM.")))}m.isMDXComponent=!0},27892:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/extraction-f94dadc4bef17c7d73ce5d056eab70c8.png"},41092:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/extraction_trace_function-60b77e0eefd08ca8190f3ab4fd16b180.png"},14620:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/extraction_trace_function_2-95aa9ea2bc32469a0bbd247ad02fc809.png"},36467:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/extraction_trace_joke-2f2622f7b19b7f23b306ef3d457d9492.png"}}]);