"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8268],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>k});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=p(n),m=r,k=u["".concat(s,".").concat(m)]||u[m]||h[m]||o;return n?a.createElement(k,i(i({ref:t},c),{},{components:n})):a.createElement(k,i({ref:t},c))}));function k(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},78105:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>k,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var a=n(87462),r=(n(67294),n(3905));const o={},i="Split by tokens",l={unversionedId:"modules/data_connection/document_transformers/text_splitters/split_by_token",id:"modules/data_connection/document_transformers/text_splitters/split_by_token",title:"Split by tokens",description:"Language models have a token limit. You should not exceed the token limit. When you split your text into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model.",source:"@site/docs/modules/data_connection/document_transformers/text_splitters/split_by_token.md",sourceDirName:"modules/data_connection/document_transformers/text_splitters",slug:"/modules/data_connection/document_transformers/text_splitters/split_by_token",permalink:"/langchain/docs/modules/data_connection/document_transformers/text_splitters/split_by_token",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Recursively split by character",permalink:"/langchain/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter"},next:{title:"Lost in the middle: The problem with long contexts",permalink:"/langchain/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder"}},s={},p=[{value:"tiktoken",id:"tiktoken",level:2},{value:"spaCy",id:"spacy",level:2},{value:"SentenceTransformers",id:"sentencetransformers",level:2},{value:"NLTK",id:"nltk",level:2},{value:"Hugging Face tokenizer",id:"hugging-face-tokenizer",level:2}],c=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var u;const h={toc:p},m="wrapper";function k(e){let{components:t,...n}=e;return(0,r.kt)(m,(0,a.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"split-by-tokens"},"Split by tokens"),(0,r.kt)("p",null,"Language models have a token limit. You should not exceed the token limit. When you split your text into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model. "),(0,r.kt)("h2",{id:"tiktoken"},"tiktoken"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/openai/tiktoken"},"tiktoken")," is a fast ",(0,r.kt)("inlineCode",{parentName:"p"},"BPE")," tokenizer created by ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenAI"),".")),(0,r.kt)("p",null,"We can use it to estimate tokens used. It will probably be more accurate for the OpenAI models."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"How the text is split: by character passed in."),(0,r.kt)("li",{parentName:"ol"},"How the chunk size is measured: by ",(0,r.kt)("inlineCode",{parentName:"li"},"tiktoken")," tokenizer.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"#!pip install tiktoken\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "CharacterTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html", "title": "Split by tokens "}]--\x3e\n# This is a long document we can split up.\nwith open("../../../state_of_the_union.txt") as f:\n    state_of_the_union = f.read()\nfrom langchain.text_splitter import CharacterTextSplitter\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=100, chunk_overlap=0\n)\ntexts = text_splitter.split_text(state_of_the_union)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"print(texts[0])\n")),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n    \n    Last year COVID-19 kept us apart. This year we are finally together again. \n    \n    Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n    \n    With a duty to one another to the American people to the Constitution.\n"))),(0,r.kt)("p",null,"Note that if we use ",(0,r.kt)("inlineCode",{parentName:"p"},"CharacterTextSplitter.from_tiktoken_encoder"),", text is only split by ",(0,r.kt)("inlineCode",{parentName:"p"},"CharacterTextSplitter")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"tiktoken")," tokenizer is used to merge splits. It means that split can be larger than chunk size measured by ",(0,r.kt)("inlineCode",{parentName:"p"},"tiktoken")," tokenizer. We can use ",(0,r.kt)("inlineCode",{parentName:"p"},"RecursiveCharacterTextSplitter.from_tiktoken_encoder")," to make sure splits are not larger than chunk size of tokens allowed by the language model, where each split will be recursively split if it has a larger size."),(0,r.kt)("p",null,"We can also load a tiktoken splitter directly, which ensure each split is smaller than chunk size."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "TokenTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.TokenTextSplitter.html", "title": "Split by tokens "}]--\x3e\nfrom langchain.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n\ntexts = text_splitter.split_text(state_of_the_union)\nprint(texts[0])\n')),(0,r.kt)("h2",{id:"spacy"},"spaCy"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("a",{parentName:"p",href:"https://spacy.io/"},"spaCy")," is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.")),(0,r.kt)("p",null,"Another alternative to ",(0,r.kt)("inlineCode",{parentName:"p"},"NLTK")," is to use ",(0,r.kt)("a",{parentName:"p",href:"https://spacy.io/api/tokenizer"},"spaCy tokenizer"),"."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"How the text is split: by ",(0,r.kt)("inlineCode",{parentName:"li"},"spaCy")," tokenizer."),(0,r.kt)("li",{parentName:"ol"},"How the chunk size is measured: by number of characters.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"#!pip install spacy\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# This is a long document we can split up.\nwith open("../../../state_of_the_union.txt") as f:\n    state_of_the_union = f.read()\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "SpacyTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.SpacyTextSplitter.html", "title": "Split by tokens "}]--\x3e\nfrom langchain.text_splitter import SpacyTextSplitter\n\ntext_splitter = SpacyTextSplitter(chunk_size=1000)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"texts = text_splitter.split_text(state_of_the_union)\nprint(texts[0])\n")),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.\n    \n    Members of Congress and the Cabinet.\n    \n    Justices of the Supreme Court.\n    \n    My fellow Americans.  \n    \n    \n    \n    Last year COVID-19 kept us apart.\n    \n    This year we are finally together again. \n    \n    \n    \n    Tonight, we meet as Democrats Republicans and Independents.\n    \n    But most importantly as Americans. \n    \n    \n    \n    With a duty to one another to the American people to the Constitution. \n    \n    \n    \n    And with an unwavering resolve that freedom will always triumph over tyranny. \n    \n    \n    \n    Six days ago, Russia\u2019s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.\n    \n    But he badly miscalculated. \n    \n    \n    \n    He thought he could roll into Ukraine and the world would roll over.\n    \n    Instead he met a wall of strength he never imagined. \n    \n    \n    \n    He met the Ukrainian people. \n    \n    \n    \n    From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\n"))),(0,r.kt)("h2",{id:"sentencetransformers"},"SentenceTransformers"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"SentenceTransformersTokenTextSplitter")," is a specialized text splitter for use with the sentence-transformer models. The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "SentenceTransformersTokenTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.SentenceTransformersTokenTextSplitter.html", "title": "Split by tokens "}]--\x3e\nfrom langchain.text_splitter import SentenceTransformersTokenTextSplitter\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)\ntext = "Lorem "\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"count_start_and_stop_tokens = 2\ntext_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokens\nprint(text_token_count)\n")),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    2\n"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'token_multiplier = splitter.maximum_tokens_per_chunk // text_token_count + 1\n\n# `text_to_split` does not fit in a single chunk\ntext_to_split = text * token_multiplier\n\nprint(f"tokens in text to split: {splitter.count_tokens(text=text_to_split)}")\n')),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    tokens in text to split: 514\n"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"text_chunks = splitter.split_text(text=text_to_split)\n\nprint(text_chunks[1])\n")),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    lorem\n"))),(0,r.kt)("h2",{id:"nltk"},"NLTK"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Natural_Language_Toolkit"},"The Natural Language Toolkit"),", or more commonly ",(0,r.kt)("a",{parentName:"p",href:"https://www.nltk.org/"},"NLTK"),", is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.")),(0,r.kt)("p",null,'Rather than just splitting on "\\n\\n", we can use ',(0,r.kt)("inlineCode",{parentName:"p"},"NLTK")," to split based on ",(0,r.kt)("a",{parentName:"p",href:"https://www.nltk.org/api/nltk.tokenize.html"},"NLTK tokenizers"),"."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"How the text is split: by ",(0,r.kt)("inlineCode",{parentName:"li"},"NLTK")," tokenizer."),(0,r.kt)("li",{parentName:"ol"},"How the chunk size is measured: by number of characters.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# pip install nltk\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# This is a long document we can split up.\nwith open("../../../state_of_the_union.txt") as f:\n    state_of_the_union = f.read()\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "NLTKTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.NLTKTextSplitter.html", "title": "Split by tokens "}]--\x3e\nfrom langchain.text_splitter import NLTKTextSplitter\n\ntext_splitter = NLTKTextSplitter(chunk_size=1000)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"texts = text_splitter.split_text(state_of_the_union)\nprint(texts[0])\n")),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.\n    \n    Members of Congress and the Cabinet.\n    \n    Justices of the Supreme Court.\n    \n    My fellow Americans.\n    \n    Last year COVID-19 kept us apart.\n    \n    This year we are finally together again.\n    \n    Tonight, we meet as Democrats Republicans and Independents.\n    \n    But most importantly as Americans.\n    \n    With a duty to one another to the American people to the Constitution.\n    \n    And with an unwavering resolve that freedom will always triumph over tyranny.\n    \n    Six days ago, Russia\u2019s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.\n    \n    But he badly miscalculated.\n    \n    He thought he could roll into Ukraine and the world would roll over.\n    \n    Instead he met a wall of strength he never imagined.\n    \n    He met the Ukrainian people.\n    \n    From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\n    \n    Groups of citizens blocking tanks with their bodies.\n"))),(0,r.kt)("h2",{id:"hugging-face-tokenizer"},"Hugging Face tokenizer"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("a",{parentName:"p",href:"https://huggingface.co/docs/tokenizers/index"},"Hugging Face")," has many tokenizers.")),(0,r.kt)("p",null,"We use Hugging Face tokenizer, the ",(0,r.kt)("a",{parentName:"p",href:"https://huggingface.co/Ransaka/gpt2-tokenizer-fast"},"GPT2TokenizerFast")," to count the text length in tokens."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"How the text is split: by character passed in."),(0,r.kt)("li",{parentName:"ol"},"How the chunk size is measured: by number of tokens calculated by the ",(0,r.kt)("inlineCode",{parentName:"li"},"Hugging Face")," tokenizer.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from transformers import GPT2TokenizerFast\n\ntokenizer = GPT2TokenizerFast.from_pretrained("gpt2")\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "CharacterTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html", "title": "Split by tokens "}]--\x3e\n# This is a long document we can split up.\nwith open("../../../state_of_the_union.txt") as f:\n    state_of_the_union = f.read()\nfrom langchain.text_splitter import CharacterTextSplitter\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n    tokenizer, chunk_size=100, chunk_overlap=0\n)\ntexts = text_splitter.split_text(state_of_the_union)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"print(texts[0])\n")),(0,r.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n    \n    Last year COVID-19 kept us apart. This year we are finally together again. \n    \n    Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n    \n    With a duty to one another to the American people to the Constitution.\n"))))}k.isMDXComponent=!0}}]);