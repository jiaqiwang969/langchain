"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[56050],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),u=p(n),m=o,h=u["".concat(s,".").concat(m)]||u[m]||d[m]||r;return n?a.createElement(h,l(l({ref:t},c),{},{components:n})):a.createElement(h,l({ref:t},c))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,l=new Array(r);l[0]=m;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[u]="string"==typeof e?e:o,l[1]=i;for(var p=2;p<r;p++)l[p]=n[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},84660:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>p});var a=n(87462),o=(n(67294),n(3905));const r={},l="WhyLabs",i={unversionedId:"integrations/providers/whylabs_profiling",id:"integrations/providers/whylabs_profiling",title:"WhyLabs",description:"WhyLabs is an observability platform designed to monitor data pipelines and ML applications for data quality regressions, data drift, and model performance degradation. Built on top of an open-source package called whylogs, the platform enables Data Scientists and Engineers to:",source:"@site/docs/integrations/providers/whylabs_profiling.md",sourceDirName:"integrations/providers",slug:"/integrations/providers/whylabs_profiling",permalink:"/langchain/docs/integrations/providers/whylabs_profiling",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"WhatsApp",permalink:"/langchain/docs/integrations/providers/whatsapp"},next:{title:"Wikipedia",permalink:"/langchain/docs/integrations/providers/wikipedia"}},s={},p=[{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"Callbacks",id:"callbacks",level:2}],c=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var u;const d={toc:p},m="wrapper";function h(e){let{components:t,...n}=e;return(0,o.kt)(m,(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"whylabs"},"WhyLabs"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("a",{parentName:"p",href:"https://docs.whylabs.ai/docs/"},"WhyLabs")," is an observability platform designed to monitor data pipelines and ML applications for data quality regressions, data drift, and model performance degradation. Built on top of an open-source package called ",(0,o.kt)("inlineCode",{parentName:"p"},"whylogs"),", the platform enables Data Scientists and Engineers to:"),(0,o.kt)("ul",{parentName:"blockquote"},(0,o.kt)("li",{parentName:"ul"},"Set up in minutes: Begin generating statistical profiles of any dataset using whylogs, the lightweight open-source library."),(0,o.kt)("li",{parentName:"ul"},"Upload dataset profiles to the WhyLabs platform for centralized and customizable monitoring/alerting of dataset features as well as model inputs, outputs, and performance."),(0,o.kt)("li",{parentName:"ul"},"Integrate seamlessly: interoperable with any data pipeline, ML infrastructure, or framework. Generate real-time insights into your existing data flow. See more about our integrations here."),(0,o.kt)("li",{parentName:"ul"},"Scale to terabytes: handle your large-scale data, keeping compute requirements low. Integrate with either batch or streaming data pipelines."),(0,o.kt)("li",{parentName:"ul"},"Maintain data privacy: WhyLabs relies statistical profiles created via whylogs so your actual data never leaves your environment!\nEnable observability to detect inputs and LLM issues faster, deliver continuous improvements, and avoid costly incidents."))),(0,o.kt)("h2",{id:"installation-and-setup"},"Installation and Setup"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"%pip install langkit openai langchain\n")),(0,o.kt)("p",null,"Make sure to set the required API keys and config required to send telemetry to WhyLabs:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"WhyLabs API Key: ",(0,o.kt)("a",{parentName:"li",href:"https://whylabs.ai/whylabs-free-sign-up"},"https://whylabs.ai/whylabs-free-sign-up")),(0,o.kt)("li",{parentName:"ul"},"Org and Dataset ",(0,o.kt)("a",{parentName:"li",href:"https://docs.whylabs.ai/docs/whylabs-onboarding#upload-a-profile-to-a-whylabs-project"},"https://docs.whylabs.ai/docs/whylabs-onboarding")),(0,o.kt)("li",{parentName:"ul"},"OpenAI: ",(0,o.kt)("a",{parentName:"li",href:"https://platform.openai.com/account/api-keys"},"https://platform.openai.com/account/api-keys"))),(0,o.kt)("p",null,"Then you can set them like this:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nos.environ["OPENAI_API_KEY"] = ""\nos.environ["WHYLABS_DEFAULT_ORG_ID"] = ""\nos.environ["WHYLABS_DEFAULT_DATASET_ID"] = ""\nos.environ["WHYLABS_API_KEY"] = ""\n')),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("em",{parentName:"p"},"Note"),": the callback supports directly passing in these variables to the callback, when no auth is directly passed in it will default to the environment. Passing in auth directly allows for writing profiles to multiple projects or organizations in WhyLabs.")),(0,o.kt)("h2",{id:"callbacks"},"Callbacks"),(0,o.kt)("p",null,"Here's a single LLM integration with OpenAI, which will log various out of the box metrics and send telemetry to WhyLabs for monitoring."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "WhyLabsCallbackHandler", "source": "langchain.callbacks", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.whylabs_callback.WhyLabsCallbackHandler.html", "title": "WhyLabs"}]--\x3e\nfrom langchain.callbacks import WhyLabsCallbackHandler\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "OpenAI", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html", "title": "WhyLabs"}]--\x3e\nfrom langchain.llms import OpenAI\n\nwhylabs = WhyLabsCallbackHandler.from_params()\nllm = OpenAI(temperature=0, callbacks=[whylabs])\n\nresult = llm.generate(["Hello, World!"])\nprint(result)\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    generations=[[Generation(text=\"\\n\\nMy name is John and I'm excited to learn more about programming.\", generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 20, 'prompt_tokens': 4, 'completion_tokens': 16}, 'model_name': 'text-davinci-003'}\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'result = llm.generate(\n    [\n        "Can you give me 3 SSNs so I can understand the format?",\n        "Can you give me 3 fake email addresses?",\n        "Can you give me 3 fake US mailing addresses?",\n    ]\n)\nprint(result)\n# you don\'t need to call close to write profiles to WhyLabs, upload will occur periodically, but to demo let\'s not wait.\nwhylabs.close()\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    generations=[[Generation(text='\\n\\n1. 123-45-6789\\n2. 987-65-4321\\n3. 456-78-9012', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n1. johndoe@example.com\\n2. janesmith@example.com\\n3. johnsmith@example.com', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n1. 123 Main Street, Anytown, USA 12345\\n2. 456 Elm Street, Nowhere, USA 54321\\n3. 789 Pine Avenue, Somewhere, USA 98765', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 137, 'prompt_tokens': 33, 'completion_tokens': 104}, 'model_name': 'text-davinci-003'}\n"))))}h.isMDXComponent=!0}}]);