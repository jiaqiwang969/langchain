"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[66600],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),m=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=m(e.components);return a.createElement(s.Provider,{value:t},e.children)},p="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),p=m(n),g=i,h=p["".concat(s,".").concat(g)]||p[g]||c[g]||o;return n?a.createElement(h,r(r({ref:t},u),{},{components:n})):a.createElement(h,r({ref:t},u))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=g;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:i,r[1]=l;for(var m=2;m<o;m++)r[m]=n[m];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},69463:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>m});var a=n(87462),i=(n(67294),n(3905));const o={sidebar_position:0},r="Agent simulations",l={unversionedId:"use_cases/more/agents/agent_simulations/index",id:"use_cases/more/agents/agent_simulations/index",title:"Agent simulations",description:"Agent simulations involve interacting one or more agents with each other.",source:"@site/docs/use_cases/more/agents/agent_simulations/index.mdx",sourceDirName:"use_cases/more/agents/agent_simulations",slug:"/use_cases/more/agents/agent_simulations/",permalink:"/langchain/docs/use_cases/more/agents/agent_simulations/",draft:!1,tags:[],version:"current",sidebarPosition:0,frontMatter:{sidebar_position:0},sidebar:"use_cases",previous:{title:"Agents",permalink:"/langchain/docs/use_cases/more/agents/"},next:{title:"CAMEL Role-Playing Autonomous Cooperative Agents",permalink:"/langchain/docs/use_cases/more/agents/agent_simulations/camel_role_playing"}},s={},m=[{value:"Simulations with One Agent",id:"simulations-with-one-agent",level:2},{value:"Simulations with Two Agents",id:"simulations-with-two-agents",level:2},{value:"Simulations with Multiple Agents",id:"simulations-with-multiple-agents",level:2}],u={toc:m},p="wrapper";function c(e){let{components:t,...n}=e;return(0,i.kt)(p,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"agent-simulations"},"Agent simulations"),(0,i.kt)("p",null,"Agent simulations involve interacting one or more agents with each other.\nAgent simulations generally involve two main components:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Long Term Memory"),(0,i.kt)("li",{parentName:"ul"},"Simulation Environment")),(0,i.kt)("p",null,"Specific implementations of agent simulations (or parts of agent simulations) include:"),(0,i.kt)("h2",{id:"simulations-with-one-agent"},"Simulations with One Agent"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"./gymnasium.html"},"Simulated Environment: Gymnasium"),": an example of how to create a simple agent-environment interaction loop with ",(0,i.kt)("a",{parentName:"li",href:"https://gymnasium.farama.org/"},"Gymnasium")," (formerly ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/openai/gym"},"OpenAI Gym"),").")),(0,i.kt)("h2",{id:"simulations-with-two-agents"},"Simulations with Two Agents"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"./camel_role_playing.html"},"CAMEL"),": an implementation of the CAMEL (Communicative Agents for \u201cMind\u201d Exploration of Large Scale Language Model Society) paper, where two agents communicate with each other."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"./two_player_dnd.html"},"Two Player D&D"),": an example of how to use a generic simulator for two agents to implement a variant of the popular Dungeons & Dragons role playing game."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"./two_agent_debate_tools.html"},"Agent Debates with Tools"),": an example of how to enable Dialogue Agents to use tools to inform their responses.")),(0,i.kt)("h2",{id:"simulations-with-multiple-agents"},"Simulations with Multiple Agents"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"./multi_player_dnd.html"},"Multi-Player D&D"),": an example of how to use a generic dialogue simulator for multiple dialogue agents with a custom speaker-ordering, illustrated with a variant of the popular Dungeons & Dragons role playing game."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"./multiagent_bidding.html"},"Decentralized Speaker Selection"),": an example of how to implement a multi-agent dialogue without a fixed schedule for who speaks when. Instead the agents decide for themselves who speaks by outputting bids to speak. This example shows how to do this in the context of a fictitious presidential debate."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"./multiagent_authoritarian.html"},"Authoritarian Speaker Selection"),": an example of how to implement a multi-agent dialogue, where a privileged agent directs who speaks what. This example also showcases how to enable the privileged agent to determine when the conversation terminates. This example shows how to do this in the context of a fictitious news show."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"./petting_zoo.html"},"Simulated Environment: PettingZoo"),": an example of how to create a agent-environment interaction loop for multiple agents with ",(0,i.kt)("a",{parentName:"li",href:"https://pettingzoo.farama.org/"},"PettingZoo")," (a multi-agent version of ",(0,i.kt)("a",{parentName:"li",href:"https://gymnasium.farama.org/"},"Gymnasium"),")."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"./characters.html"},"Generative Agents"),": This notebook implements a generative agent based on the paper ",(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2304.03442"},"Generative Agents: Interactive Simulacra of Human Behavior")," by Park, et. al.")))}c.isMDXComponent=!0}}]);