"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[34040],{3905:(e,t,r)=>{r.d(t,{Zo:()=>p,kt:()=>h});var n=r(67294);function i(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach((function(t){i(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,n,i=function(e,t){if(null==e)return{};var r,n,i={},a=Object.keys(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||(i[r]=e[r]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(i[r]=e[r])}return i}var l=n.createContext({}),u=function(e){var t=n.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},p=function(e){var t=u(e.components);return n.createElement(l.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var r=e.components,i=e.mdxType,a=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=u(r),d=i,h=c["".concat(l,".").concat(d)]||c[d]||m[d]||a;return r?n.createElement(h,o(o({ref:t},p),{},{components:r})):n.createElement(h,o({ref:t},p))}));function h(e,t){var r=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=r.length,o=new Array(a);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:i,o[1]=s;for(var u=2;u<a;u++)o[u]=r[u];return n.createElement.apply(null,o)}return n.createElement.apply(null,r)}d.displayName="MDXCreateElement"},1041:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>u});var n=r(87462),i=(r(67294),r(3905));const a={},o="MultiQueryRetriever",s={unversionedId:"modules/data_connection/retrievers/MultiQueryRetriever",id:"modules/data_connection/retrievers/MultiQueryRetriever",title:"MultiQueryRetriever",description:'Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on "distance". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.',source:"@site/docs/modules/data_connection/retrievers/MultiQueryRetriever.md",sourceDirName:"modules/data_connection/retrievers",slug:"/modules/data_connection/retrievers/MultiQueryRetriever",permalink:"/langchain/docs/modules/data_connection/retrievers/MultiQueryRetriever",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Retrievers",permalink:"/langchain/docs/modules/data_connection/retrievers/"},next:{title:"Contextual compression",permalink:"/langchain/docs/modules/data_connection/retrievers/contextual_compression/"}},l={},u=[{value:"Simple usage",id:"simple-usage",level:4},{value:"Supplying your own prompt",id:"supplying-your-own-prompt",level:4}],p=(c="CodeOutputBlock",function(e){return console.warn("Component "+c+" was not imported, exported, or provided by MDXProvider as global scope"),(0,i.kt)("div",e)});var c;const m={toc:u},d="wrapper";function h(e){let{components:t,...r}=e;return(0,i.kt)(d,(0,n.Z)({},m,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"multiqueryretriever"},"MultiQueryRetriever"),(0,i.kt)("p",null,'Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on "distance". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.'),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"MultiQueryRetriever")," automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the ",(0,i.kt)("inlineCode",{parentName:"p"},"MultiQueryRetriever")," might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "Chroma", "source": "langchain.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html", "title": "MultiQueryRetriever"}, {"imported": "WebBaseLoader", "source": "langchain.document_loaders", "docs": "https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.web_base.WebBaseLoader.html", "title": "MultiQueryRetriever"}, {"imported": "OpenAIEmbeddings", "source": "langchain.embeddings.openai", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html", "title": "MultiQueryRetriever"}, {"imported": "RecursiveCharacterTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html", "title": "MultiQueryRetriever"}]--\x3e\n# Build a sample vectorDB\nfrom langchain.vectorstores import Chroma\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Load blog post\nloader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")\ndata = loader.load()\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nsplits = text_splitter.split_documents(data)\n\n# VectorDB\nembedding = OpenAIEmbeddings()\nvectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n')),(0,i.kt)("h4",{id:"simple-usage"},"Simple usage"),(0,i.kt)("p",null,"Specify the LLM to use for query generation, and the retriver will do the rest."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html", "title": "MultiQueryRetriever"}, {"imported": "MultiQueryRetriever", "source": "langchain.retrievers.multi_query", "docs": "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html", "title": "MultiQueryRetriever"}]--\x3e\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\n\nquestion = "What are the approaches to Task Decomposition?"\nllm = ChatOpenAI(temperature=0)\nretriever_from_llm = MultiQueryRetriever.from_llm(\n    retriever=vectordb.as_retriever(), llm=llm\n)\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Set logging for the queries\nimport logging\n\nlogging.basicConfig()\nlogging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"unique_docs = retriever_from_llm.get_relevant_documents(query=question)\nlen(unique_docs)\n")),(0,i.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"    INFO:langchain.retrievers.multi_query:Generated queries: ['1. How can Task Decomposition be approached?', '2. What are the different methods for Task Decomposition?', '3. What are the various approaches to decomposing tasks?']\n\n\n\n\n\n    5\n"))),(0,i.kt)("h4",{id:"supplying-your-own-prompt"},"Supplying your own prompt"),(0,i.kt)("p",null,"You can also supply a prompt along with an output parser to split the results into a list of queries."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html", "title": "MultiQueryRetriever"}, {"imported": "PydanticOutputParser", "source": "langchain.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.pydantic.PydanticOutputParser.html", "title": "MultiQueryRetriever"}]--\x3e\nfrom typing import List\nfrom langchain import LLMChain\nfrom pydantic import BaseModel, Field\nfrom langchain.prompts import PromptTemplate\nfrom langchain.output_parsers import PydanticOutputParser\n\n\n# Output parser will split the LLM result into a list of queries\nclass LineList(BaseModel):\n    # "lines" is the key (attribute name) of the parsed output\n    lines: List[str] = Field(description="Lines of text")\n\n\nclass LineListOutputParser(PydanticOutputParser):\n    def __init__(self) -> None:\n        super().__init__(pydantic_object=LineList)\n\n    def parse(self, text: str) -> LineList:\n        lines = text.strip().split("\\n")\n        return LineList(lines=lines)\n\n\noutput_parser = LineListOutputParser()\n\nQUERY_PROMPT = PromptTemplate(\n    input_variables=["question"],\n    template="""You are an AI language model assistant. Your task is to generate five \n    different versions of the given user question to retrieve relevant documents from a vector \n    database. By generating multiple perspectives on the user question, your goal is to help\n    the user overcome some of the limitations of the distance-based similarity search. \n    Provide these alternative questions seperated by newlines.\n    Original question: {question}""",\n)\nllm = ChatOpenAI(temperature=0)\n\n# Chain\nllm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n\n# Other inputs\nquestion = "What are the approaches to Task Decomposition?"\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Run\nretriever = MultiQueryRetriever(\n    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key="lines"\n)  # "lines" is the key (attribute name) of the parsed output\n\n# Results\nunique_docs = retriever.get_relevant_documents(\n    query="What does the course say about regression?"\n)\nlen(unique_docs)\n')),(0,i.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"    INFO:langchain.retrievers.multi_query:Generated queries: [\"1. What is the course's perspective on regression?\", '2. Can you provide information on regression as discussed in the course?', '3. How does the course cover the topic of regression?', \"4. What are the course's teachings on regression?\", '5. In relation to the course, what is mentioned about regression?']\n\n\n\n\n\n    11\n"))))}h.isMDXComponent=!0}}]);