"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[56494],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>h});var o=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=o.createContext({}),m=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},p=function(e){var n=m(e.components);return o.createElement(s.Provider,{value:n},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},c=o.forwardRef((function(e,n){var t=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),u=m(t),c=a,h=u["".concat(s,".").concat(c)]||u[c]||d[c]||r;return t?o.createElement(h,l(l({ref:n},p),{},{components:t})):o.createElement(h,l({ref:n},p))}));function h(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=t.length,l=new Array(r);l[0]=c;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[u]="string"==typeof e?e:a,l[1]=i;for(var m=2;m<r;m++)l[m]=t[m];return o.createElement.apply(null,l)}return o.createElement.apply(null,t)}c.displayName="MDXCreateElement"},17384:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>m});var o=t(87462),a=(t(67294),t(3905));const r={},l="Azure ML",i={unversionedId:"integrations/llms/azure_ml",id:"integrations/llms/azure_ml",title:"Azure ML",description:"Azure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.",source:"@site/docs/integrations/llms/azure_ml.md",sourceDirName:"integrations/llms",slug:"/integrations/llms/azure_ml",permalink:"/langchain/docs/integrations/llms/azure_ml",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Anyscale",permalink:"/langchain/docs/integrations/llms/anyscale"},next:{title:"Azure OpenAI",permalink:"/langchain/docs/integrations/llms/azure_openai"}},s={},m=[{value:"Set up",id:"set-up",level:2},{value:"Content Formatter",id:"content-formatter",level:2},{value:"Custom Content Formatter",id:"custom-content-formatter",level:3},{value:"Dolly with LLMChain",id:"dolly-with-llmchain",level:3},{value:"Serializing an LLM",id:"serializing-an-llm",level:3}],p=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var u;const d={toc:m},c="wrapper";function h(e){let{components:n,...t}=e;return(0,a.kt)(c,(0,o.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"azure-ml"},"Azure ML"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://azure.microsoft.com/en-us/products/machine-learning/"},"Azure ML")," is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML."),(0,a.kt)("p",null,"This notebook goes over how to use an LLM hosted on an ",(0,a.kt)("inlineCode",{parentName:"p"},"AzureML online endpoint")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "AzureMLOnlineEndpoint", "source": "langchain.llms.azureml_endpoint", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.azureml_endpoint.AzureMLOnlineEndpoint.html", "title": "Azure ML"}]--\x3e\nfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint\n')),(0,a.kt)("h2",{id:"set-up"},"Set up"),(0,a.kt)("p",null,"To use the wrapper, you must ",(0,a.kt)("a",{parentName:"p",href:"https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-foundation-models?view=azureml-api-2#deploying-foundation-models-to-endpoints-for-inferencing"},"deploy a model on AzureML")," and obtain the following parameters:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"endpoint_api_key"),": Required - The API key provided by the endpoint"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"endpoint_url"),": Required - The REST endpoint url provided by the endpoint"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"deployment_name"),": Not required - The deployment name of the model using the endpoint")),(0,a.kt)("h2",{id:"content-formatter"},"Content Formatter"),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"content_formatter")," parameter is a handler class for transforming the request and response of an AzureML endpoint to match with required schema. Since there are a wide range of models in the model catalog, each of which may process data differently from one another, a ",(0,a.kt)("inlineCode",{parentName:"p"},"ContentFormatterBase")," class is provided to allow users to transform data to their liking. The following content formatters are provided:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"GPT2ContentFormatter"),": Formats request and response data for GPT2"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"DollyContentFormatter"),": Formats request and response data for the Dolly-v2"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"HFContentFormatter"),": Formats request and response data for text-generation Hugging Face models"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"LLamaContentFormatter"),": Formats request and response data for LLaMa2")),(0,a.kt)("p",null,(0,a.kt)("em",{parentName:"p"},"Note: ",(0,a.kt)("inlineCode",{parentName:"em"},"OSSContentFormatter")," is being deprecated and replaced with ",(0,a.kt)("inlineCode",{parentName:"em"},"GPT2ContentFormatter"),". The logic is the same but ",(0,a.kt)("inlineCode",{parentName:"em"},"GPT2ContentFormatter")," is a more suitable name. You can still continue to use ",(0,a.kt)("inlineCode",{parentName:"em"},"OSSContentFormatter")," as the changes are backwards compatibile.")),(0,a.kt)("p",null,"Below is an example using a summarization model from Hugging Face."),(0,a.kt)("h3",{id:"custom-content-formatter"},"Custom Content Formatter"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "AzureMLOnlineEndpoint", "source": "langchain.llms.azureml_endpoint", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.azureml_endpoint.AzureMLOnlineEndpoint.html", "title": "Azure ML"}, {"imported": "ContentFormatterBase", "source": "langchain.llms.azureml_endpoint", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.azureml_endpoint.ContentFormatterBase.html", "title": "Azure ML"}]--\x3e\nfrom typing import Dict\n\nfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBase\nimport os\nimport json\n\n\nclass CustomFormatter(ContentFormatterBase):\n    content_type = "application/json"\n    accepts = "application/json"\n\n    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:\n        input_str = json.dumps(\n            {\n                "inputs": [prompt],\n                "parameters": model_kwargs,\n                "options": {"use_cache": False, "wait_for_model": True},\n            }\n        )\n        return str.encode(input_str)\n\n    def format_response_payload(self, output: bytes) -> str:\n        response_json = json.loads(output)\n        return response_json[0]["summary_text"]\n\n\ncontent_formatter = CustomFormatter()\n\nllm = AzureMLOnlineEndpoint(\n    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),\n    endpoint_url=os.getenv("BART_ENDPOINT_URL"),\n    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},\n    content_formatter=content_formatter,\n)\nlarge_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona\'s \nnext album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be \ntaking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along \nwith the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other \nsongs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at \nnumber 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet\'s \nM Countdown.[42]\n\nOn October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single \n"Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] \nThe EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released \nthe music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona\'s first entry on the Billboard \nMainstream Top 40, making them the second K-pop girl group to enter the chart.[47]\n\nOn June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).\n[48] The following day, on June 2, a teaser was posted to Loona\'s official social media accounts showing twelve sets of eyes, \nconfirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, \nChoerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another \ncollaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are \nmaking their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced \nthat Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October \n20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]\n"""\nsummarized_text = llm(large_text)\nprint(summarized_text)\n')),(0,a.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'    HaSeul won her first music show trophy with "So What" on Mnet\'s M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.\n'))),(0,a.kt)("h3",{id:"dolly-with-llmchain"},"Dolly with LLMChain"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "DollyContentFormatter", "source": "langchain.llms.azureml_endpoint", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.azureml_endpoint.DollyContentFormatter.html", "title": "Azure ML"}, {"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "Azure ML"}]--\x3e\nfrom langchain import PromptTemplate\nfrom langchain.llms.azureml_endpoint import DollyContentFormatter\nfrom langchain.chains import LLMChain\n\nformatter_template = "Write a {word_count} word essay about {topic}."\n\nprompt = PromptTemplate(\n    input_variables=["word_count", "topic"], template=formatter_template\n)\n\ncontent_formatter = DollyContentFormatter()\n\nllm = AzureMLOnlineEndpoint(\n    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),\n    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),\n    model_kwargs={"temperature": 0.8, "max_tokens": 300},\n    content_formatter=content_formatter,\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run({"word_count": 100, "topic": "how to make friends"}))\n')),(0,a.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.\n"))),(0,a.kt)("h3",{id:"serializing-an-llm"},"Serializing an LLM"),(0,a.kt)("p",null,"You can also save and load LLM configurations"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "load_llm", "source": "langchain.llms.loading", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.loading.load_llm.html", "title": "Azure ML"}, {"imported": "AzureMLEndpointClient", "source": "langchain.llms.azureml_endpoint", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.azureml_endpoint.AzureMLEndpointClient.html", "title": "Azure ML"}]--\x3e\nfrom langchain.llms.loading import load_llm\nfrom langchain.llms.azureml_endpoint import AzureMLEndpointClient\n\nsave_llm = AzureMLOnlineEndpoint(\n    deployment_name="databricks-dolly-v2-12b-4",\n    model_kwargs={\n        "temperature": 0.2,\n        "max_tokens": 150,\n        "top_p": 0.8,\n        "frequency_penalty": 0.32,\n        "presence_penalty": 72e-3,\n    },\n)\nsave_llm.save("azureml.json")\nloaded_llm = load_llm("azureml.json")\n\nprint(loaded_llm)\n')),(0,a.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    AzureMLOnlineEndpoint\n    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}\n"))))}h.isMDXComponent=!0}}]);