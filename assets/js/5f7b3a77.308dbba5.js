"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[71690],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>h});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(n),d=o,h=u["".concat(l,".").concat(d)]||u[d]||m[d]||r;return n?a.createElement(h,i(i({ref:t},p),{},{components:n})):a.createElement(h,i({ref:t},p))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:o,i[1]=s;for(var c=2;c<r;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},52991:(e,t,n)=>{n.d(t,{Z:()=>f});var a=n(67294),o=n(86010),r=n(53438),i=n(39960),s=n(13919),l=n(95999);const c={cardContainer:"cardContainer_fWXF",cardTitle:"cardTitle_rnsV",cardDescription:"cardDescription_PWke"};function p(e){let{href:t,children:n}=e;return a.createElement(i.Z,{href:t,className:(0,o.Z)("card padding--lg",c.cardContainer)},n)}function u(e){let{href:t,icon:n,title:r,description:i}=e;return a.createElement(p,{href:t},a.createElement("h2",{className:(0,o.Z)("text--truncate",c.cardTitle),title:r},n," ",r),i&&a.createElement("p",{className:(0,o.Z)("text--truncate",c.cardDescription),title:i},i))}function m(e){let{item:t}=e;const n=(0,r.Wl)(t);return n?a.createElement(u,{href:n,icon:"\ud83d\uddc3\ufe0f",title:t.label,description:t.description??(0,l.I)({message:"{count} items",id:"theme.docs.DocCard.categoryDescription",description:"The default description for a category card in the generated index about how many items this category includes"},{count:t.items.length})}):null}function d(e){let{item:t}=e;const n=(0,s.Z)(t.href)?"\ud83d\udcc4\ufe0f":"\ud83d\udd17",o=(0,r.xz)(t.docId??void 0);return a.createElement(u,{href:t.href,icon:n,title:t.label,description:t.description??o?.description})}function h(e){let{item:t}=e;switch(t.type){case"link":return a.createElement(d,{item:t});case"category":return a.createElement(m,{item:t});default:throw new Error(`unknown item type ${JSON.stringify(t)}`)}}function g(e){let{className:t}=e;const n=(0,r.jA)();return a.createElement(f,{items:n.items,className:t})}function f(e){const{items:t,className:n}=e;if(!t)return a.createElement(g,e);const i=(0,r.MN)(t);return a.createElement("section",{className:(0,o.Z)("row",n)},i.map(((e,t)=>a.createElement("article",{key:t,className:"col col--6 margin-bottom--lg"},a.createElement(h,{item:e})))))}},75618:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var a=n(87462),o=(n(67294),n(3905)),r=n(52991);const i={sidebar_position:0},s="Chat loaders",l={unversionedId:"integrations/chat_loaders/index",id:"integrations/chat_loaders/index",title:"Chat loaders",description:"Like document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice.",source:"@site/docs/integrations/chat_loaders/index.mdx",sourceDirName:"integrations/chat_loaders",slug:"/integrations/chat_loaders/",permalink:"/langchain/docs/integrations/chat_loaders/",draft:!1,tags:[],version:"current",sidebarPosition:0,frontMatter:{sidebar_position:0},sidebar:"integrations",previous:{title:"PromptLayer ChatOpenAI",permalink:"/langchain/docs/integrations/chat/promptlayer_chatopenai"},next:{title:"Discord",permalink:"/langchain/docs/integrations/chat_loaders/discord"}},c={},p=[{value:"1. Export your chat data",id:"1-export-your-chat-data",level:3},{value:"2. Load the chat",id:"2-load-the-chat",level:3},{value:"3. Export messages to OpenAI format",id:"3-export-messages-to-openai-format",level:3},{value:"4. Upload the data to OpenAI",id:"4-upload-the-data-to-openai",level:3},{value:"5. Fine-tune the model",id:"5-fine-tune-the-model",level:3},{value:"6. Use the model in LangChain",id:"6-use-the-model-in-langchain",level:3},{value:"Supported Chat Loaders",id:"supported-chat-loaders",level:2}],u={toc:p},m="wrapper";function d(e){let{components:t,...n}=e;return(0,o.kt)(m,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"chat-loaders"},"Chat loaders"),(0,o.kt)("p",null,"Like document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice. "),(0,o.kt)("p",null,"This brief guide will illustrate the process using ",(0,o.kt)("a",{parentName:"p",href:"https://platform.openai.com/docs/guides/fine-tuning"},"OpenAI's fine-tuning API")," comprised of six steps:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Export your Facebook Messenger chat data in a compatible format for your intended chat loader."),(0,o.kt)("li",{parentName:"ol"},"Load the chat data into memory as LangChain chat message objects. (",(0,o.kt)("em",{parentName:"li"},"this is what is covered in each integration notebook in this section of the documentation"),").",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},'Assign a person to the "AI" role and optionally filter, group, and merge messages.'))),(0,o.kt)("li",{parentName:"ol"},"Export these acquired messages in a format expected by the fine-tuning API."),(0,o.kt)("li",{parentName:"ol"},"Upload this data to OpenAI."),(0,o.kt)("li",{parentName:"ol"},"Fine-tune your model."),(0,o.kt)("li",{parentName:"ol"},"Implement the fine-tuned model in LangChain.")),(0,o.kt)("p",null,"This guide is not wholly comprehensive but is designed to take you through the fundamentals of going from raw data to fine-tuned model."),(0,o.kt)("p",null,"We will demonstrate the procedure through an example of fine-tuning a ",(0,o.kt)("inlineCode",{parentName:"p"},"gpt-3.5-turbo")," model on Facebook Messenger data. "),(0,o.kt)("h3",{id:"1-export-your-chat-data"},"1. Export your chat data"),(0,o.kt)("p",null,"To export your Facebook messenger data, you can follow the ",(0,o.kt)("a",{parentName:"p",href:"https://www.zapptales.com/en/download-facebook-messenger-chat-history-how-to/"},"instructions here"),". "),(0,o.kt)("admonition",{title:"JSON format",type:"important"},(0,o.kt)("p",{parentName:"admonition"},'You must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader.')),(0,o.kt)("p",null,"OpenAI requires at least 10 examples to fine-tune your model, but they recommend between 50-100 for more optimal results.\nYou can use the example data stored at ",(0,o.kt)("a",{parentName:"p",href:"https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing"},"this google drive link")," to test the process."),(0,o.kt)("h3",{id:"2-load-the-chat"},"2. Load the chat"),(0,o.kt)("p",null,"Once you've obtained your chat data, you can load it into memory as LangChain chat message objects. Here\u2019s an example of loading data using the Python code:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chat_loaders.facebook_messenger import FolderFacebookMessengerChatLoader\n\nloader = FolderFacebookMessengerChatLoader(\n    path="./facebook_messenger_chats",\n)\n\nchat_sessions = loader.load()\n')),(0,o.kt)("p",null,'In this snippet, we point the loader to a directory of Facebook chat dumps which are then loaded as multiple "sessions" of messages, one session per conversation file.'),(0,o.kt)("p",null,"Once you've loaded the messages, you should decide which person you want to fine-tune the model to (usually yourself). You can also decide to merge consecutive messages from the same sender into a single chat message.\nFor both of these tasks, you can use the chat_loaders utilities to do so:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'from langchain.chat_loaders.utils import (\n    merge_chat_runs,\n    map_ai_messages,\n)\n\nmerged_sessions = merge_chat_runs(chat_sessions)\nalternating_sessions = list(map_ai_messages(merged_sessions, "My Name"))\n')),(0,o.kt)("h3",{id:"3-export-messages-to-openai-format"},"3. Export messages to OpenAI format"),(0,o.kt)("p",null,"Convert the chat messages to dictionaries using the ",(0,o.kt)("inlineCode",{parentName:"p"},"convert_messages_for_finetuning")," function. Then, group the data into chunks for better context modeling and overlap management."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.adapters.openai import convert_messages_for_finetuning\n\nopenai_messages = convert_messages_for_finetuning(chat_sessions)\n")),(0,o.kt)("p",null,"At this point, the data is ready for upload to OpenAI. You can choose to split up conversations into smaller chunks for training if you\ndo not have enough conversations to train on. Feel free to play around with different chunk sizes or with adding system messages to the fine-tuning data."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"chunk_size = 8\noverlap = 2\n\nmessage_groups = [\n    conversation_messages[i: i + chunk_size] \n    for conversation_messages in openai_messages\n    for i in range(\n        0, len(conversation_messages) - chunk_size + 1, \n        chunk_size - overlap)\n]\n\nlen(message_groups)\n# 9\n")),(0,o.kt)("h3",{id:"4-upload-the-data-to-openai"},"4. Upload the data to OpenAI"),(0,o.kt)("p",null,"Ensure you have set your OpenAI API key by following these ",(0,o.kt)("a",{parentName:"p",href:"https://platform.openai.com/account/api-keys"},"instructions"),", then upload the training file.\nAn audit is performed to ensure data compliance, so you may have to wait a few minutes for the dataset to become ready for use."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import time\nimport json\nimport io\n\nimport openai\n\nmy_file = io.BytesIO()\nfor group in message_groups:\n    my_file.write((json.dumps({"messages": group}) + "\\n").encode(\'utf-8\'))\n\nmy_file.seek(0)\ntraining_file = openai.File.create(\n  file=my_file,\n  purpose=\'fine-tune\'\n)\n\n# Wait while the file is processed\nstatus = openai.File.retrieve(training_file.id).status\nstart_time = time.time()\nwhile status != "processed":\n    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\\r", flush=True)\n    time.sleep(5)\n    status = openai.File.retrieve(training_file.id).status\nprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")\n')),(0,o.kt)("p",null,"Once this is done, you can proceed to the model training!"),(0,o.kt)("h3",{id:"5-fine-tune-the-model"},"5. Fine-tune the model"),(0,o.kt)("p",null,"Start the fine-tuning job with your chosen base model."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'job = openai.FineTuningJob.create(\n    training_file=training_file.id,\n    model="gpt-3.5-turbo",\n)\n')),(0,o.kt)("p",null,"This might take a while. Check the status with ",(0,o.kt)("inlineCode",{parentName:"p"},"openai.FineTuningJob.retrieve(job.id).status")," and wait for it to report ",(0,o.kt)("inlineCode",{parentName:"p"},"succeeded"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# It may take 10-20+ minutes to complete training.\nstatus = openai.FineTuningJob.retrieve(job.id).status\nstart_time = time.time()\nwhile status != "succeeded":\n    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\\r", flush=True)\n    time.sleep(5)\n    job = openai.FineTuningJob.retrieve(job.id)\n    status = job.status\n')),(0,o.kt)("h3",{id:"6-use-the-model-in-langchain"},"6. Use the model in LangChain"),(0,o.kt)("p",null,"You're almost there! Use the fine-tuned model in LangChain."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain import chat_models\n\nmodel_name = job.fine_tuned_model\n# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsed\nmodel = chat_models.ChatOpenAI(model=model_name)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser \n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        ("human", "{input}"),\n    ]\n)\n\nchain = prompt | model | StrOutputParser()\n\nfor tok in chain.stream({"input": "What classes are you taking?"}):\n    print(tok, end="", flush=True)\n\n# The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?\n')),(0,o.kt)("p",null,"And that's it! You've successfully fine-tuned a model and used it in LangChain."),(0,o.kt)("h2",{id:"supported-chat-loaders"},"Supported Chat Loaders"),(0,o.kt)("p",null,"LangChain currently supports the following chat loaders. Feel free to contribute more!"),(0,o.kt)(r.Z,{mdxType:"DocCardList"}))}d.isMDXComponent=!0}}]);