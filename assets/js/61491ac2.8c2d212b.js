"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[11626],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>g});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=o.createContext({}),m=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},c=function(e){var t=m(e.components);return o.createElement(l.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),p=m(n),h=a,g=p["".concat(l,".").concat(h)]||p[h]||u[h]||r;return n?o.createElement(g,s(s({ref:t},c),{},{components:n})):o.createElement(g,s({ref:t},c))}));function g(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,s=new Array(r);s[0]=h;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[p]="string"==typeof e?e:a,s[1]=i;for(var m=2;m<r;m++)s[m]=n[m];return o.createElement.apply(null,s)}return o.createElement.apply(null,n)}h.displayName="MDXCreateElement"},73194:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>m});var o=n(87462),a=(n(67294),n(3905));const r={},s="AutoGPT",i={unversionedId:"use_cases/more/agents/autonomous_agents/autogpt",id:"use_cases/more/agents/autonomous_agents/autogpt",title:"AutoGPT",description:"Implementation of https://github.com/Significant-Gravitas/Auto-GPT but with LangChain primitives (LLMs, PromptTemplates, VectorStores, Embeddings, Tools)",source:"@site/docs/use_cases/more/agents/autonomous_agents/autogpt.md",sourceDirName:"use_cases/more/agents/autonomous_agents",slug:"/use_cases/more/agents/autonomous_agents/autogpt",permalink:"/langchain/docs/use_cases/more/agents/autonomous_agents/autogpt",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"use_cases",previous:{title:"Autonomous (long-running) agents",permalink:"/langchain/docs/use_cases/more/agents/autonomous_agents/"},next:{title:"BabyAGI User Guide",permalink:"/langchain/docs/use_cases/more/agents/autonomous_agents/baby_agi"}},l={},m=[{value:"Set up tools",id:"set-up-tools",level:2},{value:"Set up memory",id:"set-up-memory",level:2},{value:"Setup model and AutoGPT",id:"setup-model-and-autogpt",level:2},{value:"Run an example",id:"run-an-example",level:2},{value:"Chat History Memory",id:"chat-history-memory",level:2}],c={toc:m},p="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(p,(0,o.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"autogpt"},"AutoGPT"),(0,a.kt)("p",null,"Implementation of ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/Significant-Gravitas/Auto-GPT"},"https://github.com/Significant-Gravitas/Auto-GPT")," but with LangChain primitives (LLMs, PromptTemplates, VectorStores, Embeddings, Tools)"),(0,a.kt)("h2",{id:"set-up-tools"},"Set up tools"),(0,a.kt)("p",null,"We'll set up an AutoGPT with a search tool, and write-file tool, and a read-file tool"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "SerpAPIWrapper", "source": "langchain.utilities", "docs": "https://api.python.langchain.com/en/latest/utilities/langchain.utilities.serpapi.SerpAPIWrapper.html", "title": "AutoGPT"}, {"imported": "Tool", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/tools/langchain.tools.base.Tool.html", "title": "AutoGPT"}, {"imported": "WriteFileTool", "source": "langchain.tools.file_management.write", "docs": "https://api.python.langchain.com/en/latest/tools/langchain.tools.file_management.write.WriteFileTool.html", "title": "AutoGPT"}, {"imported": "ReadFileTool", "source": "langchain.tools.file_management.read", "docs": "https://api.python.langchain.com/en/latest/tools/langchain.tools.file_management.read.ReadFileTool.html", "title": "AutoGPT"}]--\x3e\nfrom langchain.utilities import SerpAPIWrapper\nfrom langchain.agents import Tool\nfrom langchain.tools.file_management.write import WriteFileTool\nfrom langchain.tools.file_management.read import ReadFileTool\n\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name="search",\n        func=search.run,\n        description="useful for when you need to answer questions about current events. You should ask targeted questions",\n    ),\n    WriteFileTool(),\n    ReadFileTool(),\n]\n')),(0,a.kt)("h2",{id:"set-up-memory"},"Set up memory"),(0,a.kt)("p",null,"The memory here is used for the agents intermediate steps"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "FAISS", "source": "langchain.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html", "title": "AutoGPT"}, {"imported": "InMemoryDocstore", "source": "langchain.docstore", "docs": "https://api.python.langchain.com/en/latest/docstore/langchain.docstore.in_memory.InMemoryDocstore.html", "title": "AutoGPT"}, {"imported": "OpenAIEmbeddings", "source": "langchain.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html", "title": "AutoGPT"}]--\x3e\nfrom langchain.vectorstores import FAISS\nfrom langchain.docstore import InMemoryDocstore\nfrom langchain.embeddings import OpenAIEmbeddings\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# Define your embedding model\nembeddings_model = OpenAIEmbeddings()\n# Initialize the vectorstore as empty\nimport faiss\n\nembedding_size = 1536\nindex = faiss.IndexFlatL2(embedding_size)\nvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\n")),(0,a.kt)("h2",{id:"setup-model-and-autogpt"},"Setup model and AutoGPT"),(0,a.kt)("p",null,"Initialize everything! We will use ChatOpenAI model"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html", "title": "AutoGPT"}]--\x3e\nfrom langchain_experimental.autonomous_agents import AutoGPT\nfrom langchain.chat_models import ChatOpenAI\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'agent = AutoGPT.from_llm_and_tools(\n    ai_name="Tom",\n    ai_role="Assistant",\n    tools=tools,\n    llm=ChatOpenAI(temperature=0),\n    memory=vectorstore.as_retriever(),\n)\n# Set verbose to be true\nagent.chain.verbose = True\n')),(0,a.kt)("h2",{id:"run-an-example"},"Run an example"),(0,a.kt)("p",null,"Here we will make it write a weather report for SF"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'agent.run(["write a weather report for SF today"])\n')),(0,a.kt)("h2",{id:"chat-history-memory"},"Chat History Memory"),(0,a.kt)("p",null,"In addition to the memory that holds the agent immediate steps, we also have a chat history memory. By default, the agent will use 'ChatMessageHistory' and it can be changed. This is useful when you want to use a different type of memory for example 'FileChatHistoryMemory'"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "FileChatMessageHistory", "source": "langchain.memory.chat_message_histories", "docs": "https://api.python.langchain.com/en/latest/memory/langchain.memory.chat_message_histories.file.FileChatMessageHistory.html", "title": "AutoGPT"}]--\x3e\nfrom langchain.memory.chat_message_histories import FileChatMessageHistory\n\nagent = AutoGPT.from_llm_and_tools(\n    ai_name="Tom",\n    ai_role="Assistant",\n    tools=tools,\n    llm=ChatOpenAI(temperature=0),\n    memory=vectorstore.as_retriever(),\n    chat_history_memory=FileChatMessageHistory("chat_history.txt"),\n)\n')))}u.isMDXComponent=!0}}]);