"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[42974],{3905:(e,a,t)=>{t.d(a,{Zo:()=>m,kt:()=>h});var n=t(67294);function l(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function r(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){l(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,n,l=function(e,a){if(null==e)return{};var t,n,l={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(l[t]=e[t]);return l}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(l[t]=e[t])}return l}var s=n.createContext({}),c=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):r(r({},a),e)),t},m=function(e){var a=c(e.components);return n.createElement(s.Provider,{value:a},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},u=n.forwardRef((function(e,a){var t=e.components,l=e.mdxType,o=e.originalType,s=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),p=c(t),u=l,h=p["".concat(s,".").concat(u)]||p[u]||d[u]||o;return t?n.createElement(h,r(r({ref:a},m),{},{components:t})):n.createElement(h,r({ref:a},m))}));function h(e,a){var t=arguments,l=a&&a.mdxType;if("string"==typeof e||l){var o=t.length,r=new Array(o);r[0]=u;var i={};for(var s in a)hasOwnProperty.call(a,s)&&(i[s]=a[s]);i.originalType=e,i[p]="string"==typeof e?e:l,r[1]=i;for(var c=2;c<o;c++)r[c]=t[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,t)}u.displayName="MDXCreateElement"},93124:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>s,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var n=t(87462),l=(t(67294),t(3905));const o={},r="Ollama",i={unversionedId:"integrations/llms/ollama",id:"integrations/llms/ollama",title:"Ollama",description:"Ollama allows you to run open-source large language models, such as Llama 2, locally.",source:"@site/docs/integrations/llms/ollama.md",sourceDirName:"integrations/llms",slug:"/integrations/llms/ollama",permalink:"/langchain/docs/integrations/llms/ollama",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"OctoAI",permalink:"/langchain/docs/integrations/llms/octoai"},next:{title:"OpaquePrompts",permalink:"/langchain/docs/integrations/llms/opaqueprompts"}},s={},c=[{value:"Setup",id:"setup",level:2},{value:"Usage",id:"usage",level:2},{value:"RAG",id:"rag",level:2}],m=(p="CodeOutputBlock",function(e){return console.warn("Component "+p+" was not imported, exported, or provided by MDXProvider as global scope"),(0,l.kt)("div",e)});var p;const d={toc:c},u="wrapper";function h(e){let{components:a,...t}=e;return(0,l.kt)(u,(0,n.Z)({},d,t,{components:a,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"ollama"},"Ollama"),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://ollama.ai/"},"Ollama")," allows you to run open-source large language models, such as Llama 2, locally."),(0,l.kt)("p",null,"Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. "),(0,l.kt)("p",null,"It optimizes setup and configuration details, including GPU usage."),(0,l.kt)("p",null,"For a complete list of supported models and model variants, see the ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/jmorganca/ollama#model-library"},"Ollama model library"),"."),(0,l.kt)("h2",{id:"setup"},"Setup"),(0,l.kt)("p",null,"First, follow ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/jmorganca/ollama"},"these instructions")," to set up and run a local Ollama instance:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://ollama.ai/download"},"Download")),(0,l.kt)("li",{parentName:"ul"},"Fetch a model via ",(0,l.kt)("inlineCode",{parentName:"li"},"ollama pull <model family>")),(0,l.kt)("li",{parentName:"ul"},"e.g., for ",(0,l.kt)("inlineCode",{parentName:"li"},"Llama-7b"),": ",(0,l.kt)("inlineCode",{parentName:"li"},"ollama pull llama2")," (see full list ",(0,l.kt)("a",{parentName:"li",href:"https://github.com/jmorganca/ollama"},"here"),")"),(0,l.kt)("li",{parentName:"ul"},"This will download the most basic version of the model typically (e.g., smallest # parameters and ",(0,l.kt)("inlineCode",{parentName:"li"},"q4_0"),")"),(0,l.kt)("li",{parentName:"ul"},"On Mac, it will download to ")),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"~/.ollama/models/manifests/registry.ollama.ai/library/<model family>/latest")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"And we specify a particular version, e.g., for ",(0,l.kt)("inlineCode",{parentName:"li"},"ollama pull vicuna:13b-v1.5-16k-q4_0")),(0,l.kt)("li",{parentName:"ul"},"The file is here with the model version in place of ",(0,l.kt)("inlineCode",{parentName:"li"},"latest"))),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"~/.ollama/models/manifests/registry.ollama.ai/library/vicuna/13b-v1.5-16k-q4_0")),(0,l.kt)("p",null,"You can easily access models in a few ways:"),(0,l.kt)("p",null,"1/ if the app is running:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"All of your local models are automatically served on ",(0,l.kt)("inlineCode",{parentName:"li"},"localhost:11434")),(0,l.kt)("li",{parentName:"ul"},"Select your model when setting ",(0,l.kt)("inlineCode",{parentName:"li"},'llm = Ollama(..., model="<model family>:<version>")')),(0,l.kt)("li",{parentName:"ul"},"If you set ",(0,l.kt)("inlineCode",{parentName:"li"},'llm = Ollama(..., model="<model family")')," withoout a version it will simply look for ",(0,l.kt)("inlineCode",{parentName:"li"},"latest"))),(0,l.kt)("p",null,"2/ if building from source or just running the binary: "),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Then you must run ",(0,l.kt)("inlineCode",{parentName:"li"},"ollama serve")),(0,l.kt)("li",{parentName:"ul"},"All of your local models are automatically served on ",(0,l.kt)("inlineCode",{parentName:"li"},"localhost:11434")),(0,l.kt)("li",{parentName:"ul"},"Then, select as shown above")),(0,l.kt)("h2",{id:"usage"},"Usage"),(0,l.kt)("p",null,"You can see a full list of supported parameters on the ",(0,l.kt)("a",{parentName:"p",href:"https://api.python.langchain.com/en/latest/llms/langchain.llms.ollama.Ollama.html"},"API reference page"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "Ollama", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.ollama.Ollama.html", "title": "Ollama"}, {"imported": "CallbackManager", "source": "langchain.callbacks.manager", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.CallbackManager.html", "title": "Ollama"}, {"imported": "StreamingStdOutCallbackHandler", "source": "langchain.callbacks.streaming_stdout", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html", "title": "Ollama"}]--\x3e\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler                                  \nllm = Ollama(base_url="http://localhost:11434", \n             model="llama2", \n             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\n')),(0,l.kt)("p",null,"With ",(0,l.kt)("inlineCode",{parentName:"p"},"StreamingStdOutCallbackHandler"),", you will see tokens streamed."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'llm("Tell me about the history of AI")\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    \n    Great! The history of Artificial Intelligence (AI) is a fascinating and complex topic that spans several decades. Here's a brief overview:\n    \n    1. Early Years (1950s-1960s): The term \"Artificial Intelligence\" was coined in 1956 by computer scientist John McCarthy. However, the concept of AI dates back to ancient Greece, where mythical creatures like Talos and Hephaestus were created to perform tasks without any human intervention. In the 1950s and 1960s, researchers began exploring ways to replicate human intelligence using computers, leading to the development of simple AI programs like ELIZA (1966) and PARRY (1972).\n    2. Rule-Based Systems (1970s-1980s): As computing power increased, researchers developed rule-based systems, such as Mycin (1976), which could diagnose medical conditions based on a set of rules. This period also saw the rise of expert systems, like EDICT (1985), which mimicked human experts in specific domains.\n    3. Machine Learning (1990s-2000s): With the advent of big data and machine learning algorithms, AI evolved to include neural networks, decision trees, and other techniques for training models on large datasets. This led to the development of applications like speech recognition (e.g., Siri, Alexa), image recognition (e.g., Google Image Search), and natural language processing (e.g., chatbots).\n    4. Deep Learning (2010s-present): The rise of deep learning techniques, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), has enabled AI to perform complex tasks like image and speech recognition, natural language processing, and even autonomous driving. Companies like Google, Facebook, and Baidu have invested heavily in deep learning research, leading to breakthroughs in areas like facial recognition, object detection, and machine translation.\n    5. Current Trends (present-future): AI is currently being applied to various industries, including healthcare, finance, education, and entertainment. With the growth of cloud computing, edge AI, and autonomous systems, we can expect to see more sophisticated AI applications in the near future. However, there are also concerns about the ethical implications of AI, such as data privacy, algorithmic bias, and job displacement.\n    \n    Remember, AI has a long history, and its development is an ongoing process. As technology advances, we can expect to see even more innovative applications of AI in various fields.\n\n\n\n\n    '\\nGreat! The history of Artificial Intelligence (AI) is a fascinating and complex topic that spans several decades. Here\\'s a brief overview:\\n\\n1. Early Years (1950s-1960s): The term \"Artificial Intelligence\" was coined in 1956 by computer scientist John McCarthy. However, the concept of AI dates back to ancient Greece, where mythical creatures like Talos and Hephaestus were created to perform tasks without any human intervention. In the 1950s and 1960s, researchers began exploring ways to replicate human intelligence using computers, leading to the development of simple AI programs like ELIZA (1966) and PARRY (1972).\\n2. Rule-Based Systems (1970s-1980s): As computing power increased, researchers developed rule-based systems, such as Mycin (1976), which could diagnose medical conditions based on a set of rules. This period also saw the rise of expert systems, like EDICT (1985), which mimicked human experts in specific domains.\\n3. Machine Learning (1990s-2000s): With the advent of big data and machine learning algorithms, AI evolved to include neural networks, decision trees, and other techniques for training models on large datasets. This led to the development of applications like speech recognition (e.g., Siri, Alexa), image recognition (e.g., Google Image Search), and natural language processing (e.g., chatbots).\\n4. Deep Learning (2010s-present): The rise of deep learning techniques, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), has enabled AI to perform complex tasks like image and speech recognition, natural language processing, and even autonomous driving. Companies like Google, Facebook, and Baidu have invested heavily in deep learning research, leading to breakthroughs in areas like facial recognition, object detection, and machine translation.\\n5. Current Trends (present-future): AI is currently being applied to various industries, including healthcare, finance, education, and entertainment. With the growth of cloud computing, edge AI, and autonomous systems, we can expect to see more sophisticated AI applications in the near future. However, there are also concerns about the ethical implications of AI, such as data privacy, algorithmic bias, and job displacement.\\n\\nRemember, AI has a long history, and its development is an ongoing process. As technology advances, we can expect to see even more innovative applications of AI in various fields.'\n"))),(0,l.kt)("h2",{id:"rag"},"RAG"),(0,l.kt)("p",null,"We can use Olama with RAG, ",(0,l.kt)("a",{parentName:"p",href:"https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa"},"just as shown here"),"."),(0,l.kt)("p",null,"Let's use the 13b model:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"ollama pull llama2:13b\nollama run llama2:13b \n")),(0,l.kt)("p",null,"Let's also use local embeddings from ",(0,l.kt)("inlineCode",{parentName:"p"},"GPT4AllEmbeddings")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"Chroma"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"pip install gpt4all chromadb\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "WebBaseLoader", "source": "langchain.document_loaders", "docs": "https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.web_base.WebBaseLoader.html", "title": "Ollama"}, {"imported": "RecursiveCharacterTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html", "title": "Ollama"}]--\x3e\nfrom langchain.document_loaders import WebBaseLoader\nloader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")\ndata = loader.load()\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "Chroma", "source": "langchain.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html", "title": "Ollama"}, {"imported": "GPT4AllEmbeddings", "source": "langchain.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.gpt4all.GPT4AllEmbeddings.html", "title": "Ollama"}]--\x3e\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import GPT4AllEmbeddings\n\nvectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'question = "What are the approaches to Task Decomposition?"\ndocs = vectorstore.similarity_search(question)\nlen(docs)\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    4\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from langchain import PromptTemplate\n\n# Prompt\ntemplate = """Use the following pieces of context to answer the question at the end. \nIf you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer. \nUse three sentences maximum and keep the answer as concise as possible. \n{context}\nQuestion: {question}\nHelpful Answer:"""\nQA_CHAIN_PROMPT = PromptTemplate(\n    input_variables=["context", "question"],\n    template=template,\n)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "Ollama", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.ollama.Ollama.html", "title": "Ollama"}, {"imported": "CallbackManager", "source": "langchain.callbacks.manager", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.CallbackManager.html", "title": "Ollama"}, {"imported": "StreamingStdOutCallbackHandler", "source": "langchain.callbacks.streaming_stdout", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html", "title": "Ollama"}]--\x3e\n# LLM\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nllm = Ollama(base_url="http://localhost:11434",\n             model="llama2",\n             verbose=True,\n             callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "RetrievalQA", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html", "title": "Ollama"}]--\x3e\n# QA chain\nfrom langchain.chains import RetrievalQA\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectorstore.as_retriever(),\n    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},\n)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'question = "What are the various approaches to Task Decomposition for AI Agents?"\nresult = qa_chain({"query": question})\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'    Task decomposition can be approached in different ways for AI agents, including:\n    \n    1. Using simple prompts like "Steps for XYZ." or "What are the subgoals for achieving XYZ?" to guide the LLM.\n    2. Providing task-specific instructions, such as "Write a story outline" for writing a novel.\n    3. Utilizing human inputs to help the AI agent understand the task and break it down into smaller steps.\n'))),(0,l.kt)("p",null,"You can also get logging for tokens."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "LLMResult", "source": "langchain.schema", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.output.LLMResult.html", "title": "Ollama"}, {"imported": "BaseCallbackHandler", "source": "langchain.callbacks.base", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.base.BaseCallbackHandler.html", "title": "Ollama"}]--\x3e\nfrom langchain.schema import LLMResult\nfrom langchain.callbacks.base import BaseCallbackHandler\n\nclass GenerationStatisticsCallback(BaseCallbackHandler):\n    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n        print(response.generations[0][0].generation_info)\n        \ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler(), GenerationStatisticsCallback()])\n\nllm = Ollama(base_url="http://localhost:11434",\n             model="llama2",\n             verbose=True,\n             callback_manager=callback_manager)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectorstore.as_retriever(),\n    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},\n)\n\nquestion = "What are the approaches to Task Decomposition?"\nresult = qa_chain({"query": question})\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    Task decomposition can be approached in three ways: (1) using simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions, or (3) with human inputs.{'model': 'llama2', 'created_at': '2023-08-08T04:01:09.005367Z', 'done': True, 'context': [1, 29871, 1, 13, 9314, 14816, 29903, 6778, 13, 13, 3492, 526, 263, 8444, 29892, 3390, 1319, 322, 15993, 20255, 29889, 29849, 1234, 408, 1371, 3730, 408, 1950, 29892, 1550, 1641, 9109, 29889, 3575, 6089, 881, 451, 3160, 738, 10311, 1319, 29892, 443, 621, 936, 29892, 11021, 391, 29892, 7916, 391, 29892, 304, 27375, 29892, 18215, 29892, 470, 27302, 2793, 29889, 3529, 9801, 393, 596, 20890, 526, 5374, 635, 443, 5365, 1463, 322, 6374, 297, 5469, 29889, 13, 13, 3644, 263, 1139, 947, 451, 1207, 738, 4060, 29892, 470, 338, 451, 2114, 1474, 16165, 261, 296, 29892, 5649, 2020, 2012, 310, 22862, 1554, 451, 1959, 29889, 960, 366, 1016, 29915, 29873, 1073, 278, 1234, 304, 263, 1139, 29892, 3113, 1016, 29915, 29873, 6232, 2089, 2472, 29889, 13, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 29961, 25580, 29962, 4803, 278, 1494, 12785, 310, 3030, 304, 1234, 278, 1139, 472, 278, 1095, 29889, 29871, 13, 3644, 366, 1016, 29915, 29873, 1073, 278, 1234, 29892, 925, 1827, 393, 366, 1016, 29915, 29873, 1073, 29892, 1016, 29915, 29873, 1018, 304, 1207, 701, 385, 1234, 29889, 29871, 13, 11403, 2211, 25260, 7472, 322, 3013, 278, 1234, 408, 3022, 895, 408, 1950, 29889, 29871, 13, 5398, 26227, 508, 367, 2309, 313, 29896, 29897, 491, 365, 26369, 411, 2560, 9508, 292, 763, 376, 7789, 567, 363, 1060, 29979, 29999, 7790, 29876, 29896, 19602, 376, 5618, 526, 278, 1014, 1484, 1338, 363, 3657, 15387, 1060, 29979, 29999, 29973, 613, 313, 29906, 29897, 491, 773, 3414, 29899, 14940, 11994, 29936, 321, 29889, 29887, 29889, 376, 6113, 263, 5828, 27887, 1213, 363, 5007, 263, 9554, 29892, 470, 313, 29941, 29897, 411, 5199, 10970, 29889, 13, 13, 5398, 26227, 508, 367, 2309, 313, 29896, 29897, 491, 365, 26369, 411, 2560, 9508, 292, 763, 376, 7789, 567, 363, 1060, 29979, 29999, 7790, 29876, 29896, 19602, 376, 5618, 526, 278, 1014, 1484, 1338, 363, 3657, 15387, 1060, 29979, 29999, 29973, 613, 313, 29906, 29897, 491, 773, 3414, 29899, 14940, 11994, 29936, 321, 29889, 29887, 29889, 376, 6113, 263, 5828, 27887, 1213, 363, 5007, 263, 9554, 29892, 470, 313, 29941, 29897, 411, 5199, 10970, 29889, 13, 13, 5398, 26227, 508, 367, 2309, 313, 29896, 29897, 491, 365, 26369, 411, 2560, 9508, 292, 763, 376, 7789, 567, 363, 1060, 29979, 29999, 7790, 29876, 29896, 19602, 376, 5618, 526, 278, 1014, 1484, 1338, 363, 3657, 15387, 1060, 29979, 29999, 29973, 613, 313, 29906, 29897, 491, 773, 3414, 29899, 14940, 11994, 29936, 321, 29889, 29887, 29889, 376, 6113, 263, 5828, 27887, 1213, 363, 5007, 263, 9554, 29892, 470, 313, 29941, 29897, 411, 5199, 10970, 29889, 13, 13, 1451, 16047, 267, 297, 1472, 29899, 8489, 18987, 322, 3414, 26227, 29901, 1858, 9450, 975, 263, 3309, 29891, 4955, 322, 17583, 3902, 8253, 278, 1650, 2913, 3933, 18066, 292, 29889, 365, 26369, 29879, 21117, 304, 10365, 13900, 746, 20050, 411, 15668, 4436, 29892, 3907, 963, 3109, 16424, 9401, 304, 25618, 1058, 5110, 515, 14260, 322, 1059, 29889, 13, 16492, 29901, 1724, 526, 278, 13501, 304, 9330, 897, 510, 3283, 29973, 13, 29648, 1319, 673, 29901, 518, 29914, 25580, 29962, 13, 5398, 26227, 508, 367, 26733, 297, 2211, 5837, 29901, 313, 29896, 29897, 773, 2560, 9508, 292, 763, 376, 7789, 567, 363, 1060, 29979, 29999, 7790, 29876, 29896, 19602, 376, 5618, 526, 278, 1014, 1484, 1338, 363, 3657, 15387, 1060, 29979, 29999, 29973, 613, 313, 29906, 29897, 491, 773, 3414, 29899, 14940, 11994, 29892, 470, 313, 29941, 29897, 411, 5199, 10970, 29889, 2], 'total_duration': 1364428708, 'load_duration': 1246375, 'sample_count': 62, 'sample_duration': 44859000, 'prompt_eval_count': 1, 'eval_count': 62, 'eval_duration': 1313002000}\n"))),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"eval_count")," / (",(0,l.kt)("inlineCode",{parentName:"p"},"eval_duration"),"/10e9)  gets ",(0,l.kt)("inlineCode",{parentName:"p"},"tok / s")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"62 / (1313002000/1000/1000/1000)\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    47.22003469910937\n"))))}h.isMDXComponent=!0}}]);