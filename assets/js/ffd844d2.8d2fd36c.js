"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[73182],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>g});var r=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=r.createContext({}),l=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=l(n),u=o,g=d["".concat(c,".").concat(u)]||d[u]||m[u]||a;return n?r.createElement(g,i(i({ref:t},p),{},{components:n})):r.createElement(g,i({ref:t},p))}));function g(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=u;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[d]="string"==typeof e?e:o,i[1]=s;for(var l=2;l<a;l++)i[l]=n[l];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},85865:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>g,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var r=n(87462),o=(n(67294),n(3905));const a={},i="Postgres Embedding",s={unversionedId:"integrations/vectorstores/pgembedding",id:"integrations/vectorstores/pgembedding",title:"Postgres Embedding",description:"Postgres Embedding is an open-source vector similarity search for Postgres that uses  Hierarchical Navigable Small Worlds (HNSW) for approximate nearest neighbor search.",source:"@site/docs/integrations/vectorstores/pgembedding.md",sourceDirName:"integrations/vectorstores",slug:"/integrations/vectorstores/pgembedding",permalink:"/langchain/docs/integrations/vectorstores/pgembedding",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"OpenSearch",permalink:"/langchain/docs/integrations/vectorstores/opensearch"},next:{title:"PGVector",permalink:"/langchain/docs/integrations/vectorstores/pgvector"}},c={},l=[{value:"Working with vectorstore in Postgres",id:"working-with-vectorstore-in-postgres",level:2},{value:"Uploading a vectorstore in PG",id:"uploading-a-vectorstore-in-pg",level:3},{value:"Create HNSW Index",id:"create-hnsw-index",level:3},{value:"Retrieving a vectorstore in PG",id:"retrieving-a-vectorstore-in-pg",level:3}],p=(d="CodeOutputBlock",function(e){return console.warn("Component "+d+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var d;const m={toc:l},u="wrapper";function g(e){let{components:t,...n}=e;return(0,o.kt)(u,(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"postgres-embedding"},"Postgres Embedding"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("a",{parentName:"p",href:"https://github.com/neondatabase/pg_embedding"},"Postgres Embedding")," is an open-source vector similarity search for ",(0,o.kt)("inlineCode",{parentName:"p"},"Postgres")," that uses  ",(0,o.kt)("inlineCode",{parentName:"p"},"Hierarchical Navigable Small Worlds (HNSW)")," for approximate nearest neighbor search.")),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"It supports:"),(0,o.kt)("ul",{parentName:"blockquote"},(0,o.kt)("li",{parentName:"ul"},"exact and approximate nearest neighbor search using HNSW"),(0,o.kt)("li",{parentName:"ul"},"L2 distance"))),(0,o.kt)("p",null,"This notebook shows how to use the Postgres vector database (",(0,o.kt)("inlineCode",{parentName:"p"},"PGEmbedding"),")."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"The PGEmbedding integration creates the pg_embedding extension for you, but you run the following Postgres query to add it:"),(0,o.kt)("pre",{parentName:"blockquote"},(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE EXTENSION embedding;\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"# Pip install necessary package\npip install openai\npip install psycopg2-binary\npip install tiktoken\n")),(0,o.kt)("p",null,"Add the OpenAI API Key to the environment variables to use ",(0,o.kt)("inlineCode",{parentName:"p"},"OpenAIEmbeddings"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport getpass\n\nos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")\n')),(0,o.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    OpenAI API Key:\xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"## Loading Environment Variables\nfrom typing import List, Tuple\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "OpenAIEmbeddings", "source": "langchain.embeddings.openai", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html", "title": "Postgres Embedding"}, {"imported": "CharacterTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html", "title": "Postgres Embedding"}, {"imported": "PGEmbedding", "source": "langchain.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.pgembedding.PGEmbedding.html", "title": "Postgres Embedding"}, {"imported": "TextLoader", "source": "langchain.document_loaders", "docs": "https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html", "title": "Postgres Embedding"}, {"imported": "Document", "source": "langchain.docstore.document", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.document.Document.html", "title": "Postgres Embedding"}]--\x3e\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import PGEmbedding\nfrom langchain.document_loaders import TextLoader\nfrom langchain.docstore.document import Document\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'os.environ["DATABASE_URL"] = getpass.getpass("Database Url:")\n')),(0,o.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    Database Url:\xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = TextLoader("state_of_the_union.txt")\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings()\nconnection_string = os.environ.get("DATABASE_URL")\ncollection_name = "state_of_the_union"\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'db = PGEmbedding.from_documents(\n    embedding=embeddings,\n    documents=docs,\n    collection_name=collection_name,\n    connection_string=connection_string,\n)\n\nquery = "What did the president say about Ketanji Brown Jackson"\ndocs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'for doc, score in docs_with_score:\n    print("-" * 80)\n    print("Score: ", score)\n    print(doc.page_content)\n    print("-" * 80)\n')),(0,o.kt)("h2",{id:"working-with-vectorstore-in-postgres"},"Working with vectorstore in Postgres"),(0,o.kt)("h3",{id:"uploading-a-vectorstore-in-pg"},"Uploading a vectorstore in PG"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"db = PGEmbedding.from_documents(\n    embedding=embeddings,\n    documents=docs,\n    collection_name=collection_name,\n    connection_string=connection_string,\n    pre_delete_collection=False,\n)\n")),(0,o.kt)("h3",{id:"create-hnsw-index"},"Create HNSW Index"),(0,o.kt)("p",null,"By default, the extension performs a sequential scan search, with 100% recall. You might consider creating an HNSW index for approximate nearest neighbor (ANN) search to speed up ",(0,o.kt)("inlineCode",{parentName:"p"},"similarity_search_with_score")," execution time. To create the HNSW index on your vector column, use a ",(0,o.kt)("inlineCode",{parentName:"p"},"create_hnsw_index")," function:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"PGEmbedding.create_hnsw_index(\n    max_elements=10000, dims=1536, m=8, ef_construction=16, ef_search=16\n)\n")),(0,o.kt)("p",null,"The function above is equivalent to running the below SQL query:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE INDEX ON vectors USING hnsw(vec) WITH (maxelements=10000, dims=1536, m=3, efconstruction=16, efsearch=16);\n")),(0,o.kt)("p",null,"The HNSW index options used in the statement above include:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},'maxelements: Defines the maximum number of elements indexed. This is a required parameter. The example shown above has a value of 3. A real-world example would have a much large value, such as 1000000. An "element" refers to a data point (a vector) in the dataset, which is represented as a node in the HNSW graph. Typically, you would set this option to a value able to accommodate the number of rows in your in your dataset.')),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"dims: Defines the number of dimensions in your vector data. This is a required parameter. A small value is used in the example above. If you are storing data generated using OpenAI's text-embedding-ada-002 model, which supports 1536 dimensions, you would define a value of 1536, for example.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},'m: Defines the maximum number of bi-directional links (also referred to as "edges") created for each node during graph construction.\nThe following additional index options are supported:')),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"efConstruction: Defines the number of nearest neighbors considered during index construction. The default value is 32.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"efsearch: Defines the number of nearest neighbors considered during index search. The default value is 32.\nFor information about how you can configure these options to influence the HNSW algorithm, refer to ",(0,o.kt)("a",{parentName:"p",href:"https://neon.tech/docs/extensions/pg_embedding#tuning-the-hnsw-algorithm"},"Tuning the HNSW algorithm"),"."))),(0,o.kt)("h3",{id:"retrieving-a-vectorstore-in-pg"},"Retrieving a vectorstore in PG"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"store = PGEmbedding(\n    connection_string=connection_string,\n    embedding_function=embeddings,\n    collection_name=collection_name,\n)\n\nretriever = store.as_retriever()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"retriever\n")),(0,o.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'db1 = PGEmbedding.from_existing_index(\n    embedding=embeddings,\n    collection_name=collection_name,\n    pre_delete_collection=False,\n    connection_string=connection_string,\n)\n\nquery = "What did the president say about Ketanji Brown Jackson"\ndocs_with_score: List[Tuple[Document, float]] = db1.similarity_search_with_score(query)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'for doc, score in docs_with_score:\n    print("-" * 80)\n    print("Score: ", score)\n    print(doc.page_content)\n    print("-" * 80)\n')))}g.isMDXComponent=!0}}]);