"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[18928],{3905:(e,n,a)=>{a.d(n,{Zo:()=>h,kt:()=>g});var t=a(67294);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function i(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function s(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},o=Object.keys(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var p=t.createContext({}),l=function(e){var n=t.useContext(p),a=n;return e&&(a="function"==typeof e?e(n):i(i({},n),e)),a},h=function(e){var n=l(e.components);return t.createElement(p.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},c=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,o=e.originalType,p=e.parentName,h=s(e,["components","mdxType","originalType","parentName"]),u=l(a),c=r,g=u["".concat(p,".").concat(c)]||u[c]||m[c]||o;return a?t.createElement(g,i(i({ref:n},h),{},{components:a})):t.createElement(g,i({ref:n},h))}));function g(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=c;var s={};for(var p in n)hasOwnProperty.call(n,p)&&(s[p]=n[p]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var l=2;l<o;l++)i[l]=a[l];return t.createElement.apply(null,i)}return t.createElement.apply(null,a)}c.displayName="MDXCreateElement"},49096:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>p,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var t=a(87462),r=(a(67294),a(3905));const o={},i="Memgraph QA chain",s={unversionedId:"use_cases/more/graph/graph_memgraph_qa",id:"use_cases/more/graph/graph_memgraph_qa",title:"Memgraph QA chain",description:"This notebook shows how to use LLMs to provide a natural language interface to a Memgraph database. To complete this tutorial, you will need Docker and Python 3.x installed.",source:"@site/docs/use_cases/more/graph/graph_memgraph_qa.md",sourceDirName:"use_cases/more/graph",slug:"/use_cases/more/graph/graph_memgraph_qa",permalink:"/langchain/docs/use_cases/more/graph/graph_memgraph_qa",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"use_cases",previous:{title:"KuzuQAChain",permalink:"/langchain/docs/use_cases/more/graph/graph_kuzu_qa"},next:{title:"NebulaGraphQAChain",permalink:"/langchain/docs/use_cases/more/graph/graph_nebula_qa"}},p={},l=[{value:"Populating the database",id:"populating-the-database",level:2},{value:"Refresh graph schema",id:"refresh-graph-schema",level:2},{value:"Querying the database",id:"querying-the-database",level:2},{value:"Chain modifiers",id:"chain-modifiers",level:2},{value:"Return direct query results",id:"return-direct-query-results",level:4},{value:"Return query intermediate steps",id:"return-query-intermediate-steps",level:4},{value:"Limit the number of query results",id:"limit-the-number-of-query-results",level:4},{value:"Prompt refinement",id:"prompt-refinement",level:3}],h={toc:l},u="wrapper";function m(e){let{components:n,...a}=e;return(0,r.kt)(u,(0,t.Z)({},h,a,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"memgraph-qa-chain"},"Memgraph QA chain"),(0,r.kt)("p",null,"This notebook shows how to use LLMs to provide a natural language interface to a ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/memgraph/memgraph"},"Memgraph")," database. To complete this tutorial, you will need ",(0,r.kt)("a",{parentName:"p",href:"https://www.docker.com/get-started/"},"Docker")," and ",(0,r.kt)("a",{parentName:"p",href:"https://www.python.org/"},"Python 3.x")," installed."),(0,r.kt)("p",null,"To follow along with this tutorial, ensure you have a running Memgraph instance. You can download and run it in a local Docker container by executing the following script:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'docker run \\\n    -it \\\n    -p 7687:7687 \\\n    -p 7444:7444 \\\n    -p 3000:3000 \\\n    -e MEMGRAPH="--bolt-server-name-for-init=Neo4j/" \\\n    -v mg_lib:/var/lib/memgraph memgraph/memgraph-platform\n')),(0,r.kt)("p",null,"You will need to wait a few seconds for the database to start. If the process completes successfully, you should see something like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"mgconsole X.X\nConnected to 'memgraph://127.0.0.1:7687'\nType :help for shell usage\nQuit the shell by typing Ctrl-D(eof) or :quit\nmemgraph>\n")),(0,r.kt)("p",null,"Now you can start playing with Memgraph!"),(0,r.kt)("p",null,"Begin by installing and importing all the necessary packages. We'll use the package manager called ",(0,r.kt)("a",{parentName:"p",href:"https://pip.pypa.io/en/stable/installation/"},"pip"),", along with the ",(0,r.kt)("inlineCode",{parentName:"p"},"--user")," flag, to ensure proper permissions. If you've installed Python 3.4 or a later version, pip is included by default. You can install all the required packages using the following command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"pip install langchain openai neo4j gqlalchemy --user\n")),(0,r.kt)("p",null,"You can either run the provided code blocks in this notebook or use a separate Python file to experiment with Memgraph and LangChain."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html", "title": "Memgraph QA chain"}, {"imported": "GraphCypherQAChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.cypher.GraphCypherQAChain.html", "title": "Memgraph QA chain"}, {"imported": "MemgraphGraph", "source": "langchain.graphs", "docs": "https://api.python.langchain.com/en/latest/graphs/langchain.graphs.memgraph_graph.MemgraphGraph.html", "title": "Memgraph QA chain"}]--\x3e\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import GraphCypherQAChain\nfrom langchain.graphs import MemgraphGraph\nfrom langchain import PromptTemplate\n\nfrom gqlalchemy import Memgraph\n\nimport os\n')),(0,r.kt)("p",null,"We're utilizing the Python library ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/memgraph/gqlalchemy"},"GQLAlchemy")," to establish a connection between our Memgraph database and Python script. To execute queries, we can set up a Memgraph instance as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"memgraph = Memgraph(host='127.0.0.1', port=7687)\n")),(0,r.kt)("h2",{id:"populating-the-database"},"Populating the database"),(0,r.kt)("p",null,"You can effortlessly populate your new, empty database using the Cypher query language. Don't worry if you don't grasp every line just yet, you can learn Cypher from the documentation ",(0,r.kt)("a",{parentName:"p",href:"https://memgraph.com/docs/cypher-manual/"},"here"),". Running the following script will execute a seeding query on the database, giving us data about a video game, including details like the publisher, available platforms, and genres. This data will serve as a basis for our work."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# Creating and executing the seeding query\nquery = """\n    MERGE (g:Game {name: "Baldur\'s Gate 3"})\n    WITH g, ["PlayStation 5", "Mac OS", "Windows", "Xbox Series X/S"] AS platforms,\n            ["Adventure", "Role-Playing Game", "Strategy"] AS genres\n    FOREACH (platform IN platforms |\n        MERGE (p:Platform {name: platform})\n        MERGE (g)-[:AVAILABLE_ON]->(p)\n    )\n    FOREACH (genre IN genres |\n        MERGE (gn:Genre {name: genre})\n        MERGE (g)-[:HAS_GENRE]->(gn)\n    )\n    MERGE (p:Publisher {name: "Larian Studios"})\n    MERGE (g)-[:PUBLISHED_BY]->(p);\n"""\n\nmemgraph.execute(query)\n')),(0,r.kt)("h2",{id:"refresh-graph-schema"},"Refresh graph schema"),(0,r.kt)("p",null,"You're all set to instantiate the Memgraph-LangChain graph using the following script. This interface will allow us to query our database using LangChain, automatically creating the required graph schema for generating Cypher queries through LLM."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'graph = MemgraphGraph(url="bolt://localhost:7687", username="", password="")\n')),(0,r.kt)("p",null,"If necessary, you can manually refresh the graph schema as follows."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"graph.refresh_schema()\n")),(0,r.kt)("p",null,"To familiarize yourself with the data and verify the updated graph schema, you can print it using the following statement."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"print(graph.get_schema)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Node properties are the following:\nNode name: 'Game', Node properties: [{'property': 'name', 'type': 'str'}]\nNode name: 'Platform', Node properties: [{'property': 'name', 'type': 'str'}]\nNode name: 'Genre', Node properties: [{'property': 'name', 'type': 'str'}]\nNode name: 'Publisher', Node properties: [{'property': 'name', 'type': 'str'}]\n\nRelationship properties are the following:\n\nThe relationships are the following:\n['(:Game)-[:AVAILABLE_ON]->(:Platform)']\n['(:Game)-[:HAS_GENRE]->(:Genre)']\n['(:Game)-[:PUBLISHED_BY]->(:Publisher)']\n")),(0,r.kt)("h2",{id:"querying-the-database"},"Querying the database"),(0,r.kt)("p",null,"To interact with the OpenAI API, you must configure your API key as an environment variable using the Python ",(0,r.kt)("a",{parentName:"p",href:"https://docs.python.org/3/library/os.html"},"os")," package. This ensures proper authorization for your requests. You can find more information on obtaining your API key ",(0,r.kt)("a",{parentName:"p",href:"https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key"},"here"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'os.environ["OPENAI_API_KEY"] = "your-key-here"\n')),(0,r.kt)("p",null,"You should create the graph chain using the following script, which will be utilized in the question-answering process based on your graph data. While it defaults to GPT-3.5-turbo, you might also consider experimenting with other models like ",(0,r.kt)("a",{parentName:"p",href:"https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4"},"GPT-4")," for notably improved Cypher queries and outcomes. We'll utilize the OpenAI chat, utilizing the key you previously configured. We'll set the temperature to zero, ensuring predictable and consistent answers. Additionally, we'll use our Memgraph-LangChain graph and set the verbose parameter, which defaults to False, to True to receive more detailed messages regarding query generation."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"chain = GraphCypherQAChain.from_llm(\n    ChatOpenAI(temperature=0), graph=graph, verbose=True, model_name='gpt-3.5-turbo'\n)\n")),(0,r.kt)("p",null,"Now you can start asking questions!"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response = chain.run("Which platforms is Baldur\'s Gate 3 available on?")\nprint(response)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"> Entering new GraphCypherQAChain chain...\nGenerated Cypher:\nMATCH (g:Game {name: 'Baldur\\'s Gate 3'})-[:AVAILABLE_ON]->(p:Platform)\nRETURN p.name\nFull Context:\n[{'p.name': 'PlayStation 5'}, {'p.name': 'Mac OS'}, {'p.name': 'Windows'}, {'p.name': 'Xbox Series X/S'}]\n\n> Finished chain.\nBaldur's Gate 3 is available on PlayStation 5, Mac OS, Windows, and Xbox Series X/S.\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response = chain.run("Is Baldur\'s Gate 3 available on Windows?")\nprint(response)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"> Entering new GraphCypherQAChain chain...\nGenerated Cypher:\nMATCH (:Game {name: 'Baldur\\'s Gate 3'})-[:AVAILABLE_ON]->(:Platform {name: 'Windows'})\nRETURN true\nFull Context:\n[{'true': True}]\n\n> Finished chain.\nYes, Baldur's Gate 3 is available on Windows.\n")),(0,r.kt)("h2",{id:"chain-modifiers"},"Chain modifiers"),(0,r.kt)("p",null,"To modify the behavior of your chain and obtain more context or additional information, you can modify the chain's parameters."),(0,r.kt)("h4",{id:"return-direct-query-results"},"Return direct query results"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"return_direct")," modifier specifies whether to return the direct results of the executed Cypher query or the processed natural language response."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Return the result of querying the graph directly\nchain = GraphCypherQAChain.from_llm(\n    ChatOpenAI(temperature=0), graph=graph, verbose=True, return_direct=True\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response = chain.run("Which studio published Baldur\'s Gate 3?")\nprint(response)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"> Entering new GraphCypherQAChain chain...\nGenerated Cypher:\nMATCH (:Game {name: 'Baldur\\'s Gate 3'})-[:PUBLISHED_BY]->(p:Publisher)\nRETURN p.name\n\n> Finished chain.\n[{'p.name': 'Larian Studios'}]\n")),(0,r.kt)("h4",{id:"return-query-intermediate-steps"},"Return query intermediate steps"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"return_intermediate_steps")," chain modifier enhances the returned response by including the intermediate steps of the query in addition to the initial query result."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Return all the intermediate steps of query execution\nchain = GraphCypherQAChain.from_llm(\n    ChatOpenAI(temperature=0), graph=graph, verbose=True, return_intermediate_steps=True\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response = chain("Is Baldur\'s Gate 3 an Adventure game?")\nprint(f"Intermediate steps: {response[\'intermediate_steps\']}")\nprint(f"Final response: {response[\'result\']}")\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"> Entering new GraphCypherQAChain chain...\nGenerated Cypher:\nMATCH (g:Game {name: 'Baldur\\'s Gate 3'})-[:HAS_GENRE]->(genre:Genre {name: 'Adventure'})\nRETURN g, genre\nFull Context:\n[{'g': {'name': \"Baldur's Gate 3\"}, 'genre': {'name': 'Adventure'}}]\n\n> Finished chain.\nIntermediate steps: [{'query': \"MATCH (g:Game {name: 'Baldur\\\\'s Gate 3'})-[:HAS_GENRE]->(genre:Genre {name: 'Adventure'})\\nRETURN g, genre\"}, {'context': [{'g': {'name': \"Baldur's Gate 3\"}, 'genre': {'name': 'Adventure'}}]}]\nFinal response: Yes, Baldur's Gate 3 is an Adventure game.\n")),(0,r.kt)("h4",{id:"limit-the-number-of-query-results"},"Limit the number of query results"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"top_k")," modifier can be used when you want to restrict the maximum number of query results."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Limit the maximum number of results returned by query\nchain = GraphCypherQAChain.from_llm(\n    ChatOpenAI(temperature=0), graph=graph, verbose=True, top_k=2\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response = chain.run("What genres are associated with Baldur\'s Gate 3?")\nprint(response)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"> Entering new GraphCypherQAChain chain...\nGenerated Cypher:\nMATCH (:Game {name: 'Baldur\\'s Gate 3'})-[:HAS_GENRE]->(g:Genre)\nRETURN g.name\nFull Context:\n[{'g.name': 'Adventure'}, {'g.name': 'Role-Playing Game'}]\n\n> Finished chain.\nBaldur's Gate 3 is associated with the genres Adventure and Role-Playing Game.\n")),(0,r.kt)("h1",{id:"advanced-querying"},"Advanced querying"),(0,r.kt)("p",null,"As the complexity of your solution grows, you might encounter different use-cases that require careful handling. Ensuring your application's scalability is essential to maintain a smooth user flow without any hitches."),(0,r.kt)("p",null,"Let's instantiate our chain once again and attempt to ask some questions that users might potentially ask."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"chain = GraphCypherQAChain.from_llm(\n    ChatOpenAI(temperature=0), graph=graph, verbose=True, model_name='gpt-3.5-turbo'\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response = chain.run("Is Baldur\'s Gate 3 available on PS5?")\nprint(response)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"> Entering new GraphCypherQAChain chain...\nGenerated Cypher:\nMATCH (g:Game {name: 'Baldur\\'s Gate 3'})-[:AVAILABLE_ON]->(p:Platform {name: 'PS5'})\nRETURN g.name, p.name\nFull Context:\n[]\n\n> Finished chain.\nI'm sorry, but I don't have the information to answer your question.\n")),(0,r.kt)("p",null,"The generated Cypher query looks fine, but we didn't receive any information in response. This illustrates a common challenge when working with LLMs - the misalignment between how users phrase queries and how data is stored. In this case, the difference between user perception and the actual data storage can cause mismatches. Prompt refinement, the process of honing the model's prompts to better grasp these distinctions, is an efficient solution that tackles this issue. Through prompt refinement, the model gains increased proficiency in generating precise and pertinent queries, leading to the successful retrieval of the desired data."),(0,r.kt)("h3",{id:"prompt-refinement"},"Prompt refinement"),(0,r.kt)("p",null,"To address this, we can adjust the initial Cypher prompt of the QA chain. This involves adding guidance to the LLM on how users can refer to specific platforms, such as PS5 in our case. We achieve this using the LangChain ",(0,r.kt)("a",{parentName:"p",href:"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/"},"PromptTemplate"),", creating a modified initial prompt. This modified prompt is then supplied as an argument to our refined Memgraph-LangChain instance."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'CYPHER_GENERATION_TEMPLATE = """\nTask:Generate Cypher statement to query a graph database.\nInstructions:\nUse only the provided relationship types and properties in the schema.\nDo not use any other relationship types or properties that are not provided.\nSchema:\n{schema}\nNote: Do not include any explanations or apologies in your responses.\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\nDo not include any text except the generated Cypher statement.\nIf the user asks about PS5, Play Station 5 or PS 5, that is the platform called PlayStation 5.\n\nThe question is:\n{question}\n"""\n\nCYPHER_GENERATION_PROMPT = PromptTemplate(\n    input_variables=["schema", "question"], template=CYPHER_GENERATION_TEMPLATE\n)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"chain = GraphCypherQAChain.from_llm(\n    ChatOpenAI(temperature=0), \n    cypher_prompt=CYPHER_GENERATION_PROMPT,\n    graph=graph, \n    verbose=True, \n    model_name='gpt-3.5-turbo'\n)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response = chain.run("Is Baldur\'s Gate 3 available on PS5?")\nprint(response)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"> Entering new GraphCypherQAChain chain...\nGenerated Cypher:\nMATCH (g:Game {name: 'Baldur\\'s Gate 3'})-[:AVAILABLE_ON]->(p:Platform {name: 'PlayStation 5'})\nRETURN g.name, p.name\nFull Context:\n[{'g.name': \"Baldur's Gate 3\", 'p.name': 'PlayStation 5'}]\n\n> Finished chain.\nYes, Baldur's Gate 3 is available on PlayStation 5.\n")),(0,r.kt)("p",null,"Now, with the revised initial Cypher prompt that includes guidance on platform naming, we are obtaining accurate and relevant results that align more closely with user queries. "),(0,r.kt)("p",null,"This approach allows for further improvement of your QA chain. You can effortlessly integrate extra prompt refinement data into your chain, thereby enhancing the overall user experience of your app."))}m.isMDXComponent=!0}}]);