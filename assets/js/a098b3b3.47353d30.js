"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[54626],{3905:(e,n,a)=>{a.d(n,{Zo:()=>m,kt:()=>d});var t=a(67294);function l(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function r(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach((function(n){l(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function i(e,n){if(null==e)return{};var a,t,l=function(e,n){if(null==e)return{};var a,t,l={},o=Object.keys(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||(l[a]=e[a]);return l}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var p=t.createContext({}),c=function(e){var n=t.useContext(p),a=n;return e&&(a="function"==typeof e?e(n):r(r({},n),e)),a},m=function(e){var n=c(e.components);return t.createElement(p.Provider,{value:n},e.children)},u="mdxType",s={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},g=t.forwardRef((function(e,n){var a=e.components,l=e.mdxType,o=e.originalType,p=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),u=c(a),g=l,d=u["".concat(p,".").concat(g)]||u[g]||s[g]||o;return a?t.createElement(d,r(r({ref:n},m),{},{components:a})):t.createElement(d,r({ref:n},m))}));function d(e,n){var a=arguments,l=n&&n.mdxType;if("string"==typeof e||l){var o=a.length,r=new Array(o);r[0]=g;var i={};for(var p in n)hasOwnProperty.call(n,p)&&(i[p]=n[p]);i.originalType=e,i[u]="string"==typeof e?e:l,r[1]=i;for(var c=2;c<o;c++)r[c]=a[c];return t.createElement.apply(null,r)}return t.createElement.apply(null,a)}g.displayName="MDXCreateElement"},82694:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>p,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var t=a(87462),l=(a(67294),a(3905));const o={},r="Hugging Face Hub",i={unversionedId:"integrations/llms/huggingface_hub",id:"integrations/llms/huggingface_hub",title:"Hugging Face Hub",description:"The Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.",source:"@site/docs/integrations/llms/huggingface_hub.md",sourceDirName:"integrations/llms",slug:"/integrations/llms/huggingface_hub",permalink:"/langchain/docs/integrations/llms/huggingface_hub",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"GPT4All",permalink:"/langchain/docs/integrations/llms/gpt4all"},next:{title:"Hugging Face Local Pipelines",permalink:"/langchain/docs/integrations/llms/huggingface_pipelines"}},p={},c=[{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"Prepare Examples",id:"prepare-examples",level:2},{value:"Examples",id:"examples",level:2},{value:"<code>Flan</code>, by <code>Google</code>",id:"flan-by-google",level:3},{value:"<code>Dolly</code>, by <code>Databricks</code>",id:"dolly-by-databricks",level:3},{value:"<code>Camel</code>, by <code>Writer</code>",id:"camel-by-writer",level:3},{value:"<code>XGen</code>, by <code>Salesforce</code>",id:"xgen-by-salesforce",level:3},{value:"<code>Falcon</code>, by <code>Technology Innovation Institute (TII)</code>",id:"falcon-by-technology-innovation-institute-tii",level:3},{value:"<code>InternLM-Chat</code>, by <code>Shanghai AI Laboratory</code>",id:"internlm-chat-by-shanghai-ai-laboratory",level:3},{value:"<code>Qwen</code>, by <code>Alibaba Cloud</code>",id:"qwen-by-alibaba-cloud",level:3}],m=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,l.kt)("div",e)});var u;const s={toc:c},g="wrapper";function d(e){let{components:n,...a}=e;return(0,l.kt)(g,(0,t.Z)({},s,a,{components:n,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"hugging-face-hub"},"Hugging Face Hub"),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"The ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/docs/hub/index"},"Hugging Face Hub")," is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.")),(0,l.kt)("p",null,"This example showcases how to connect to the ",(0,l.kt)("inlineCode",{parentName:"p"},"Hugging Face Hub")," and use different models."),(0,l.kt)("h2",{id:"installation-and-setup"},"Installation and Setup"),(0,l.kt)("p",null,"To use, you should have the ",(0,l.kt)("inlineCode",{parentName:"p"},"huggingface_hub")," python ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/docs/huggingface_hub/installation"},"package installed"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"pip install huggingface_hub\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n\nfrom getpass import getpass\n\nHUGGINGFACEHUB_API_TOKEN = getpass()\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"     \xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN\n')),(0,l.kt)("h2",{id:"prepare-examples"},"Prepare Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from langchain import HuggingFaceHub\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from langchain import PromptTemplate, LLMChain\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'question = "Who won the FIFA World Cup in the year 1994? "\n\ntemplate = """Question: {question}\n\nAnswer: Let\'s think step by step."""\n\nprompt = PromptTemplate(template=template, input_variables=["question"])\n')),(0,l.kt)("h2",{id:"examples"},"Examples"),(0,l.kt)("p",null,"Below are some examples of models you can access through the ",(0,l.kt)("inlineCode",{parentName:"p"},"Hugging Face Hub")," integration."),(0,l.kt)("h3",{id:"flan-by-google"},(0,l.kt)("inlineCode",{parentName:"h3"},"Flan"),", by ",(0,l.kt)("inlineCode",{parentName:"h3"},"Google")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'repo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'llm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nprint(llm_chain.run(question))\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994\n"))),(0,l.kt)("h3",{id:"dolly-by-databricks"},(0,l.kt)("inlineCode",{parentName:"h3"},"Dolly"),", by ",(0,l.kt)("inlineCode",{parentName:"h3"},"Databricks")),(0,l.kt)("p",null,"See ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/databricks"},"Databricks")," organization page for a list of available models."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'repo_id = "databricks/dolly-v2-3b"\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'llm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"     First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.\n    \n    \n    Question: Who\n"))),(0,l.kt)("h3",{id:"camel-by-writer"},(0,l.kt)("inlineCode",{parentName:"h3"},"Camel"),", by ",(0,l.kt)("inlineCode",{parentName:"h3"},"Writer")),(0,l.kt)("p",null,"See ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/Writer"},"Writer's")," organization page for a list of available models."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other options\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'llm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n')),(0,l.kt)("h3",{id:"xgen-by-salesforce"},(0,l.kt)("inlineCode",{parentName:"h3"},"XGen"),", by ",(0,l.kt)("inlineCode",{parentName:"h3"},"Salesforce")),(0,l.kt)("p",null,"See ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/salesforce/xgen"},"more information"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'repo_id = "Salesforce/xgen-7b-8k-base"\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'llm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n')),(0,l.kt)("h3",{id:"falcon-by-technology-innovation-institute-tii"},(0,l.kt)("inlineCode",{parentName:"h3"},"Falcon"),", by ",(0,l.kt)("inlineCode",{parentName:"h3"},"Technology Innovation Institute (TII)")),(0,l.kt)("p",null,"See ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/tiiuae/falcon-40b"},"more information"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'repo_id = "tiiuae/falcon-40b"\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'llm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n')),(0,l.kt)("h3",{id:"internlm-chat-by-shanghai-ai-laboratory"},(0,l.kt)("inlineCode",{parentName:"h3"},"InternLM-Chat"),", by ",(0,l.kt)("inlineCode",{parentName:"h3"},"Shanghai AI Laboratory")),(0,l.kt)("p",null,"See ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/internlm/internlm-7b"},"more information"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'repo_id = "internlm/internlm-chat-7b"\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'llm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n')),(0,l.kt)("h3",{id:"qwen-by-alibaba-cloud"},(0,l.kt)("inlineCode",{parentName:"h3"},"Qwen"),", by ",(0,l.kt)("inlineCode",{parentName:"h3"},"Alibaba Cloud")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},(0,l.kt)("inlineCode",{parentName:"p"},"Tongyi Qianwen-7B")," (",(0,l.kt)("inlineCode",{parentName:"p"},"Qwen-7B"),") is a model with a scale of 7 billion parameters in the ",(0,l.kt)("inlineCode",{parentName:"p"},"Tongyi Qianwen")," large model series developed by ",(0,l.kt)("inlineCode",{parentName:"p"},"Alibaba Cloud"),". ",(0,l.kt)("inlineCode",{parentName:"p"},"Qwen-7B")," is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data.")),(0,l.kt)("p",null,"See ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/Qwen/Qwen-7B"},"more information on HuggingFace")," of on ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/QwenLM/Qwen-7B"},"GitHub"),"."),(0,l.kt)("p",null,"See here a ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/QwenLM/Qwen-7B/blob/main/examples/langchain_tooluse.ipynb"},"big example for LangChain integration and Qwen"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'repo_id = "Qwen/Qwen-7B"\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'llm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n')))}d.isMDXComponent=!0}}]);