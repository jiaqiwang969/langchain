"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[33292],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>h});var a=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var c=a.createContext({}),l=function(e){var n=a.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},u=function(e){var n=l(e.components);return a.createElement(c.Provider,{value:n},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,o=e.mdxType,r=e.originalType,c=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),p=l(t),m=o,h=p["".concat(c,".").concat(m)]||p[m]||d[m]||r;return t?a.createElement(h,s(s({ref:n},u),{},{components:t})):a.createElement(h,s({ref:n},u))}));function h(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var r=t.length,s=new Array(r);s[0]=m;var i={};for(var c in n)hasOwnProperty.call(n,c)&&(i[c]=n[c]);i.originalType=e,i[p]="string"==typeof e?e:o,s[1]=i;for(var l=2;l<r;l++)s[l]=t[l];return a.createElement.apply(null,s)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},54681:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>l});var a=t(87462),o=(t(67294),t(3905));const r={},s="Structure answers with OpenAI functions",i={unversionedId:"use_cases/question_answering/integrations/openai_functions_retrieval_qa",id:"use_cases/question_answering/integrations/openai_functions_retrieval_qa",title:"Structure answers with OpenAI functions",description:"OpenAI functions allows for structuring of response output. This is often useful in question answering when you want to not only get the final answer but also supporting evidence, citations, etc.",source:"@site/docs/use_cases/question_answering/integrations/openai_functions_retrieval_qa.md",sourceDirName:"use_cases/question_answering/integrations",slug:"/use_cases/question_answering/integrations/openai_functions_retrieval_qa",permalink:"/langchain/docs/use_cases/question_answering/integrations/openai_functions_retrieval_qa",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"use_cases",previous:{title:"Retrieve from vector stores directly",permalink:"/langchain/docs/use_cases/question_answering/how_to/vector_db_text_generation"},next:{title:"QA using Activeloop's DeepLake",permalink:"/langchain/docs/use_cases/question_answering/integrations/semantic-search-over-chat"}},c={},l=[{value:"Using Pydantic",id:"using-pydantic",level:2},{value:"Using in ConversationalRetrievalChain",id:"using-in-conversationalretrievalchain",level:2},{value:"Using your own output schema",id:"using-your-own-output-schema",level:2}],u=(p="CodeOutputBlock",function(e){return console.warn("Component "+p+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var p;const d={toc:l},m="wrapper";function h(e){let{components:n,...t}=e;return(0,o.kt)(m,(0,a.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"structure-answers-with-openai-functions"},"Structure answers with OpenAI functions"),(0,o.kt)("p",null,"OpenAI functions allows for structuring of response output. This is often useful in question answering when you want to not only get the final answer but also supporting evidence, citations, etc."),(0,o.kt)("p",null,"In this notebook we show how to use an LLM chain which uses OpenAI functions as part of an overall retrieval pipeline."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'loader = TextLoader("../../state_of_the_union.txt", encoding="utf-8")\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\nfor i, text in enumerate(texts):\n    text.metadata["source"] = f"{i}-pl"\nembeddings = OpenAIEmbeddings()\ndocsearch = Chroma.from_documents(texts, embeddings)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.chat_models import ChatOpenAI\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import create_qa_with_sources_chain\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"qa_chain = create_qa_with_sources_chain(llm)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'doc_prompt = PromptTemplate(\n    template="Content: {page_content}\\nSource: {source}",\n    input_variables=["page_content", "source"],\n)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'final_qa_chain = StuffDocumentsChain(\n    llm_chain=qa_chain,\n    document_variable_name="context",\n    document_prompt=doc_prompt,\n)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"retrieval_qa = RetrievalQA(\n    retriever=docsearch.as_retriever(), combine_documents_chain=final_qa_chain\n)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'query = "What did the president say about russia"\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"retrieval_qa.run(query)\n")),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'    \'{\\n  "answer": "The President expressed strong condemnation of Russia\\\'s actions in Ukraine and announced measures to isolate Russia and provide support to Ukraine. He stated that Russia\\\'s invasion of Ukraine will have long-term consequences for Russia and emphasized the commitment to defend NATO countries. The President also mentioned taking robust action through sanctions and releasing oil reserves to mitigate gas prices. Overall, the President conveyed a message of solidarity with Ukraine and determination to protect American interests.",\\n  "sources": ["0-pl", "4-pl", "5-pl", "6-pl"]\\n}\'\n'))),(0,o.kt)("h2",{id:"using-pydantic"},"Using Pydantic"),(0,o.kt)("p",null,"If we want to, we can set the chain to return in Pydantic. Note that if downstream chains consume the output of this chain - including memory - they will generally expect it to be in string format, so you should only use this chain when it is the final chain."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'qa_chain_pydantic = create_qa_with_sources_chain(llm, output_parser="pydantic")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'final_qa_chain_pydantic = StuffDocumentsChain(\n    llm_chain=qa_chain_pydantic,\n    document_variable_name="context",\n    document_prompt=doc_prompt,\n)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"retrieval_qa_pydantic = RetrievalQA(\n    retriever=docsearch.as_retriever(), combine_documents_chain=final_qa_chain_pydantic\n)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"retrieval_qa_pydantic.run(query)\n")),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    AnswerWithSources(answer=\"The President expressed strong condemnation of Russia's actions in Ukraine and announced measures to isolate Russia and provide support to Ukraine. He stated that Russia's invasion of Ukraine will have long-term consequences for Russia and emphasized the commitment to defend NATO countries. The President also mentioned taking robust action through sanctions and releasing oil reserves to mitigate gas prices. Overall, the President conveyed a message of solidarity with Ukraine and determination to protect American interests.\", sources=['0-pl', '4-pl', '5-pl', '6-pl'])\n"))),(0,o.kt)("h2",{id:"using-in-conversationalretrievalchain"},"Using in ConversationalRetrievalChain"),(0,o.kt)("p",null,"We can also show what it's like to use this in the ConversationalRetrievalChain. Note that because this chain involves memory, we will NOT use the Pydantic return type."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import LLMChain\n\nmemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)\n_template = """Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\\nMake sure to avoid using any unclear pronouns.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:"""\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\ncondense_question_chain = LLMChain(\n    llm=llm,\n    prompt=CONDENSE_QUESTION_PROMPT,\n)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"qa = ConversationalRetrievalChain(\n    question_generator=condense_question_chain,\n    retriever=docsearch.as_retriever(),\n    memory=memory,\n    combine_docs_chain=final_qa_chain,\n)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'query = "What did the president say about Ketanji Brown Jackson"\nresult = qa({"question": query})\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"result\n")),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    {'question': 'What did the president say about Ketanji Brown Jackson',\n     'chat_history': [HumanMessage(content='What did the president say about Ketanji Brown Jackson', additional_kwargs={}, example=False),\n      AIMessage(content='{\\n  \"answer\": \"The President nominated Ketanji Brown Jackson as a Circuit Court of Appeals Judge and praised her as one of the nation\\'s top legal minds who will continue Justice Breyer\\'s legacy of excellence.\",\\n  \"sources\": [\"31-pl\"]\\n}', additional_kwargs={}, example=False)],\n     'answer': '{\\n  \"answer\": \"The President nominated Ketanji Brown Jackson as a Circuit Court of Appeals Judge and praised her as one of the nation\\'s top legal minds who will continue Justice Breyer\\'s legacy of excellence.\",\\n  \"sources\": [\"31-pl\"]\\n}'}\n"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'query = "what did he say about her predecessor?"\nresult = qa({"question": query})\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"result\n")),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'    {\'question\': \'what did he say about her predecessor?\',\n     \'chat_history\': [HumanMessage(content=\'What did the president say about Ketanji Brown Jackson\', additional_kwargs={}, example=False),\n      AIMessage(content=\'{\\n  "answer": "The President nominated Ketanji Brown Jackson as a Circuit Court of Appeals Judge and praised her as one of the nation\\\'s top legal minds who will continue Justice Breyer\\\'s legacy of excellence.",\\n  "sources": ["31-pl"]\\n}\', additional_kwargs={}, example=False),\n      HumanMessage(content=\'what did he say about her predecessor?\', additional_kwargs={}, example=False),\n      AIMessage(content=\'{\\n  "answer": "The President honored Justice Stephen Breyer for his service as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.",\\n  "sources": ["31-pl"]\\n}\', additional_kwargs={}, example=False)],\n     \'answer\': \'{\\n  "answer": "The President honored Justice Stephen Breyer for his service as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.",\\n  "sources": ["31-pl"]\\n}\'}\n'))),(0,o.kt)("h2",{id:"using-your-own-output-schema"},"Using your own output schema"),(0,o.kt)("p",null,"We can change the outputs of our chain by passing in our own schema. The values and descriptions of this schema will inform the function we pass to the OpenAI API, meaning it won't just affect how we parse outputs but will also change the OpenAI output itself. For example we can add a ",(0,o.kt)("inlineCode",{parentName:"p"},"countries_referenced")," parameter to our schema and describe what we want this parameter to mean, and that'll cause the OpenAI output to include a description of a speaker in the response."),(0,o.kt)("p",null,"In addition to the previous example, we can also add a custom prompt to the chain. This will allow you to add additional context to the response, which can be useful for question answering."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from typing import List\n\nfrom pydantic import BaseModel, Field\n\nfrom langchain.chains.openai_functions import create_qa_with_structure_chain\n\nfrom langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\nfrom langchain.schema import SystemMessage, HumanMessage\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'class CustomResponseSchema(BaseModel):\n    """An answer to the question being asked, with sources."""\n\n    answer: str = Field(..., description="Answer to the question that was asked")\n    countries_referenced: List[str] = Field(\n        ..., description="All of the countries mentioned in the sources"\n    )\n    sources: List[str] = Field(\n        ..., description="List of sources used to answer the question"\n    )\n\n\nprompt_messages = [\n    SystemMessage(\n        content=(\n            "You are a world class algorithm to answer "\n            "questions in a specific format."\n        )\n    ),\n    HumanMessage(content="Answer question using the following context"),\n    HumanMessagePromptTemplate.from_template("{context}"),\n    HumanMessagePromptTemplate.from_template("Question: {question}"),\n    HumanMessage(\n        content="Tips: Make sure to answer in the correct format. Return all of the countries mentioned in the sources in uppercase characters."\n    ),\n]\n\nchain_prompt = ChatPromptTemplate(messages=prompt_messages)\n\nqa_chain_pydantic = create_qa_with_structure_chain(\n    llm, CustomResponseSchema, output_parser="pydantic", prompt=chain_prompt\n)\nfinal_qa_chain_pydantic = StuffDocumentsChain(\n    llm_chain=qa_chain_pydantic,\n    document_variable_name="context",\n    document_prompt=doc_prompt,\n)\nretrieval_qa_pydantic = RetrievalQA(\n    retriever=docsearch.as_retriever(), combine_documents_chain=final_qa_chain_pydantic\n)\nquery = "What did he say about russia"\nretrieval_qa_pydantic.run(query)\n')),(0,o.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    CustomResponseSchema(answer=\"He announced that American airspace will be closed off to all Russian flights, further isolating Russia and adding an additional squeeze on their economy. The Ruble has lost 30% of its value and the Russian stock market has lost 40% of its value. He also mentioned that Putin alone is to blame for Russia's reeling economy. The United States and its allies are providing support to Ukraine in their fight for freedom, including military, economic, and humanitarian assistance. The United States is giving more than $1 billion in direct assistance to Ukraine. He made it clear that American forces are not engaged and will not engage in conflict with Russian forces in Ukraine, but they are deployed to defend NATO allies in case Putin decides to keep moving west. He also mentioned that Putin's attack on Ukraine was premeditated and unprovoked, and that the West and NATO responded by building a coalition of freedom-loving nations to confront Putin. The free world is holding Putin accountable through powerful economic sanctions, cutting off Russia's largest banks from the international financial system, and preventing Russia's central bank from defending the Russian Ruble. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs.\", countries_referenced=['AMERICA', 'RUSSIA', 'UKRAINE'], sources=['4-pl', '5-pl', '2-pl', '3-pl'])\n"))))}h.isMDXComponent=!0}}]);