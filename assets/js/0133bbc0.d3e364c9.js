"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[23191],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>g});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),p=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},m=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),c=p(n),u=a,g=c["".concat(l,".").concat(u)]||c[u]||d[u]||o;return n?r.createElement(g,s(s({ref:t},m),{},{components:n})):r.createElement(g,s({ref:t},m))}));function g(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,s=new Array(o);s[0]=u;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[c]="string"==typeof e?e:a,s[1]=i;for(var p=2;p<o;p++)s[p]=n[p];return r.createElement.apply(null,s)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},2266:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>c,default:()=>y,frontMatter:()=>m,metadata:()=>d,toc:()=>g});var r=n(87462),a=(n(67294),n(3905));const o=(s="CodeOutputBlock",function(e){return console.warn("Component "+s+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var s;const i={toc:[]},l="wrapper";function p(e){let{components:t,...n}=e;return(0,a.kt)(l,(0,r.Z)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"Currently, we support streaming for a broad range of LLM implementations, including but not limited to ",(0,a.kt)("inlineCode",{parentName:"p"},"OpenAI"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"ChatOpenAI"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"ChatAnthropic"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"Hugging Face Text Generation Inference"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"Replicate"),". This feature has been expanded to accommodate most of the models. To utilize streaming, use a ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/hwchase17/langchain/blob/master/langchain/callbacks/base.py"},(0,a.kt)("inlineCode",{parentName:"a"},"CallbackHandler"))," that implements ",(0,a.kt)("inlineCode",{parentName:"p"},"on_llm_new_token"),". In this example, we are using ",(0,a.kt)("inlineCode",{parentName:"p"},"StreamingStdOutCallbackHandler"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.llms import OpenAI\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\n\nllm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\nresp = llm("Write me a song about sparkling water.")\n')),(0,a.kt)(o,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    Verse 1\n    I'm sippin' on sparkling water,\n    It's so refreshing and light,\n    It's the perfect way to quench my thirst\n    On a hot summer night.\n    \n    Chorus\n    Sparkling water, sparkling water,\n    It's the best way to stay hydrated,\n    It's so crisp and so clean,\n    It's the perfect way to stay refreshed.\n    \n    Verse 2\n    I'm sippin' on sparkling water,\n    It's so bubbly and bright,\n    It's the perfect way to cool me down\n    On a hot summer night.\n    \n    Chorus\n    Sparkling water, sparkling water,\n    It's the best way to stay hydrated,\n    It's so crisp and so clean,\n    It's the perfect way to stay refreshed.\n    \n    Verse 3\n    I'm sippin' on sparkling water,\n    It's so light and so clear,\n    It's the perfect way to keep me cool\n    On a hot summer night.\n    \n    Chorus\n    Sparkling water, sparkling water,\n    It's the best way to stay hydrated,\n    It's so crisp and so clean,\n    It's the perfect way to stay refreshed.\n"))),(0,a.kt)("p",null,"We still have access to the end ",(0,a.kt)("inlineCode",{parentName:"p"},"LLMResult")," if using ",(0,a.kt)("inlineCode",{parentName:"p"},"generate"),". However, ",(0,a.kt)("inlineCode",{parentName:"p"},"token_usage")," is not currently supported for streaming."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'llm.generate(["Tell me a joke."])\n')),(0,a.kt)(o,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    Q: What did the fish say when it hit the wall?\n    A: Dam!\n\n\n    LLMResult(generations=[[Generation(text='\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {}, 'model_name': 'text-davinci-003'})\n"))))}p.isMDXComponent=!0;const m={},c="Streaming",d={unversionedId:"modules/model_io/models/llms/streaming_llm",id:"modules/model_io/models/llms/streaming_llm",title:"Streaming",description:"Some LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.",source:"@site/docs/modules/model_io/models/llms/streaming_llm.mdx",sourceDirName:"modules/model_io/models/llms",slug:"/modules/model_io/models/llms/streaming_llm",permalink:"/langchain/docs/modules/model_io/models/llms/streaming_llm",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Serialization",permalink:"/langchain/docs/modules/model_io/models/llms/llm_serialization"},next:{title:"Tracking token usage",permalink:"/langchain/docs/modules/model_io/models/llms/token_usage_tracking"}},u={},g=[],h={toc:g},f="wrapper";function y(e){let{components:t,...n}=e;return(0,a.kt)(f,(0,r.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"streaming"},"Streaming"),(0,a.kt)("p",null,"Some LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated."),(0,a.kt)(p,{mdxType:"StreamingLLM"}))}y.isMDXComponent=!0}}]);