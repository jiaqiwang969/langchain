"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[48383],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>d});var a=n(67294);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,l=e.mdxType,r=e.originalType,p=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),m=s(n),h=l,d=m["".concat(p,".").concat(h)]||m[h]||u[h]||r;return n?a.createElement(d,o(o({ref:t},c),{},{components:n})):a.createElement(d,o({ref:t},c))}));function d(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=n.length,o=new Array(r);o[0]=h;var i={};for(var p in t)hasOwnProperty.call(t,p)&&(i[p]=t[p]);i.originalType=e,i[m]="string"==typeof e?e:l,o[1]=i;for(var s=2;s<r;s++)o[s]=n[s];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},81440:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>s});var a=n(87462),l=(n(67294),n(3905));const r={},o="Petals",i={unversionedId:"integrations/llms/petals",id:"integrations/llms/petals",title:"Petals",description:"Petals runs 100B+ language models at home, BitTorrent-style.",source:"@site/docs/integrations/llms/petals.md",sourceDirName:"integrations/llms",slug:"/integrations/llms/petals",permalink:"/langchain/docs/integrations/llms/petals",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"OpenLM",permalink:"/langchain/docs/integrations/llms/openlm"},next:{title:"PipelineAI",permalink:"/langchain/docs/integrations/llms/pipelineai"}},p={},s=[{value:"Install petals",id:"install-petals",level:2},{value:"Imports",id:"imports",level:2},{value:"Set the Environment API Key",id:"set-the-environment-api-key",level:2},{value:"Create the Petals instance",id:"create-the-petals-instance",level:2},{value:"Create a Prompt Template",id:"create-a-prompt-template",level:2},{value:"Initiate the LLMChain",id:"initiate-the-llmchain",level:2},{value:"Run the LLMChain",id:"run-the-llmchain",level:2}],c=(m="CodeOutputBlock",function(e){return console.warn("Component "+m+" was not imported, exported, or provided by MDXProvider as global scope"),(0,l.kt)("div",e)});var m;const u={toc:s},h="wrapper";function d(e){let{components:t,...n}=e;return(0,l.kt)(h,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"petals"},"Petals"),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"Petals")," runs 100B+ language models at home, BitTorrent-style."),(0,l.kt)("p",null,"This notebook goes over how to use Langchain with ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/bigscience-workshop/petals"},"Petals"),"."),(0,l.kt)("h2",{id:"install-petals"},"Install petals"),(0,l.kt)("p",null,"The ",(0,l.kt)("inlineCode",{parentName:"p"},"petals")," package is required to use the Petals API. Install ",(0,l.kt)("inlineCode",{parentName:"p"},"petals")," using ",(0,l.kt)("inlineCode",{parentName:"p"},"pip3 install petals"),"."),(0,l.kt)("p",null,"For Apple Silicon(M1/M2) users please follow this guide ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/bigscience-workshop/petals/issues/147#issuecomment-1365379642"},"https://github.com/bigscience-workshop/petals/issues/147#issuecomment-1365379642")," to install petals "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"pip3 install petals\n")),(0,l.kt)("h2",{id:"imports"},"Imports"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "Petals", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.petals.Petals.html", "title": "Petals"}]--\x3e\nimport os\nfrom langchain.llms import Petals\nfrom langchain import PromptTemplate, LLMChain\n')),(0,l.kt)("h2",{id:"set-the-environment-api-key"},"Set the Environment API Key"),(0,l.kt)("p",null,"Make sure to get ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/docs/api-inference/quicktour#get-your-api-token"},"your API key")," from Huggingface."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from getpass import getpass\n\nHUGGINGFACE_API_KEY = getpass()\n")),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"     \xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'os.environ["HUGGINGFACE_API_KEY"] = HUGGINGFACE_API_KEY\n')),(0,l.kt)("h2",{id:"create-the-petals-instance"},"Create the Petals instance"),(0,l.kt)("p",null,"You can specify different parameters such as the model name, max new tokens, temperature, etc."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# this can take several minutes to download big files!\n\nllm = Petals(model_name="bigscience/bloom-petals")\n')),(0,l.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    Downloading:   1%|\u258f                        | 40.8M/7.19G [00:24<15:44, 7.57MB/s]\n"))),(0,l.kt)("h2",{id:"create-a-prompt-template"},"Create a Prompt Template"),(0,l.kt)("p",null,"We will create a prompt template for Question and Answer."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'template = """Question: {question}\n\nAnswer: Let\'s think step by step."""\n\nprompt = PromptTemplate(template=template, input_variables=["question"])\n')),(0,l.kt)("h2",{id:"initiate-the-llmchain"},"Initiate the LLMChain"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"llm_chain = LLMChain(prompt=prompt, llm=llm)\n")),(0,l.kt)("h2",{id:"run-the-llmchain"},"Run the LLMChain"),(0,l.kt)("p",null,"Provide a question and run the LLMChain."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"\n\nllm_chain.run(question)\n')))}d.isMDXComponent=!0}}]);