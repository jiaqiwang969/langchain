"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[60591],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>d});var r=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=r.createContext({}),p=function(e){var n=r.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},c=function(e){var n=p(e.components);return r.createElement(l.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},h=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(t),h=a,d=u["".concat(l,".").concat(h)]||u[h]||m[h]||i;return t?r.createElement(d,o(o({ref:n},c),{},{components:t})):r.createElement(d,o({ref:n},c))}));function d(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=h;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[u]="string"==typeof e?e:a,o[1]=s;for(var p=2;p<i;p++)o[p]=t[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}h.displayName="MDXCreateElement"},61615:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var r=t(87462),a=(t(67294),t(3905));const i={},o="Custom Pairwise Evaluator",s={unversionedId:"guides/evaluation/comparison/custom",id:"guides/evaluation/comparison/custom",title:"Custom Pairwise Evaluator",description:"You can make your own pairwise string evaluators by inheriting from PairwiseStringEvaluator class and overwriting the evaluatestringpairs method (and the aevaluatestringpairs method if you want to use the evaluator asynchronously).",source:"@site/docs/guides/evaluation/comparison/custom.md",sourceDirName:"guides/evaluation/comparison",slug:"/guides/evaluation/comparison/custom",permalink:"/langchain/docs/guides/evaluation/comparison/custom",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Comparison Evaluators",permalink:"/langchain/docs/guides/evaluation/comparison/"},next:{title:"Pairwise Embedding Distance",permalink:"/langchain/docs/guides/evaluation/comparison/pairwise_embedding_distance"}},l={},p=[{value:"LLM-Based Example",id:"llm-based-example",level:2}],c=(u="CodeOutputBlock",function(e){return console.warn("Component "+u+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var u;const m={toc:p},h="wrapper";function d(e){let{components:n,...t}=e;return(0,a.kt)(h,(0,r.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"custom-pairwise-evaluator"},"Custom Pairwise Evaluator"),(0,a.kt)("p",null,"You can make your own pairwise string evaluators by inheriting from ",(0,a.kt)("inlineCode",{parentName:"p"},"PairwiseStringEvaluator")," class and overwriting the ",(0,a.kt)("inlineCode",{parentName:"p"},"_evaluate_string_pairs")," method (and the ",(0,a.kt)("inlineCode",{parentName:"p"},"_aevaluate_string_pairs")," method if you want to use the evaluator asynchronously)."),(0,a.kt)("p",null,"In this example, you will make a simple custom evaluator that just returns whether the first prediction has more whitespace tokenized 'words' than the second."),(0,a.kt)("p",null,"You can check out the reference docs for the ",(0,a.kt)("a",{parentName:"p",href:"https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.schema.PairwiseStringEvaluator.html#langchain.evaluation.schema.PairwiseStringEvaluator"},"PairwiseStringEvaluator interface")," for more info."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "PairwiseStringEvaluator", "source": "langchain.evaluation", "docs": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.schema.PairwiseStringEvaluator.html", "title": "Custom Pairwise Evaluator"}]--\x3e\nfrom typing import Optional, Any\nfrom langchain.evaluation import PairwiseStringEvaluator\n\n\nclass LengthComparisonPairwiseEvalutor(PairwiseStringEvaluator):\n    """\n    Custom evaluator to compare two strings.\n    """\n\n    def _evaluate_string_pairs(\n        self,\n        *,\n        prediction: str,\n        prediction_b: str,\n        reference: Optional[str] = None,\n        input: Optional[str] = None,\n        **kwargs: Any,\n    ) -> dict:\n        score = int(len(prediction.split()) > len(prediction_b.split()))\n        return {"score": score}\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'evaluator = LengthComparisonPairwiseEvalutor()\n\nevaluator.evaluate_string_pairs(\n    prediction="The quick brown fox jumped over the lazy dog.",\n    prediction_b="The quick brown fox jumped over the dog.",\n)\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    {'score': 1}\n"))),(0,a.kt)("h2",{id:"llm-based-example"},"LLM-Based Example"),(0,a.kt)("p",null,"That example was simple to illustrate the API, but it wasn't very useful in practice. Below, use an LLM with some custom instructions to form a simple preference scorer similar to the built-in ",(0,a.kt)("a",{parentName:"p",href:"https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain.html#langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain"},"PairwiseStringEvalChain"),". We will use ",(0,a.kt)("inlineCode",{parentName:"p"},"ChatAnthropic")," for the evaluator chain."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# %pip install anthropic\n# %env ANTHROPIC_API_KEY=YOUR_API_KEY\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "PairwiseStringEvaluator", "source": "langchain.evaluation", "docs": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.schema.PairwiseStringEvaluator.html", "title": "Custom Pairwise Evaluator"}, {"imported": "ChatAnthropic", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.anthropic.ChatAnthropic.html", "title": "Custom Pairwise Evaluator"}, {"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "Custom Pairwise Evaluator"}]--\x3e\nfrom typing import Optional, Any\nfrom langchain.evaluation import PairwiseStringEvaluator\nfrom langchain.chat_models import ChatAnthropic\nfrom langchain.chains import LLMChain\n\n\nclass CustomPreferenceEvaluator(PairwiseStringEvaluator):\n    """\n    Custom evaluator to compare two strings using a custom LLMChain.\n    """\n\n    def __init__(self) -> None:\n        llm = ChatAnthropic(model="claude-2", temperature=0)\n        self.eval_chain = LLMChain.from_string(\n            llm,\n            """Which option is preferred? Do not take order into account. Evaluate based on accuracy and helpfulness. If neither is preferred, respond with C. Provide your reasoning, then finish with Preference: A/B/C\n\nInput: How do I get the path of the parent directory in python 3.8?\nOption A: You can use the following code:\n```python\nimport os\n\nos.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n')),(0,a.kt)("p",null,"Option B: You can use the following code:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from pathlib import Path\nPath(__file__).absolute().parent\n")),(0,a.kt)("p",null,"Reasoning: Both options return the same result. However, since option B is more concise and easily understand, it is preferred.\nPreference: B"),(0,a.kt)("p",null,'Which option is preferred? Do not take order into account. Evaluate based on accuracy and helpfulness. If neither is preferred, respond with C. Provide your reasoning, then finish with Preference: A/B/C\nInput: {input}\nOption A: {prediction}\nOption B: {prediction_b}\nReasoning:""",\n)'),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'@property\ndef requires_input(self) -> bool:\n    return True\n\n@property\ndef requires_reference(self) -> bool:\n    return False\n\ndef _evaluate_string_pairs(\n    self,\n    *,\n    prediction: str,\n    prediction_b: str,\n    reference: Optional[str] = None,\n    input: Optional[str] = None,\n    **kwargs: Any,\n) -> dict:\n    result = self.eval_chain(\n        {\n            "input": input,\n            "prediction": prediction,\n            "prediction_b": prediction_b,\n            "stop": ["Which option is preferred?"],\n        },\n        **kwargs,\n    )\n\n    response_text = result["text"]\n    reasoning, preference = response_text.split("Preference:", maxsplit=1)\n    preference = preference.strip()\n    score = 1.0 if preference == "A" else (0.0 if preference == "B" else None)\n    return {"reasoning": reasoning.strip(), "value": preference, "score": score}\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"\n\n```python\nevaluator = CustomPreferenceEvaluator()\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'evaluator.evaluate_string_pairs(\n    input="How do I import from a relative directory?",\n    prediction="use importlib! importlib.import_module(\'.my_package\', \'.\')",\n    prediction_b="from .sibling import foo",\n)\n')),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    {'reasoning': 'Option B is preferred over option A for importing from a relative directory, because it is more straightforward and concise.\\n\\nOption A uses the importlib module, which allows importing a module by specifying the full name as a string. While this works, it is less clear compared to option B.\\n\\nOption B directly imports from the relative path using dot notation, which clearly shows that it is a relative import. This is the recommended way to do relative imports in Python.\\n\\nIn summary, option B is more accurate and helpful as it uses the standard Python relative import syntax.',\n     'value': 'B',\n     'score': 0.0}\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# Setting requires_input to return True adds additional validation to avoid returning a grade when insufficient data is provided to the chain.\n\ntry:\n    evaluator.evaluate_string_pairs(\n        prediction=\"use importlib! importlib.import_module('.my_package', '.')\",\n        prediction_b=\"from .sibling import foo\",\n    )\nexcept ValueError as e:\n    print(e)\n")),(0,a.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    CustomPreferenceEvaluator requires an input string.\n"))))}d.isMDXComponent=!0}}]);