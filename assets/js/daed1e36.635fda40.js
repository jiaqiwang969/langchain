"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[15670],{3905:(n,e,t)=>{t.d(e,{Zo:()=>c,kt:()=>d});var a=t(67294);function o(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function i(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(n);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,a)}return t}function r(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?i(Object(t),!0).forEach((function(e){o(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}function s(n,e){if(null==n)return{};var t,a,o=function(n,e){if(null==n)return{};var t,a,o={},i=Object.keys(n);for(a=0;a<i.length;a++)t=i[a],e.indexOf(t)>=0||(o[t]=n[t]);return o}(n,e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(n);for(a=0;a<i.length;a++)t=i[a],e.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(n,t)&&(o[t]=n[t])}return o}var l=a.createContext({}),m=function(n){var e=a.useContext(l),t=e;return n&&(t="function"==typeof n?n(e):r(r({},e),n)),t},c=function(n){var e=m(n.components);return a.createElement(l.Provider,{value:e},n.children)},_="mdxType",p={inlineCode:"code",wrapper:function(n){var e=n.children;return a.createElement(a.Fragment,{},e)}},u=a.forwardRef((function(n,e){var t=n.components,o=n.mdxType,i=n.originalType,l=n.parentName,c=s(n,["components","mdxType","originalType","parentName"]),_=m(t),u=o,d=_["".concat(l,".").concat(u)]||_[u]||p[u]||i;return t?a.createElement(d,r(r({ref:e},c),{},{components:t})):a.createElement(d,r({ref:e},c))}));function d(n,e){var t=arguments,o=e&&e.mdxType;if("string"==typeof n||o){var i=t.length,r=new Array(i);r[0]=u;var s={};for(var l in e)hasOwnProperty.call(e,l)&&(s[l]=e[l]);s.originalType=n,s[_]="string"==typeof n?n:o,r[1]=s;for(var m=2;m<i;m++)r[m]=t[m];return a.createElement.apply(null,r)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},32797:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>m});var a=t(87462),o=(t(67294),t(3905));const i={},r="Multi-Agent Simulated Environment: Petting Zoo",s={unversionedId:"use_cases/more/agents/agent_simulations/petting_zoo",id:"use_cases/more/agents/agent_simulations/petting_zoo",title:"Multi-Agent Simulated Environment: Petting Zoo",description:"In this example, we show how to define multi-agent simulations with simulated environments. Like ours single-agent example with Gymnasium, we create an agent-environment loop with an externally defined environment. The main difference is that we now implement this kind of interaction loop with multiple agents instead. We will use the Petting Zoo library, which is the multi-agent counterpart to Gymnasium.",source:"@site/docs/use_cases/more/agents/agent_simulations/petting_zoo.md",sourceDirName:"use_cases/more/agents/agent_simulations",slug:"/use_cases/more/agents/agent_simulations/petting_zoo",permalink:"/langchain/docs/use_cases/more/agents/agent_simulations/petting_zoo",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"use_cases",previous:{title:"Multi-agent decentralized speaker selection",permalink:"/langchain/docs/use_cases/more/agents/agent_simulations/multiagent_bidding"},next:{title:"Agent Debates with Tools",permalink:"/langchain/docs/use_cases/more/agents/agent_simulations/two_agent_debate_tools"}},l={},m=[{value:"Install <code>pettingzoo</code> and other dependencies",id:"install-pettingzoo-and-other-dependencies",level:2},{value:"Import modules",id:"import-modules",level:2},{value:"<code>GymnasiumAgent</code>",id:"gymnasiumagent",level:2},{value:"Main loop",id:"main-loop",level:2},{value:"<code>PettingZooAgent</code>",id:"pettingzooagent",level:2},{value:"Rock, Paper, Scissors",id:"rock-paper-scissors",level:2},{value:"<code>ActionMaskAgent</code>",id:"actionmaskagent",level:2},{value:"Tic-Tac-Toe",id:"tic-tac-toe",level:2},{value:"Texas Hold&#39;em No Limit",id:"texas-holdem-no-limit",level:2}],c=(_="CodeOutputBlock",function(n){return console.warn("Component "+_+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",n)});var _;const p={toc:m},u="wrapper";function d(n){let{components:e,...t}=n;return(0,o.kt)(u,(0,a.Z)({},p,t,{components:e,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"multi-agent-simulated-environment-petting-zoo"},"Multi-Agent Simulated Environment: Petting Zoo"),(0,o.kt)("p",null,"In this example, we show how to define multi-agent simulations with simulated environments. Like ",(0,o.kt)("a",{parentName:"p",href:"https://python.langchain.com/en/latest/use_cases/agent_simulations/gymnasium.html"},"ours single-agent example with Gymnasium"),", we create an agent-environment loop with an externally defined environment. The main difference is that we now implement this kind of interaction loop with multiple agents instead. We will use the ",(0,o.kt)("a",{parentName:"p",href:"https://pettingzoo.farama.org/"},"Petting Zoo")," library, which is the multi-agent counterpart to ",(0,o.kt)("a",{parentName:"p",href:"https://gymnasium.farama.org/"},"Gymnasium"),"."),(0,o.kt)("h2",{id:"install-pettingzoo-and-other-dependencies"},"Install ",(0,o.kt)("inlineCode",{parentName:"h2"},"pettingzoo")," and other dependencies"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install pettingzoo pygame rlcard\n")),(0,o.kt)("h2",{id:"import-modules"},"Import modules"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html", "title": "Multi-Agent Simulated Environment: Petting Zoo"}, {"imported": "HumanMessage", "source": "langchain.schema", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.HumanMessage.html", "title": "Multi-Agent Simulated Environment: Petting Zoo"}, {"imported": "SystemMessage", "source": "langchain.schema", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.SystemMessage.html", "title": "Multi-Agent Simulated Environment: Petting Zoo"}, {"imported": "RegexParser", "source": "langchain.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.regex.RegexParser.html", "title": "Multi-Agent Simulated Environment: Petting Zoo"}]--\x3e\nimport collections\nimport inspect\nimport tenacity\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import (\n    HumanMessage,\n    SystemMessage,\n)\nfrom langchain.output_parsers import RegexParser\n')),(0,o.kt)("h2",{id:"gymnasiumagent"},(0,o.kt)("inlineCode",{parentName:"h2"},"GymnasiumAgent")),(0,o.kt)("p",null,"Here we reproduce the same ",(0,o.kt)("inlineCode",{parentName:"p"},"GymnasiumAgent")," defined from ",(0,o.kt)("a",{parentName:"p",href:"https://python.langchain.com/en/latest/use_cases/agent_simulations/gymnasium.html"},"our Gymnasium example"),". If after multiple retries it does not take a valid action, it simply takes a random action. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'class GymnasiumAgent:\n    @classmethod\n    def get_docs(cls, env):\n        return env.unwrapped.__doc__\n\n    def __init__(self, model, env):\n        self.model = model\n        self.env = env\n        self.docs = self.get_docs(env)\n\n        self.instructions = """\nYour goal is to maximize your return, i.e. the sum of the rewards you receive.\nI will give you an observation, reward, terminiation flag, truncation flag, and the return so far, formatted as:\n\nObservation: <observation>\nReward: <reward>\nTermination: <termination>\nTruncation: <truncation>\nReturn: <sum_of_rewards>\n\nYou will respond with an action, formatted as:\n\nAction: <action>\n\nwhere you replace <action> with your actual action.\nDo nothing else but return the action.\n"""\n        self.action_parser = RegexParser(\n            regex=r"Action: (.*)", output_keys=["action"], default_output_key="action"\n        )\n\n        self.message_history = []\n        self.ret = 0\n\n    def random_action(self):\n        action = self.env.action_space.sample()\n        return action\n\n    def reset(self):\n        self.message_history = [\n            SystemMessage(content=self.docs),\n            SystemMessage(content=self.instructions),\n        ]\n\n    def observe(self, obs, rew=0, term=False, trunc=False, info=None):\n        self.ret += rew\n\n        obs_message = f"""\nObservation: {obs}\nReward: {rew}\nTermination: {term}\nTruncation: {trunc}\nReturn: {self.ret}\n        """\n        self.message_history.append(HumanMessage(content=obs_message))\n        return obs_message\n\n    def _act(self):\n        act_message = self.model(self.message_history)\n        self.message_history.append(act_message)\n        action = int(self.action_parser.parse(act_message.content)["action"])\n        return action\n\n    def act(self):\n        try:\n            for attempt in tenacity.Retrying(\n                stop=tenacity.stop_after_attempt(2),\n                wait=tenacity.wait_none(),  # No waiting time between retries\n                retry=tenacity.retry_if_exception_type(ValueError),\n                before_sleep=lambda retry_state: print(\n                    f"ValueError occurred: {retry_state.outcome.exception()}, retrying..."\n                ),\n            ):\n                with attempt:\n                    action = self._act()\n        except tenacity.RetryError as e:\n            action = self.random_action()\n        return action\n')),(0,o.kt)("h2",{id:"main-loop"},"Main loop"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'def main(agents, env):\n    env.reset()\n\n    for name, agent in agents.items():\n        agent.reset()\n\n    for agent_name in env.agent_iter():\n        observation, reward, termination, truncation, info = env.last()\n        obs_message = agents[agent_name].observe(\n            observation, reward, termination, truncation, info\n        )\n        print(obs_message)\n        if termination or truncation:\n            action = None\n        else:\n            action = agents[agent_name].act()\n        print(f"Action: {action}")\n        env.step(action)\n    env.close()\n')),(0,o.kt)("h2",{id:"pettingzooagent"},(0,o.kt)("inlineCode",{parentName:"h2"},"PettingZooAgent")),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"PettingZooAgent")," extends the ",(0,o.kt)("inlineCode",{parentName:"p"},"GymnasiumAgent")," to the multi-agent setting. The main differences are:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"PettingZooAgent")," takes in a ",(0,o.kt)("inlineCode",{parentName:"li"},"name")," argument to identify it among multiple agents"),(0,o.kt)("li",{parentName:"ul"},"the function ",(0,o.kt)("inlineCode",{parentName:"li"},"get_docs")," is implemented differently because the ",(0,o.kt)("inlineCode",{parentName:"li"},"PettingZoo")," repo structure is structured differently from the ",(0,o.kt)("inlineCode",{parentName:"li"},"Gymnasium")," repo")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"class PettingZooAgent(GymnasiumAgent):\n    @classmethod\n    def get_docs(cls, env):\n        return inspect.getmodule(env.unwrapped).__doc__\n\n    def __init__(self, name, model, env):\n        super().__init__(model, env)\n        self.name = name\n\n    def random_action(self):\n        action = self.env.action_space(self.name).sample()\n        return action\n")),(0,o.kt)("h2",{id:"rock-paper-scissors"},"Rock, Paper, Scissors"),(0,o.kt)("p",null,"We can now run a simulation of a multi-agent rock, paper, scissors game using the ",(0,o.kt)("inlineCode",{parentName:"p"},"PettingZooAgent"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pettingzoo.classic import rps_v2\n\nenv = rps_v2.env(max_cycles=3, render_mode="human")\nagents = {\n    name: PettingZooAgent(name=name, model=ChatOpenAI(temperature=1), env=env)\n    for name in env.possible_agents\n}\nmain(agents, env)\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    \n    Observation: 3\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 1\n    \n    Observation: 3\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 1\n    \n    Observation: 1\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 2\n    \n    Observation: 1\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 1\n    \n    Observation: 1\n    Reward: 1\n    Termination: False\n    Truncation: False\n    Return: 1\n            \n    Action: 0\n    \n    Observation: 2\n    Reward: -1\n    Termination: False\n    Truncation: False\n    Return: -1\n            \n    Action: 0\n    \n    Observation: 0\n    Reward: 0\n    Termination: False\n    Truncation: True\n    Return: 1\n            \n    Action: None\n    \n    Observation: 0\n    Reward: 0\n    Termination: False\n    Truncation: True\n    Return: -1\n            \n    Action: None\n"))),(0,o.kt)("h2",{id:"actionmaskagent"},(0,o.kt)("inlineCode",{parentName:"h2"},"ActionMaskAgent")),(0,o.kt)("p",null,"Some ",(0,o.kt)("inlineCode",{parentName:"p"},"PettingZoo")," environments provide an ",(0,o.kt)("inlineCode",{parentName:"p"},"action_mask")," to tell the agent which actions are valid. The ",(0,o.kt)("inlineCode",{parentName:"p"},"ActionMaskAgent")," subclasses ",(0,o.kt)("inlineCode",{parentName:"p"},"PettingZooAgent")," to use information from the ",(0,o.kt)("inlineCode",{parentName:"p"},"action_mask")," to select actions."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'class ActionMaskAgent(PettingZooAgent):\n    def __init__(self, name, model, env):\n        super().__init__(name, model, env)\n        self.obs_buffer = collections.deque(maxlen=1)\n\n    def random_action(self):\n        obs = self.obs_buffer[-1]\n        action = self.env.action_space(self.name).sample(obs["action_mask"])\n        return action\n\n    def reset(self):\n        self.message_history = [\n            SystemMessage(content=self.docs),\n            SystemMessage(content=self.instructions),\n        ]\n\n    def observe(self, obs, rew=0, term=False, trunc=False, info=None):\n        self.obs_buffer.append(obs)\n        return super().observe(obs, rew, term, trunc, info)\n\n    def _act(self):\n        valid_action_instruction = "Generate a valid action given by the indices of the `action_mask` that are not 0, according to the action formatting rules."\n        self.message_history.append(HumanMessage(content=valid_action_instruction))\n        return super()._act()\n')),(0,o.kt)("h2",{id:"tic-tac-toe"},"Tic-Tac-Toe"),(0,o.kt)("p",null,"Here is an example of a Tic-Tac-Toe game that uses the ",(0,o.kt)("inlineCode",{parentName:"p"},"ActionMaskAgent"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pettingzoo.classic import tictactoe_v3\n\nenv = tictactoe_v3.env(render_mode="human")\nagents = {\n    name: ActionMaskAgent(name=name, model=ChatOpenAI(temperature=0.2), env=env)\n    for name in env.possible_agents\n}\nmain(agents, env)\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    \n    Observation: {'observation': array([[[0, 0],\n            [0, 0],\n            [0, 0]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]]], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 0\n         |     |     \n      X  |  -  |  -  \n    _____|_____|_____\n         |     |     \n      -  |  -  |  -  \n    _____|_____|_____\n         |     |     \n      -  |  -  |  -  \n         |     |     \n    \n    Observation: {'observation': array([[[0, 1],\n            [0, 0],\n            [0, 0]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]]], dtype=int8), 'action_mask': array([0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 1\n         |     |     \n      X  |  -  |  -  \n    _____|_____|_____\n         |     |     \n      O  |  -  |  -  \n    _____|_____|_____\n         |     |     \n      -  |  -  |  -  \n         |     |     \n    \n    Observation: {'observation': array([[[1, 0],\n            [0, 1],\n            [0, 0]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 1, 1, 1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 2\n         |     |     \n      X  |  -  |  -  \n    _____|_____|_____\n         |     |     \n      O  |  -  |  -  \n    _____|_____|_____\n         |     |     \n      X  |  -  |  -  \n         |     |     \n    \n    Observation: {'observation': array([[[0, 1],\n            [1, 0],\n            [0, 1]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 1, 1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 3\n         |     |     \n      X  |  O  |  -  \n    _____|_____|_____\n         |     |     \n      O  |  -  |  -  \n    _____|_____|_____\n         |     |     \n      X  |  -  |  -  \n         |     |     \n    \n    Observation: {'observation': array([[[1, 0],\n            [0, 1],\n            [1, 0]],\n    \n           [[0, 1],\n            [0, 0],\n            [0, 0]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 4\n         |     |     \n      X  |  O  |  -  \n    _____|_____|_____\n         |     |     \n      O  |  X  |  -  \n    _____|_____|_____\n         |     |     \n      X  |  -  |  -  \n         |     |     \n    \n    Observation: {'observation': array([[[0, 1],\n            [1, 0],\n            [0, 1]],\n    \n           [[1, 0],\n            [0, 1],\n            [0, 0]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 5\n         |     |     \n      X  |  O  |  -  \n    _____|_____|_____\n         |     |     \n      O  |  X  |  -  \n    _____|_____|_____\n         |     |     \n      X  |  O  |  -  \n         |     |     \n    \n    Observation: {'observation': array([[[1, 0],\n            [0, 1],\n            [1, 0]],\n    \n           [[0, 1],\n            [1, 0],\n            [0, 1]],\n    \n           [[0, 0],\n            [0, 0],\n            [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 6\n         |     |     \n      X  |  O  |  X  \n    _____|_____|_____\n         |     |     \n      O  |  X  |  -  \n    _____|_____|_____\n         |     |     \n      X  |  O  |  -  \n         |     |     \n    \n    Observation: {'observation': array([[[0, 1],\n            [1, 0],\n            [0, 1]],\n    \n           [[1, 0],\n            [0, 1],\n            [1, 0]],\n    \n           [[0, 1],\n            [0, 0],\n            [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)}\n    Reward: -1\n    Termination: True\n    Truncation: False\n    Return: -1\n            \n    Action: None\n    \n    Observation: {'observation': array([[[1, 0],\n            [0, 1],\n            [1, 0]],\n    \n           [[0, 1],\n            [1, 0],\n            [0, 1]],\n    \n           [[1, 0],\n            [0, 0],\n            [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)}\n    Reward: 1\n    Termination: True\n    Truncation: False\n    Return: 1\n            \n    Action: None\n"))),(0,o.kt)("h2",{id:"texas-holdem-no-limit"},"Texas Hold'em No Limit"),(0,o.kt)("p",null,"Here is an example of a Texas Hold'em No Limit game that uses the ",(0,o.kt)("inlineCode",{parentName:"p"},"ActionMaskAgent"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pettingzoo.classic import texas_holdem_no_limit_v6\n\nenv = texas_holdem_no_limit_v6.env(num_players=4, render_mode="human")\nagents = {\n    name: ActionMaskAgent(name=name, model=ChatOpenAI(temperature=0.2), env=env)\n    for name in env.possible_agents\n}\nmain(agents, env)\n')),(0,o.kt)(c,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    \n    Observation: {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n           0., 0., 2.], dtype=float32), 'action_mask': array([1, 1, 0, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 1\n    \n    Observation: {'observation': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n           0., 0., 2.], dtype=float32), 'action_mask': array([1, 1, 0, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 1\n    \n    Observation: {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 1., 2.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 1\n    \n    Observation: {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 2., 2.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 0\n    \n    Observation: {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n           0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 2., 2.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 2\n    \n    Observation: {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n           0., 2., 6.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 2\n    \n    Observation: {'observation': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n           0., 2., 8.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 3\n    \n    Observation: {'observation': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n            0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n            0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n            1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n            6., 20.], dtype=float32), 'action_mask': array([1, 1, 1, 1, 1], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 4\n    \n    Observation: {'observation': array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   1.,   1.,\n             0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   8., 100.],\n          dtype=float32), 'action_mask': array([1, 1, 0, 0, 0], dtype=int8)}\n    Reward: 0\n    Termination: False\n    Truncation: False\n    Return: 0\n            \n    Action: 4\n    [WARNING]: Illegal move made, game terminating with current player losing. \n    obs['action_mask'] contains a mask of all legal moves that can be chosen.\n    \n    Observation: {'observation': array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   1.,   1.,\n             0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   8., 100.],\n          dtype=float32), 'action_mask': array([1, 1, 0, 0, 0], dtype=int8)}\n    Reward: -1.0\n    Termination: True\n    Truncation: True\n    Return: -1.0\n            \n    Action: None\n    \n    Observation: {'observation': array([  0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   1.,   0.,\n             0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,  20., 100.],\n          dtype=float32), 'action_mask': array([1, 1, 0, 0, 0], dtype=int8)}\n    Reward: 0\n    Termination: True\n    Truncation: True\n    Return: 0\n            \n    Action: None\n    \n    Observation: {'observation': array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n             1.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   1.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 100., 100.],\n          dtype=float32), 'action_mask': array([1, 1, 0, 0, 0], dtype=int8)}\n    Reward: 0\n    Termination: True\n    Truncation: True\n    Return: 0\n            \n    Action: None\n    \n    Observation: {'observation': array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   1.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   1.,   0.,\n             0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   2., 100.],\n          dtype=float32), 'action_mask': array([1, 1, 0, 0, 0], dtype=int8)}\n    Reward: 0\n    Termination: True\n    Truncation: True\n    Return: 0\n            \n    Action: None\n"))))}d.isMDXComponent=!0}}]);