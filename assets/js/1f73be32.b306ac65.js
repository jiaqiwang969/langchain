"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[16315],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,p=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),d=s(n),m=r,h=d["".concat(p,".").concat(m)]||d[m]||c[m]||o;return n?a.createElement(h,l(l({ref:t},u),{},{components:n})):a.createElement(h,l({ref:t},u))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,l=new Array(o);l[0]=m;var i={};for(var p in t)hasOwnProperty.call(t,p)&&(i[p]=t[p]);i.originalType=e,i[d]="string"==typeof e?e:r,l[1]=i;for(var s=2;s<o;s++)l[s]=n[s];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},19520:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>s});var a=n(87462),r=(n(67294),n(3905));const o={},l="Banana",i={unversionedId:"integrations/providers/bananadev",id:"integrations/providers/bananadev",title:"Banana",description:"Banana provided serverless GPU inference for AI models, including a CI/CD build pipeline and a simple Python framework (Potassium) to server your models.",source:"@site/docs/integrations/providers/bananadev.mdx",sourceDirName:"integrations/providers",slug:"/integrations/providers/bananadev",permalink:"/langchain/docs/integrations/providers/bananadev",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"BagelDB",permalink:"/langchain/docs/integrations/providers/bageldb"},next:{title:"Baseten",permalink:"/langchain/docs/integrations/providers/baseten"}},p={},s=[{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"Define your Banana Template",id:"define-your-banana-template",level:2},{value:"Build the Banana app",id:"build-the-banana-app",level:2},{value:"Wrappers",id:"wrappers",level:2},{value:"LLM",id:"llm",level:3}],u={toc:s},d="wrapper";function c(e){let{components:t,...n}=e;return(0,r.kt)(d,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"banana"},"Banana"),(0,r.kt)("p",null,"Banana provided serverless GPU inference for AI models, including a CI/CD build pipeline and a simple Python framework (Potassium) to server your models."),(0,r.kt)("p",null,"This page covers how to use the ",(0,r.kt)("a",{parentName:"p",href:"https://www.banana.dev"},"Banana")," ecosystem within LangChain."),(0,r.kt)("p",null,"It is broken into two parts: "),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"installation and setup, "),(0,r.kt)("li",{parentName:"ul"},"and then references to specific Banana wrappers.")),(0,r.kt)("h2",{id:"installation-and-setup"},"Installation and Setup"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Install with ",(0,r.kt)("inlineCode",{parentName:"li"},"pip install banana-dev")),(0,r.kt)("li",{parentName:"ul"},"Get an Banana api key from the ",(0,r.kt)("a",{parentName:"li",href:"https://app.banana.dev"},"Banana.dev dashboard")," and set it as an environment variable (",(0,r.kt)("inlineCode",{parentName:"li"},"BANANA_API_KEY"),")"),(0,r.kt)("li",{parentName:"ul"},"Get your model's key and url slug from the model's details page")),(0,r.kt)("h2",{id:"define-your-banana-template"},"Define your Banana Template"),(0,r.kt)("p",null,"You'll need to set up a Github repo for your Banana app. You can get started in 5 minutes using ",(0,r.kt)("a",{parentName:"p",href:"https://docs.banana.dev/banana-docs/"},"this guide"),"."),(0,r.kt)("p",null,"Alternatively, for a ready-to-go LLM example, you can check out Banana's ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/bananaml/demo-codellama-7b-instruct-gptq"},"CodeLlama-7B-Instruct-GPTQ")," GitHub repository. Just fork it and deploy it within Banana."),(0,r.kt)("p",null,"Other starter repos are available ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/orgs/bananaml/repositories?q=demo-&type=all&language=&sort="},"here"),"."),(0,r.kt)("h2",{id:"build-the-banana-app"},"Build the Banana app"),(0,r.kt)("p",null,"To use Banana apps within Langchain, they must include the ",(0,r.kt)("inlineCode",{parentName:"p"},"outputs")," key\nin the returned json, and the value must be a string."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Return the results as a dictionary\nresult = {'outputs': result}\n")),(0,r.kt)("p",null,"An example inference function would be:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'@app.handler("/")\ndef handler(context: dict, request: Request) -> Response:\n    """Handle a request to generate code from a prompt."""\n    model = context.get("model")\n    tokenizer = context.get("tokenizer")\n    max_new_tokens = request.json.get("max_new_tokens", 512)\n    temperature = request.json.get("temperature", 0.7)\n    prompt = request.json.get("prompt")\n    prompt_template=f\'\'\'[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n    {prompt}\n    [/INST]\n    \'\'\'\n    input_ids = tokenizer(prompt_template, return_tensors=\'pt\').input_ids.cuda()\n    output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens)\n    result = tokenizer.decode(output[0])\n    return Response(json={"outputs": result}, status=200)\n')),(0,r.kt)("p",null,"This example is from the ",(0,r.kt)("inlineCode",{parentName:"p"},"app.py")," file in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/bananaml/demo-codellama-7b-instruct-gptq"},"CodeLlama-7B-Instruct-GPTQ"),"."),(0,r.kt)("h2",{id:"wrappers"},"Wrappers"),(0,r.kt)("h3",{id:"llm"},"LLM"),(0,r.kt)("p",null,"Within Langchain, there exists a Banana LLM wrapper, which you can access with"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.llms import Banana\n")),(0,r.kt)("p",null,"You need to provide a model key and model url slug, which you can get from the model's details page in the ",(0,r.kt)("a",{parentName:"p",href:"https://app.banana.dev"},"Banana.dev dashboard"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'llm = Banana(model_key="YOUR_MODEL_KEY", model_url_slug="YOUR_MODEL_URL_SLUG")\n')))}c.isMDXComponent=!0}}]);