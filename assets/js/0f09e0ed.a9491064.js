"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[65832],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>g});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var i=a.createContext({}),p=function(e){var t=a.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(i.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,i=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(n),d=o,g=m["".concat(i,".").concat(d)]||m[d]||u[d]||r;return n?a.createElement(g,s(s({ref:t},c),{},{components:n})):a.createElement(g,s({ref:t},c))}));function g(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,s=new Array(r);s[0]=d;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l[m]="string"==typeof e?e:o,s[1]=l;for(var p=2;p<r;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},96899:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>m,default:()=>f,frontMatter:()=>c,metadata:()=>u,toc:()=>g});var a=n(87462),o=(n(67294),n(3905));const r=(s="CodeOutputBlock",function(e){return console.warn("Component "+s+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",e)});var s;const l={toc:[{value:"Setup",id:"setup",level:3},{value:"Messages",id:"messages",level:3},{value:"<code>__call__</code>",id:"__call__",level:3},{value:"Messages in -&gt; message out",id:"messages-in---message-out",level:4},{value:"<code>generate</code>",id:"generate",level:3},{value:"Batch calls, richer outputs",id:"batch-calls-richer-outputs",level:4}]},i="wrapper";function p(e){let{components:t,...n}=e;return(0,o.kt)(i,(0,a.Z)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h3",{id:"setup"},"Setup"),(0,o.kt)("p",null,"To start we'll need to install the OpenAI Python package:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install openai\n")),(0,o.kt)("p",null,"Accessing the API requires an API key, which you can get by creating an account and heading ",(0,o.kt)("a",{parentName:"p",href:"https://platform.openai.com/account/api-keys"},"here"),". Once we have a key we'll want to set it as an environment variable by running:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'export OPENAI_API_KEY="..."\n')),(0,o.kt)("p",null,"If you'd prefer not to set an environment variable you can pass the key in directly via the ",(0,o.kt)("inlineCode",{parentName:"p"},"openai_api_key")," named parameter when initiating the OpenAI LLM class:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.chat_models import ChatOpenAI\n\nchat = ChatOpenAI(openai_api_key="...")\n')),(0,o.kt)("p",null,"Otherwise you can initialize without any params:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.chat_models import ChatOpenAI\n\nchat = ChatOpenAI()\n")),(0,o.kt)("h3",{id:"messages"},"Messages"),(0,o.kt)("p",null,"The chat model interface is based around messages rather than raw text.\nThe types of messages currently supported in LangChain are ",(0,o.kt)("inlineCode",{parentName:"p"},"AIMessage"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"HumanMessage"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"SystemMessage"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatMessage")," -- ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatMessage")," takes in an arbitrary role parameter. Most of the time, you'll just be dealing with ",(0,o.kt)("inlineCode",{parentName:"p"},"HumanMessage"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"AIMessage"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"SystemMessage")),(0,o.kt)("h3",{id:"__call__"},(0,o.kt)("inlineCode",{parentName:"h3"},"__call__")),(0,o.kt)("h4",{id:"messages-in---message-out"},"Messages in -> message out"),(0,o.kt)("p",null,"You can get chat completions by passing one or more messages to the chat model. The response will be a message."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nchat([HumanMessage(content="Translate this sentence from English to French: I love programming.")])\n')),(0,o.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'    AIMessage(content="J\'aime programmer.", additional_kwargs={})\n'))),(0,o.kt)("p",null,"OpenAI's chat model supports multiple messages as input. See ",(0,o.kt)("a",{parentName:"p",href:"https://platform.openai.com/docs/guides/chat/chat-vs-completions"},"here")," for more information. Here is an example of sending a system and user message to the chat model:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'messages = [\n    SystemMessage(content="You are a helpful assistant that translates English to French."),\n    HumanMessage(content="I love programming.")\n]\nchat(messages)\n')),(0,o.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'    AIMessage(content="J\'aime programmer.", additional_kwargs={})\n'))),(0,o.kt)("h3",{id:"generate"},(0,o.kt)("inlineCode",{parentName:"h3"},"generate")),(0,o.kt)("h4",{id:"batch-calls-richer-outputs"},"Batch calls, richer outputs"),(0,o.kt)("p",null,"You can go one step further and generate completions for multiple sets of messages using ",(0,o.kt)("inlineCode",{parentName:"p"},"generate"),". This returns an ",(0,o.kt)("inlineCode",{parentName:"p"},"LLMResult")," with an additional ",(0,o.kt)("inlineCode",{parentName:"p"},"message")," parameter."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'batch_messages = [\n    [\n        SystemMessage(content="You are a helpful assistant that translates English to French."),\n        HumanMessage(content="I love programming.")\n    ],\n    [\n        SystemMessage(content="You are a helpful assistant that translates English to French."),\n        HumanMessage(content="I love artificial intelligence.")\n    ],\n]\nresult = chat.generate(batch_messages)\nresult\n')),(0,o.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}})\n"))),(0,o.kt)("p",null,"You can recover things like token usage from this LLMResult:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"result.llm_output\n")),(0,o.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    {'token_usage': {'prompt_tokens': 57,\n      'completion_tokens': 20,\n      'total_tokens': 77}}\n"))))}p.isMDXComponent=!0;const c={sidebar_position:1},m="Chat models",u={unversionedId:"modules/model_io/models/chat/index",id:"modules/model_io/models/chat/index",title:"Chat models",description:"Head to Integrations for documentation on built-in integrations with chat model providers.",source:"@site/docs/modules/model_io/models/chat/index.mdx",sourceDirName:"modules/model_io/models/chat",slug:"/modules/model_io/models/chat/",permalink:"/langchain/docs/modules/model_io/models/chat/",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"docs",previous:{title:"Tracking token usage",permalink:"/langchain/docs/modules/model_io/models/llms/token_usage_tracking"},next:{title:"Caching",permalink:"/langchain/docs/modules/model_io/models/chat/chat_model_caching"}},d={},g=[{value:"Get started",id:"get-started",level:2}],h={toc:g},k="wrapper";function f(e){let{components:t,...n}=e;return(0,o.kt)(k,(0,a.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"chat-models"},"Chat models"),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"Head to ",(0,o.kt)("a",{parentName:"p",href:"/docs/integrations/chat/"},"Integrations")," for documentation on built-in integrations with chat model providers.")),(0,o.kt)("p",null,'Chat models are a variation on language models.\nWhile chat models use language models under the hood, the interface they use is a bit different.\nRather than using a "text in, text out" API, they use an interface where "chat messages" are the inputs and outputs.'),(0,o.kt)("p",null,"Chat model APIs are fairly new, so we are still figuring out the correct abstractions."),(0,o.kt)("h2",{id:"get-started"},"Get started"),(0,o.kt)(p,{mdxType:"GetStarted"}))}f.isMDXComponent=!0}}]);