"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[87494],{3905:(n,e,a)=>{a.d(e,{Zo:()=>p,kt:()=>g});var t=a(67294);function o(n,e,a){return e in n?Object.defineProperty(n,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):n[e]=a,n}function r(n,e){var a=Object.keys(n);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(n);e&&(t=t.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),a.push.apply(a,t)}return a}function l(n){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?r(Object(a),!0).forEach((function(e){o(n,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(a,e))}))}return n}function c(n,e){if(null==n)return{};var a,t,o=function(n,e){if(null==n)return{};var a,t,o={},r=Object.keys(n);for(t=0;t<r.length;t++)a=r[t],e.indexOf(a)>=0||(o[a]=n[a]);return o}(n,e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(n);for(t=0;t<r.length;t++)a=r[t],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(n,a)&&(o[a]=n[a])}return o}var s=t.createContext({}),i=function(n){var e=t.useContext(s),a=e;return n&&(a="function"==typeof n?n(e):l(l({},e),n)),a},p=function(n){var e=i(n.components);return t.createElement(s.Provider,{value:e},n.children)},m="mdxType",u={inlineCode:"code",wrapper:function(n){var e=n.children;return t.createElement(t.Fragment,{},e)}},h=t.forwardRef((function(n,e){var a=n.components,o=n.mdxType,r=n.originalType,s=n.parentName,p=c(n,["components","mdxType","originalType","parentName"]),m=i(a),h=o,g=m["".concat(s,".").concat(h)]||m[h]||u[h]||r;return a?t.createElement(g,l(l({ref:e},p),{},{components:a})):t.createElement(g,l({ref:e},p))}));function g(n,e){var a=arguments,o=e&&e.mdxType;if("string"==typeof n||o){var r=a.length,l=new Array(r);l[0]=h;var c={};for(var s in e)hasOwnProperty.call(e,s)&&(c[s]=e[s]);c.originalType=n,c[m]="string"==typeof n?n:o,l[1]=c;for(var i=2;i<r;i++)l[i]=a[i];return t.createElement.apply(null,l)}return t.createElement.apply(null,a)}h.displayName="MDXCreateElement"},72860:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>s,contentTitle:()=>l,default:()=>g,frontMatter:()=>r,metadata:()=>c,toc:()=>i});var t=a(87462),o=(a(67294),a(3905));const r={},l="Custom chain",c={unversionedId:"modules/chains/how_to/custom_chain",id:"modules/chains/how_to/custom_chain",title:"Custom chain",description:"To implement your own custom chain you can subclass Chain and implement the following methods:",source:"@site/docs/modules/chains/how_to/custom_chain.md",sourceDirName:"modules/chains/how_to",slug:"/modules/chains/how_to/custom_chain",permalink:"/langchain/docs/modules/chains/how_to/custom_chain",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Different call methods",permalink:"/langchain/docs/modules/chains/how_to/call_methods"},next:{title:"Debugging chains",permalink:"/langchain/docs/modules/chains/how_to/debugging"}},s={},i=[],p=(m="CodeOutputBlock",function(n){return console.warn("Component "+m+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",n)});var m;const u={toc:i},h="wrapper";function g(n){let{components:e,...a}=n;return(0,o.kt)(h,(0,t.Z)({},u,a,{components:e,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"custom-chain"},"Custom chain"),(0,o.kt)("p",null,"To implement your own custom chain you can subclass ",(0,o.kt)("inlineCode",{parentName:"p"},"Chain")," and implement the following methods:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "BaseLanguageModel", "source": "langchain.schema.language_model", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.language_model.BaseLanguageModel.html", "title": "Custom chain"}, {"imported": "AsyncCallbackManagerForChainRun", "source": "langchain.callbacks.manager", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.AsyncCallbackManagerForChainRun.html", "title": "Custom chain"}, {"imported": "CallbackManagerForChainRun", "source": "langchain.callbacks.manager", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.CallbackManagerForChainRun.html", "title": "Custom chain"}, {"imported": "Chain", "source": "langchain.chains.base", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.base.Chain.html", "title": "Custom chain"}, {"imported": "BasePromptTemplate", "source": "langchain.prompts.base", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.prompt_template.BasePromptTemplate.html", "title": "Custom chain"}]--\x3e\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, List, Optional\n\nfrom pydantic import Extra\n\nfrom langchain.schema.language_model import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.prompts.base import BasePromptTemplate\n\n\nclass MyCustomChain(Chain):\n    """\n    An example of a custom chain.\n    """\n\n    prompt: BasePromptTemplate\n    """Prompt object to use."""\n    llm: BaseLanguageModel\n    output_key: str = "text"  #: :meta private:\n\n    class Config:\n        """Configuration for this pydantic object."""\n\n        extra = Extra.forbid\n        arbitrary_types_allowed = True\n\n    @property\n    def input_keys(self) -> List[str]:\n        """Will be whatever keys the prompt expects.\n\n        :meta private:\n        """\n        return self.prompt.input_variables\n\n    @property\n    def output_keys(self) -> List[str]:\n        """Will always return text key.\n\n        :meta private:\n        """\n        return [self.output_key]\n\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        # Your custom chain logic goes here\n        # This is just an example that mimics LLMChain\n        prompt_value = self.prompt.format_prompt(**inputs)\n\n        # Whenever you call a language model, or another chain, you should pass\n        # a callback manager to it. This allows the inner run to be tracked by\n        # any callbacks that are registered on the outer run.\n        # You can always obtain a callback manager for this by calling\n        # `run_manager.get_child()` as shown below.\n        response = self.llm.generate_prompt(\n            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n        )\n\n        # If you want to log something about this run, you can do so by calling\n        # methods on the `run_manager`, as shown below. This will trigger any\n        # callbacks that are registered for that event.\n        if run_manager:\n            run_manager.on_text("Log something about this run")\n\n        return {self.output_key: response.generations[0][0].text}\n\n    async def _acall(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        # Your custom chain logic goes here\n        # This is just an example that mimics LLMChain\n        prompt_value = self.prompt.format_prompt(**inputs)\n\n        # Whenever you call a language model, or another chain, you should pass\n        # a callback manager to it. This allows the inner run to be tracked by\n        # any callbacks that are registered on the outer run.\n        # You can always obtain a callback manager for this by calling\n        # `run_manager.get_child()` as shown below.\n        response = await self.llm.agenerate_prompt(\n            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n        )\n\n        # If you want to log something about this run, you can do so by calling\n        # methods on the `run_manager`, as shown below. This will trigger any\n        # callbacks that are registered for that event.\n        if run_manager:\n            await run_manager.on_text("Log something about this run")\n\n        return {self.output_key: response.generations[0][0].text}\n\n    @property\n    def _chain_type(self) -> str:\n        return "my_custom_chain"\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "StdOutCallbackHandler", "source": "langchain.callbacks.stdout", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.stdout.StdOutCallbackHandler.html", "title": "Custom chain"}, {"imported": "ChatOpenAI", "source": "langchain.chat_models.openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html", "title": "Custom chain"}, {"imported": "PromptTemplate", "source": "langchain.prompts.prompt", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html", "title": "Custom chain"}]--\x3e\nfrom langchain.callbacks.stdout import StdOutCallbackHandler\nfrom langchain.chat_models.openai import ChatOpenAI\nfrom langchain.prompts.prompt import PromptTemplate\n\n\nchain = MyCustomChain(\n    prompt=PromptTemplate.from_template("tell us a joke about {topic}"),\n    llm=ChatOpenAI(),\n)\n\nchain.run({"topic": "callbacks"}, callbacks=[StdOutCallbackHandler()])\n')),(0,o.kt)(p,{lang:"python",mdxType:"CodeOutputBlock"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    \n    \n    > Entering new MyCustomChain chain...\n    Log something about this run\n    > Finished chain.\n\n\n\n\n\n    'Why did the callback function feel lonely? Because it was always waiting for someone to call it back!'\n"))))}g.isMDXComponent=!0}}]);