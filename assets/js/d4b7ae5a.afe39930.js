"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[31160],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>f});var r=a(67294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=r.createContext({}),d=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},c=function(e){var t=d(e.components);return r.createElement(s.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=d(a),u=n,f=p["".concat(s,".").concat(u)]||p[u]||m[u]||i;return a?r.createElement(f,o(o({ref:t},c),{},{components:a})):r.createElement(f,o({ref:t},c))}));function f(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,o=new Array(i);o[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:n,o[1]=l;for(var d=2;d<i;d++)o[d]=a[d];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}u.displayName="MDXCreateElement"},85461:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var r=a(87462),n=(a(67294),a(3905));const i={},o="Clarifai",l={unversionedId:"integrations/providers/clarifai",id:"integrations/providers/clarifai",title:"Clarifai",description:"Clarifai is one of first deep learning platforms having been founded in 2013. Clarifai provides an AI platform with the full AI lifecycle for data exploration, data labeling, model training, evaluation and inference around images, video, text and audio data. In the LangChain ecosystem, as far as we're aware, Clarifai is the only provider that supports LLMs, embeddings and a vector store in one production scale platform, making it an excellent choice to operationalize your LangChain implementations.",source:"@site/docs/integrations/providers/clarifai.mdx",sourceDirName:"integrations/providers",slug:"/integrations/providers/clarifai",permalink:"/langchain/docs/integrations/providers/clarifai",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Chroma",permalink:"/langchain/docs/integrations/providers/chroma"},next:{title:"ClearML",permalink:"/langchain/docs/integrations/providers/clearml_tracking"}},s={},d=[{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"Models",id:"models",level:2},{value:"LLMs",id:"llms",level:3},{value:"Text Embedding Models",id:"text-embedding-models",level:3},{value:"Vectorstore",id:"vectorstore",level:2}],c={toc:d},p="wrapper";function m(e){let{components:t,...a}=e;return(0,n.kt)(p,(0,r.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"clarifai"},"Clarifai"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},(0,n.kt)("a",{parentName:"p",href:"https://clarifai.com"},"Clarifai")," is one of first deep learning platforms having been founded in 2013. Clarifai provides an AI platform with the full AI lifecycle for data exploration, data labeling, model training, evaluation and inference around images, video, text and audio data. In the LangChain ecosystem, as far as we're aware, Clarifai is the only provider that supports LLMs, embeddings and a vector store in one production scale platform, making it an excellent choice to operationalize your LangChain implementations.")),(0,n.kt)("h2",{id:"installation-and-setup"},"Installation and Setup"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Install the Python SDK:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"pip install clarifai\n")),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://clarifai.com/signup"},"Sign-up")," for a Clarifai account, then get a personal access token to access the Clarifai API from your ",(0,n.kt)("a",{parentName:"p",href:"https://clarifai.com/settings/security"},"security settings")," and set it as an environment variable (",(0,n.kt)("inlineCode",{parentName:"p"},"CLARIFAI_PAT"),")."),(0,n.kt)("h2",{id:"models"},"Models"),(0,n.kt)("p",null,"Clarifai provides 1,000s of AI models for many different use cases. You can ",(0,n.kt)("a",{parentName:"p",href:"https://clarifai.com/explore"},"explore them here")," to find the one most suited for your use case. These models include those created by other providers such as OpenAI, Anthropic, Cohere, AI21, etc. as well as state of the art from open source such as Falcon, InstructorXL, etc. so that you build the best in AI into your products. You'll find these organized by the creator's user_id and into projects we call applications denoted by their app_id. Those IDs will be needed in additional to the model_id and optionally the version_id, so make note of all these IDs once you found the best model for your use case!"),(0,n.kt)("p",null,"Also note that given there are many models for images, video, text and audio understanding, you can build some interested AI agents that utilize the variety of AI models as experts to understand those data types."),(0,n.kt)("h3",{id:"llms"},"LLMs"),(0,n.kt)("p",null,"To find the selection of LLMs in the Clarifai platform you can select the text to text model type ",(0,n.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22text-to-text%22%5D%7D%5D&page=1&perPage=24"},"here"),"."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.llms import Clarifai\nllm = Clarifai(pat=CLARIFAI_PAT, user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\n")),(0,n.kt)("p",null,"For more details, the docs on the Clarifai LLM wrapper provide a ",(0,n.kt)("a",{parentName:"p",href:"/docs/integrations/llms/clarifai.html"},"detailed walkthrough"),"."),(0,n.kt)("h3",{id:"text-embedding-models"},"Text Embedding Models"),(0,n.kt)("p",null,"To find the selection of text embeddings models in the Clarifai platform you can select the text to embedding model type ",(0,n.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22text-embedder%22%5D%7D%5D"},"here"),"."),(0,n.kt)("p",null,"There is a Clarifai Embedding model in LangChain, which you can access with:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.embeddings import ClarifaiEmbeddings\nembeddings = ClarifaiEmbeddings(pat=CLARIFAI_PAT, user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\n")),(0,n.kt)("p",null,"For more details, the docs on the Clarifai Embeddings wrapper provide a ",(0,n.kt)("a",{parentName:"p",href:"/docs/integrations/text_embedding/clarifai.html"},"detailed walkthrough"),"."),(0,n.kt)("h2",{id:"vectorstore"},"Vectorstore"),(0,n.kt)("p",null,"Clarifai's vector DB was launched in 2016 and has been optimized to support live search queries. With workflows in the Clarifai platform, you data is automatically indexed by am embedding model and optionally other models as well to index that information in the DB for search. You can query the DB not only via the vectors but also filter by metadata matches, other AI predicted concepts, and even do geo-coordinate search. Simply create an application, select the appropriate base workflow for your type of data, and upload it (through the API as ",(0,n.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/data/create-get-update-delete"},"documented here")," or the UIs at clarifai.com)."),(0,n.kt)("p",null,"You an also add data directly from LangChain as well, and the auto-indexing will take place for you. You'll notice this is a little different than other vectorstores where you need to provde an embedding model in their constructor and have LangChain coordinate getting the embeddings from text and writing those to the index. Not only is it more convenient, but it's much more scalable to use Clarifai's distributed cloud to do all the index in the background."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.vectorstores import Clarifai\nclarifai_vector_db = Clarifai.from_texts(user_id=USER_ID, app_id=APP_ID, texts=texts, pat=CLARIFAI_PAT, number_of_docs=NUMBER_OF_DOCS, metadatas = metadatas)\n")),(0,n.kt)("p",null,"For more details, the docs on the Clarifai vector store provide a ",(0,n.kt)("a",{parentName:"p",href:"/docs/integrations/text_embedding/clarifai.html"},"detailed walkthrough"),"."))}m.isMDXComponent=!0}}]);