"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[78277],{3905:(e,a,t)=>{t.d(a,{Zo:()=>m,kt:()=>u});var n=t(67294);function l(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){l(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,l=function(e,a){if(null==e)return{};var t,n,l={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(l[t]=e[t]);return l}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(l[t]=e[t])}return l}var r=n.createContext({}),c=function(e){var a=n.useContext(r),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},m=function(e){var a=c(e.components);return n.createElement(r.Provider,{value:a},e.children)},h="mdxType",p={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},d=n.forwardRef((function(e,a){var t=e.components,l=e.mdxType,o=e.originalType,r=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),h=c(t),d=l,u=h["".concat(r,".").concat(d)]||h[d]||p[d]||o;return t?n.createElement(u,i(i({ref:a},m),{},{components:t})):n.createElement(u,i({ref:a},m))}));function u(e,a){var t=arguments,l=a&&a.mdxType;if("string"==typeof e||l){var o=t.length,i=new Array(o);i[0]=d;var s={};for(var r in a)hasOwnProperty.call(a,r)&&(s[r]=a[r]);s.originalType=e,s[h]="string"==typeof e?e:l,i[1]=s;for(var c=2;c<o;c++)i[c]=t[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}d.displayName="MDXCreateElement"},23335:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>r,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var n=t(87462),l=(t(67294),t(3905));const o={},i="Ollama",s={unversionedId:"integrations/chat/ollama",id:"integrations/chat/ollama",title:"Ollama",description:"Ollama allows you to run open-source large language models, such as LLaMA2, locally.",source:"@site/docs/integrations/chat/ollama.md",sourceDirName:"integrations/chat",slug:"/integrations/chat/ollama",permalink:"/langchain/docs/integrations/chat/ollama",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Llama API",permalink:"/langchain/docs/integrations/chat/llama_api"},next:{title:"OpenAI",permalink:"/langchain/docs/integrations/chat/openai"}},r={},c=[{value:"Setup",id:"setup",level:2},{value:"Usage",id:"usage",level:2},{value:"RAG",id:"rag",level:2}],m=(h="CodeOutputBlock",function(e){return console.warn("Component "+h+" was not imported, exported, or provided by MDXProvider as global scope"),(0,l.kt)("div",e)});var h;const p={toc:c},d="wrapper";function u(e){let{components:a,...t}=e;return(0,l.kt)(d,(0,n.Z)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"ollama"},"Ollama"),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://ollama.ai/"},"Ollama")," allows you to run open-source large language models, such as LLaMA2, locally."),(0,l.kt)("p",null,"Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. "),(0,l.kt)("p",null,"It optimizes setup and configuration details, including GPU usage."),(0,l.kt)("p",null,"For a complete list of supported models and model variants, see the ",(0,l.kt)("a",{parentName:"p",href:"https://ollama.ai/library"},"Ollama model library"),"."),(0,l.kt)("h2",{id:"setup"},"Setup"),(0,l.kt)("p",null,"First, follow ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/jmorganca/ollama"},"these instructions")," to set up and run a local Ollama instance:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://ollama.ai/download"},"Download")),(0,l.kt)("li",{parentName:"ul"},"Fetch a model via ",(0,l.kt)("inlineCode",{parentName:"li"},"ollama pull <model family>")),(0,l.kt)("li",{parentName:"ul"},"e.g., for ",(0,l.kt)("inlineCode",{parentName:"li"},"Llama-7b"),": ",(0,l.kt)("inlineCode",{parentName:"li"},"ollama pull llama2")),(0,l.kt)("li",{parentName:"ul"},"This will download the most basic version of the model (e.g., minimum # parameters and 4-bit quantization)"),(0,l.kt)("li",{parentName:"ul"},"On Mac, it will download to:")),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"~/.ollama/models/manifests/registry.ollama.ai/library/<model family>/latest")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"And we can specify a particular version, e.g., for ",(0,l.kt)("inlineCode",{parentName:"li"},"ollama pull vicuna:13b-v1.5-16k-q4_0")),(0,l.kt)("li",{parentName:"ul"},"The file is here with the model version in place of ",(0,l.kt)("inlineCode",{parentName:"li"},"latest"))),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"~/.ollama/models/manifests/registry.ollama.ai/library/vicuna/13b-v1.5-16k-q4_0")),(0,l.kt)("p",null,"You can easily access models in a few ways:"),(0,l.kt)("p",null,"1/ if the app is running:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"All of your local models are automatically served on ",(0,l.kt)("inlineCode",{parentName:"li"},"localhost:11434")),(0,l.kt)("li",{parentName:"ul"},"Select your model when setting ",(0,l.kt)("inlineCode",{parentName:"li"},'llm = Ollama(..., model="<model family>:<version>")')),(0,l.kt)("li",{parentName:"ul"},"If you set ",(0,l.kt)("inlineCode",{parentName:"li"},'llm = Ollama(..., model="<model family")')," withoout a version it will simply look for ",(0,l.kt)("inlineCode",{parentName:"li"},"latest"))),(0,l.kt)("p",null,"2/ if building from source or just running the binary: "),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Then you must run ",(0,l.kt)("inlineCode",{parentName:"li"},"ollama serve")),(0,l.kt)("li",{parentName:"ul"},"All of your local models are automatically served on ",(0,l.kt)("inlineCode",{parentName:"li"},"localhost:11434")),(0,l.kt)("li",{parentName:"ul"},"Then, select as shown above")),(0,l.kt)("h2",{id:"usage"},"Usage"),(0,l.kt)("p",null,"You can see a full list of supported parameters on the ",(0,l.kt)("a",{parentName:"p",href:"https://api.python.langchain.com/en/latest/llms/langchain.llms.ollama.Ollama.html"},"API reference page"),"."),(0,l.kt)("p",null,"If you are using a LLaMA ",(0,l.kt)("inlineCode",{parentName:"p"},"chat")," model (e.g., ",(0,l.kt)("inlineCode",{parentName:"p"},"ollama pull llama2:7b-chat"),") then you can use the ",(0,l.kt)("inlineCode",{parentName:"p"},"ChatOllama")," interface."),(0,l.kt)("p",null,"This includes ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/blog/llama2#how-to-prompt-llama-2"},"special tokens")," for system message and user input."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatOllama", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.ollama.ChatOllama.html", "title": "Ollama"}, {"imported": "CallbackManager", "source": "langchain.callbacks.manager", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.CallbackManager.html", "title": "Ollama"}, {"imported": "StreamingStdOutCallbackHandler", "source": "langchain.callbacks.streaming_stdout", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html", "title": "Ollama"}]--\x3e\nfrom langchain.chat_models import ChatOllama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler                                  \nchat_model = ChatOllama(model="llama2:7b-chat", \n                        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\n')),(0,l.kt)("p",null,"With ",(0,l.kt)("inlineCode",{parentName:"p"},"StreamingStdOutCallbackHandler"),", you will see tokens streamed."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain.schema", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.HumanMessage.html", "title": "Ollama"}]--\x3e\nfrom langchain.schema import HumanMessage\n\nmessages = [\n    HumanMessage(content="Tell me about the history of AI")\n]\nchat_model(messages)\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'     Artificial intelligence (AI) has a rich and varied history that spans several decades. Hinweis: The following is a brief overview of the major milestones in the history of AI, but it is by no means exhaustive.\n    \n    1. Early Beginnings (1950s-1960s): The term "Artificial Intelligence" was coined in 1956 by computer scientist John McCarthy. However, the concept of creating machines that can think and learn like humans dates back to ancient times. In the 1950s and 1960s, researchers began exploring the possibilities of AI using simple algorithms and machine learning techniques.\n    2. Rule-Based Systems (1970s-1980s): In the 1970s and 1980s, AI research focused on developing rule-based systems, which use predefined rules to reason and make decisions. This led to the development of expert systems, which were designed to mimic the decision-making abilities of human experts in specific domains.\n    3. Machine Learning (1980s-1990s): The 1980s saw a shift towards machine learning, which enables machines to learn from data without being explicitly programmed. This led to the development of algorithms such as decision trees, neural networks, and support vector machines.\n    4. Deep Learning (2000s-present): In the early 2000s, deep learning emerged as a subfield of machine learning, focusing on neural networks with multiple layers. These networks can learn complex representations of data, leading to breakthroughs in image and speech recognition, natural language processing, and other areas.\n    5. Natural Language Processing (NLP) (1980s-present): NLP has been an active area of research since the 1980s, with a focus on developing algorithms that can understand and generate human language. This has led to applications such as chatbots, voice assistants, and language translation systems.\n    6. Robotics (1970s-present): The development of robotics has been closely tied to AI research, with a focus on creating machines that can perform tasks that typically require human intelligence, such as manipulation and locomotion.\n    7. Computer Vision (1980s-present): Computer vision has been an active area of research since the 1980s, with a focus on enabling machines to interpret and understand visual data from the world around us. This has led to applications such as image recognition, object detection, and autonomous driving.\n    8. Ethics and Society (1990s-present): As AI technology has become more advanced and integrated into various aspects of society, there has been a growing concern about the ethical implications of AI. This includes issues related to privacy, bias, and job displacement.\n    9. Reinforcement Learning (2000s-present): Reinforcement learning is a subfield of machine learning that involves training machines to make decisions based on feedback from their environment. This has led to breakthroughs in areas such as game playing, robotics, and autonomous driving.\n    10. Generative Models (2010s-present): Generative models are a class of AI algorithms that can generate new data that is similar to a given dataset. This has led to applications such as image synthesis, music generation, and language creation.\n    \n    These are just a few of the many developments in the history of AI. As the field continues to evolve, we can expect even more exciting breakthroughs and innovations in the years to come.\n\n\n\n\n    AIMessage(content=\' Artificial intelligence (AI) has a rich and varied history that spans several decades. Hinweis: The following is a brief overview of the major milestones in the history of AI, but it is by no means exhaustive.\\n\\n1. Early Beginnings (1950s-1960s): The term "Artificial Intelligence" was coined in 1956 by computer scientist John McCarthy. However, the concept of creating machines that can think and learn like humans dates back to ancient times. In the 1950s and 1960s, researchers began exploring the possibilities of AI using simple algorithms and machine learning techniques.\\n2. Rule-Based Systems (1970s-1980s): In the 1970s and 1980s, AI research focused on developing rule-based systems, which use predefined rules to reason and make decisions. This led to the development of expert systems, which were designed to mimic the decision-making abilities of human experts in specific domains.\\n3. Machine Learning (1980s-1990s): The 1980s saw a shift towards machine learning, which enables machines to learn from data without being explicitly programmed. This led to the development of algorithms such as decision trees, neural networks, and support vector machines.\\n4. Deep Learning (2000s-present): In the early 2000s, deep learning emerged as a subfield of machine learning, focusing on neural networks with multiple layers. These networks can learn complex representations of data, leading to breakthroughs in image and speech recognition, natural language processing, and other areas.\\n5. Natural Language Processing (NLP) (1980s-present): NLP has been an active area of research since the 1980s, with a focus on developing algorithms that can understand and generate human language. This has led to applications such as chatbots, voice assistants, and language translation systems.\\n6. Robotics (1970s-present): The development of robotics has been closely tied to AI research, with a focus on creating machines that can perform tasks that typically require human intelligence, such as manipulation and locomotion.\\n7. Computer Vision (1980s-present): Computer vision has been an active area of research since the 1980s, with a focus on enabling machines to interpret and understand visual data from the world around us. This has led to applications such as image recognition, object detection, and autonomous driving.\\n8. Ethics and Society (1990s-present): As AI technology has become more advanced and integrated into various aspects of society, there has been a growing concern about the ethical implications of AI. This includes issues related to privacy, bias, and job displacement.\\n9. Reinforcement Learning (2000s-present): Reinforcement learning is a subfield of machine learning that involves training machines to make decisions based on feedback from their environment. This has led to breakthroughs in areas such as game playing, robotics, and autonomous driving.\\n10. Generative Models (2010s-present): Generative models are a class of AI algorithms that can generate new data that is similar to a given dataset. This has led to applications such as image synthesis, music generation, and language creation.\\n\\nThese are just a few of the many developments in the history of AI. As the field continues to evolve, we can expect even more exciting breakthroughs and innovations in the years to come.\', additional_kwargs={}, example=False)\n'))),(0,l.kt)("h2",{id:"rag"},"RAG"),(0,l.kt)("p",null,"We can use Olama with RAG, ",(0,l.kt)("a",{parentName:"p",href:"https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa"},"just as shown here"),"."),(0,l.kt)("p",null,"Let's use the 13b model:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"ollama pull llama2:13b\n")),(0,l.kt)("p",null,"Or, the 13b-chat model:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"ollama pull llama2:13b-chat\n")),(0,l.kt)("p",null,"Let's also use local embeddings from ",(0,l.kt)("inlineCode",{parentName:"p"},"GPT4AllEmbeddings")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"Chroma"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"pip install gpt4all chromadb\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "WebBaseLoader", "source": "langchain.document_loaders", "docs": "https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.web_base.WebBaseLoader.html", "title": "Ollama"}, {"imported": "RecursiveCharacterTextSplitter", "source": "langchain.text_splitter", "docs": "https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html", "title": "Ollama"}]--\x3e\nfrom langchain.document_loaders import WebBaseLoader\nloader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")\ndata = loader.load()\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "Chroma", "source": "langchain.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html", "title": "Ollama"}, {"imported": "GPT4AllEmbeddings", "source": "langchain.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.gpt4all.GPT4AllEmbeddings.html", "title": "Ollama"}]--\x3e\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import GPT4AllEmbeddings\n\nvectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'question = "What are the approaches to Task Decomposition?"\ndocs = vectorstore.similarity_search(question)\nlen(docs)\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    4\n"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from langchain import PromptTemplate\n\n# Prompt\ntemplate = """[INST] <<SYS>> Use the following pieces of context to answer the question at the end. \nIf you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer. \nUse three sentences maximum and keep the answer as concise as possible. <</SYS>>\n{context}\nQuestion: {question}\nHelpful Answer:[/INST]"""\nQA_CHAIN_PROMPT = PromptTemplate(\n    input_variables=["context", "question"],\n    template=template,\n)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "ChatOllama", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.ollama.ChatOllama.html", "title": "Ollama"}, {"imported": "CallbackManager", "source": "langchain.callbacks.manager", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.CallbackManager.html", "title": "Ollama"}, {"imported": "StreamingStdOutCallbackHandler", "source": "langchain.callbacks.streaming_stdout", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html", "title": "Ollama"}]--\x3e\n# Chat model\nfrom langchain.chat_models import ChatOllama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nchat_model = ChatOllama(model="llama2:13b-chat",\n                        verbose=True,\n                        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "RetrievalQA", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html", "title": "Ollama"}]--\x3e\n# QA chain\nfrom langchain.chains import RetrievalQA\nqa_chain = RetrievalQA.from_chain_type(\n    chat_model,\n    retriever=vectorstore.as_retriever(),\n    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},\n)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'question = "What are the various approaches to Task Decomposition for AI Agents?"\nresult = qa_chain({"query": question})\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'     Based on the provided context, there are three approaches to task decomposition for AI agents:\n    \n    1. LLM with simple prompting, such as "Steps for XYZ." or "What are the subgoals for achieving XYZ?"\n    2. Task-specific instructions, such as "Write a story outline" for writing a novel.\n    3. Human inputs.\n'))),(0,l.kt)("p",null,"You can also get logging for tokens."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "LLMResult", "source": "langchain.schema", "docs": "https://api.python.langchain.com/en/latest/schema/langchain.schema.output.LLMResult.html", "title": "Ollama"}, {"imported": "BaseCallbackHandler", "source": "langchain.callbacks.base", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.base.BaseCallbackHandler.html", "title": "Ollama"}]--\x3e\nfrom langchain.schema import LLMResult\nfrom langchain.callbacks.base import BaseCallbackHandler\n\nclass GenerationStatisticsCallback(BaseCallbackHandler):\n    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n        print(response.generations[0][0].generation_info)\n        \ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler(), GenerationStatisticsCallback()])\n\nchat_model = ChatOllama(model="llama2:13b-chat",\n                        verbose=True,\n                        callback_manager=callback_manager)\n\nqa_chain = RetrievalQA.from_chain_type(\n    chat_model,\n    retriever=vectorstore.as_retriever(),\n    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},\n)\n\nquestion = "What are the approaches to Task Decomposition?"\nresult = qa_chain({"query": question})\n')),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"     Based on the given context, here is the answer to the question \"What are the approaches to Task Decomposition?\"\n    \n    There are three approaches to task decomposition:\n    \n    1. LLM with simple prompting, such as \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\"\n    2. Using task-specific instructions, like \"Write a story outline\" for writing a novel.\n    3. With human inputs.{'model': 'llama2:13b-chat', 'created_at': '2023-08-23T15:37:51.469127Z', 'done': True, 'context': [1, 29871, 1, 29961, 25580, 29962, 518, 25580, 29962, 518, 25580, 29962, 3532, 14816, 29903, 6778, 4803, 278, 1494, 12785, 310, 3030, 304, 1234, 278, 1139, 472, 278, 1095, 29889, 29871, 13, 3644, 366, 1016, 29915, 29873, 1073, 278, 1234, 29892, 925, 1827, 393, 366, 1016, 29915, 29873, 1073, 29892, 1016, 29915, 29873, 1018, 304, 1207, 701, 385, 1234, 29889, 29871, 13, 11403, 2211, 25260, 7472, 322, 3013, 278, 1234, 408, 3022, 895, 408, 1950, 29889, 529, 829, 14816, 29903, 6778, 13, 5398, 26227, 508, 367, 2309, 313, 29896, 29897, 491, 365, 26369, 411, 2560, 9508, 292, 763, 376, 7789, 567, 363, 1060, 29979, 29999, 7790, 29876, 29896, 19602, 376, 5618, 526, 278, 1014, 1484, 1338, 363, 3657, 15387, 1060, 29979, 29999, 29973, 613, 313, 29906, 29897, 491, 773, 3414, 29899, 14940, 11994, 29936, 321, 29889, 29887, 29889, 376, 6113, 263, 5828, 27887, 1213, 363, 5007, 263, 9554, 29892, 470, 313, 29941, 29897, 411, 5199, 10970, 29889, 13, 13, 5398, 26227, 508, 367, 2309, 313, 29896, 29897, 491, 365, 26369, 411, 2560, 9508, 292, 763, 376, 7789, 567, 363, 1060, 29979, 29999, 7790, 29876, 29896, 19602, 376, 5618, 526, 278, 1014, 1484, 1338, 363, 3657, 15387, 1060, 29979, 29999, 29973, 613, 313, 29906, 29897, 491, 773, 3414, 29899, 14940, 11994, 29936, 321, 29889, 29887, 29889, 376, 6113, 263, 5828, 27887, 1213, 363, 5007, 263, 9554, 29892, 470, 313, 29941, 29897, 411, 5199, 10970, 29889, 13, 13, 1451, 16047, 267, 297, 1472, 29899, 8489, 18987, 322, 3414, 26227, 29901, 1858, 9450, 975, 263, 3309, 29891, 4955, 322, 17583, 3902, 8253, 278, 1650, 2913, 3933, 18066, 292, 29889, 365, 26369, 29879, 21117, 304, 10365, 13900, 746, 20050, 411, 15668, 4436, 29892, 3907, 963, 3109, 16424, 9401, 304, 25618, 1058, 5110, 515, 14260, 322, 1059, 29889, 13, 13, 1451, 16047, 267, 297, 1472, 29899, 8489, 18987, 322, 3414, 26227, 29901, 1858, 9450, 975, 263, 3309, 29891, 4955, 322, 17583, 3902, 8253, 278, 1650, 2913, 3933, 18066, 292, 29889, 365, 26369, 29879, 21117, 304, 10365, 13900, 746, 20050, 411, 15668, 4436, 29892, 3907, 963, 3109, 16424, 9401, 304, 25618, 1058, 5110, 515, 14260, 322, 1059, 29889, 13, 16492, 29901, 1724, 526, 278, 13501, 304, 9330, 897, 510, 3283, 29973, 13, 29648, 1319, 673, 10834, 29914, 25580, 29962, 518, 29914, 25580, 29962, 518, 29914, 25580, 29962, 29871, 16564, 373, 278, 2183, 3030, 29892, 1244, 338, 278, 1234, 304, 278, 1139, 376, 5618, 526, 278, 13501, 304, 9330, 897, 510, 3283, 3026, 13, 13, 8439, 526, 2211, 13501, 304, 3414, 26227, 29901, 13, 13, 29896, 29889, 365, 26369, 411, 2560, 9508, 292, 29892, 1316, 408, 376, 7789, 567, 363, 1060, 29979, 29999, 1213, 470, 376, 5618, 526, 278, 1014, 1484, 1338, 363, 3657, 15387, 1060, 29979, 29999, 3026, 13, 29906, 29889, 5293, 3414, 29899, 14940, 11994, 29892, 763, 376, 6113, 263, 5828, 27887, 29908, 363, 5007, 263, 9554, 29889, 13, 29941, 29889, 2973, 5199, 10970, 29889, 2], 'total_duration': 9514823750, 'load_duration': 795542, 'sample_count': 99, 'sample_duration': 68732000, 'prompt_eval_count': 146, 'prompt_eval_duration': 6206275000, 'eval_count': 98, 'eval_duration': 3229641000}\n"))),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"eval_count")," / (",(0,l.kt)("inlineCode",{parentName:"p"},"eval_duration"),"/10e9)  gets ",(0,l.kt)("inlineCode",{parentName:"p"},"tok / s")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"98 / (3229641000/1000/1000/1000)\n")),(0,l.kt)(m,{lang:"python",mdxType:"CodeOutputBlock"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    30.343929867127645\n"))))}u.isMDXComponent=!0}}]);