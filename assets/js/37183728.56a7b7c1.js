"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[11847],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>u});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var i=o.createContext({}),s=function(e){var t=o.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},m=function(e){var t=s(e.components);return o.createElement(i.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,i=e.parentName,m=c(e,["components","mdxType","originalType","parentName"]),p=s(n),h=a,u=p["".concat(i,".").concat(h)]||p[h]||d[h]||r;return n?o.createElement(u,l(l({ref:t},m),{},{components:n})):o.createElement(u,l({ref:t},m))}));function u(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,l=new Array(r);l[0]=h;var c={};for(var i in t)hasOwnProperty.call(t,i)&&(c[i]=t[i]);c.originalType=e,c[p]="string"==typeof e?e:a,l[1]=c;for(var s=2;s<r;s++)l[s]=n[s];return o.createElement.apply(null,l)}return o.createElement.apply(null,n)}h.displayName="MDXCreateElement"},76559:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>p,default:()=>f,frontMatter:()=>m,metadata:()=>d,toc:()=>u});var o=n(87462),a=(n(67294),n(3905));const r=(l="CodeOutputBlock",function(e){return console.warn("Component "+l+" was not imported, exported, or provided by MDXProvider as global scope"),(0,a.kt)("div",e)});var l;const c={toc:[{value:"In Memory Cache",id:"in-memory-cache",level:2},{value:"SQLite Cache",id:"sqlite-cache",level:2}]},i="wrapper";function s(e){let{components:t,...n}=e;return(0,a.kt)(i,(0,o.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import langchain\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI()\n")),(0,a.kt)("h2",{id:"in-memory-cache"},"In Memory Cache"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.cache import InMemoryCache\nlangchain.llm_cache = InMemoryCache()\n\n# The first time, it is not yet in cache, so it should take longer\nllm.predict("Tell me a joke")\n')),(0,a.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'    CPU times: user 35.9 ms, sys: 28.6 ms, total: 64.6 ms\n    Wall time: 4.83 s\n    \n\n    "\\n\\nWhy couldn\'t the bicycle stand up by itself? It was...two tired!"\n'))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# The second time it is, so it goes faster\nllm.predict("Tell me a joke")\n')),(0,a.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    CPU times: user 238 \xb5s, sys: 143 \xb5s, total: 381 \xb5s\n    Wall time: 1.76 ms\n\n\n    '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'\n"))),(0,a.kt)("h2",{id:"sqlite-cache"},"SQLite Cache"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"rm .langchain.db\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# We can do the same thing with a SQLite cache\nfrom langchain.cache import SQLiteCache\nlangchain.llm_cache = SQLiteCache(database_path=".langchain.db")\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# The first time, it is not yet in cache, so it should take longer\nllm.predict("Tell me a joke")\n')),(0,a.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    CPU times: user 17 ms, sys: 9.76 ms, total: 26.7 ms\n    Wall time: 825 ms\n\n\n    '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'\n"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# The second time it is, so it goes faster\nllm.predict("Tell me a joke")\n')),(0,a.kt)(r,{lang:"python",mdxType:"CodeOutputBlock"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    CPU times: user 2.46 ms, sys: 1.23 ms, total: 3.7 ms\n    Wall time: 2.67 ms\n\n\n    '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'\n"))))}s.isMDXComponent=!0;const m={},p="Caching",d={unversionedId:"modules/model_io/models/chat/chat_model_caching",id:"modules/model_io/models/chat/chat_model_caching",title:"Caching",description:"LangChain provides an optional caching layer for chat models. This is useful for two reasons:",source:"@site/docs/modules/model_io/models/chat/chat_model_caching.mdx",sourceDirName:"modules/model_io/models/chat",slug:"/modules/model_io/models/chat/chat_model_caching",permalink:"/langchain/docs/modules/model_io/models/chat/chat_model_caching",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Chat models",permalink:"/langchain/docs/modules/model_io/models/chat/"},next:{title:"Human input chat model",permalink:"/langchain/docs/modules/model_io/models/chat/human_input_chat_model"}},h={},u=[],y={toc:u},g="wrapper";function f(e){let{components:t,...n}=e;return(0,a.kt)(g,(0,o.Z)({},y,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"caching"},"Caching"),(0,a.kt)("p",null,"LangChain provides an optional caching layer for chat models. This is useful for two reasons:"),(0,a.kt)("p",null,"It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\nIt can speed up your application by reducing the number of API calls you make to the LLM provider."),(0,a.kt)(s,{mdxType:"CachingChat"}))}f.isMDXComponent=!0}}]);