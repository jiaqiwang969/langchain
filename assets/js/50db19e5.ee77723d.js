"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[36241],{3905:(e,n,t)=>{t.d(n,{Zo:()=>s,kt:()=>h});var a=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var p=a.createContext({}),c=function(e){var n=a.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},s=function(e){var n=c(e.components);return a.createElement(p.Provider,{value:n},e.children)},g="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,p=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),g=c(t),u=r,h=g["".concat(p,".").concat(u)]||g[u]||m[u]||o;return t?a.createElement(h,i(i({ref:n},s),{},{components:t})):a.createElement(h,i({ref:n},s))}));function h(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=u;var l={};for(var p in n)hasOwnProperty.call(n,p)&&(l[p]=n[p]);l.originalType=e,l[g]="string"==typeof e?e:r,i[1]=l;for(var c=2;c<o;c++)i[c]=t[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},69697:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var a=t(87462),r=(t(67294),t(3905));const o={},i="Hugging Face Local Pipelines",l={unversionedId:"integrations/llms/huggingface_pipelines",id:"integrations/llms/huggingface_pipelines",title:"Hugging Face Local Pipelines",description:"Hugging Face models can be run locally through the HuggingFacePipeline class.",source:"@site/docs/integrations/llms/huggingface_pipelines.md",sourceDirName:"integrations/llms",slug:"/integrations/llms/huggingface_pipelines",permalink:"/langchain/docs/integrations/llms/huggingface_pipelines",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Hugging Face Hub",permalink:"/langchain/docs/integrations/llms/huggingface_hub"},next:{title:"Huggingface TextGen Inference",permalink:"/langchain/docs/integrations/llms/huggingface_textgen_inference"}},p={},c=[{value:"Load the model",id:"load-the-model",level:3},{value:"Create Chain",id:"create-chain",level:3}],s=(g="CodeOutputBlock",function(e){return console.warn("Component "+g+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var g;const m={toc:c},u="wrapper";function h(e){let{components:n,...t}=e;return(0,r.kt)(u,(0,a.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"hugging-face-local-pipelines"},"Hugging Face Local Pipelines"),(0,r.kt)("p",null,"Hugging Face models can be run locally through the ",(0,r.kt)("inlineCode",{parentName:"p"},"HuggingFacePipeline")," class."),(0,r.kt)("p",null,"The ",(0,r.kt)("a",{parentName:"p",href:"https://huggingface.co/models"},"Hugging Face Model Hub")," hosts over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together."),(0,r.kt)("p",null,"These can be called from LangChain either through this local pipeline wrapper or by calling their hosted inference endpoints through the HuggingFaceHub class. For more information on the hosted pipelines, see the ",(0,r.kt)("a",{parentName:"p",href:"huggingface_hub.html"},"HuggingFaceHub")," notebook."),(0,r.kt)("p",null,"To use, you should have the ",(0,r.kt)("inlineCode",{parentName:"p"},"transformers")," python ",(0,r.kt)("a",{parentName:"p",href:"https://pypi.org/project/transformers/"},"package installed"),", as well as ",(0,r.kt)("a",{parentName:"p",href:"https://pytorch.org/get-started/locally/"},"pytorch"),". You can also install ",(0,r.kt)("inlineCode",{parentName:"p"},"xformer")," for a more memory-efficient attention implementation."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"%pip install transformers --quiet\n")),(0,r.kt)("h3",{id:"load-the-model"},"Load the model"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "HuggingFacePipeline", "source": "langchain.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain.llms.huggingface_pipeline.HuggingFacePipeline.html", "title": "Hugging Face Local Pipelines"}]--\x3e\nfrom langchain.llms import HuggingFacePipeline\n\nllm = HuggingFacePipeline.from_model_id(\n    model_id="bigscience/bloom-1b7",\n    task="text-generation",\n    model_kwargs={"temperature": 0, "max_length": 64},\n)\n')),(0,r.kt)("h3",{id:"create-chain"},"Create Chain"),(0,r.kt)("p",null,"With the model loaded into memory, you can compose it with a prompt to\nform a chain."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\x3c!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html", "title": "Hugging Face Local Pipelines"}]--\x3e\nfrom langchain.prompts import PromptTemplate\n\ntemplate = """Question: {question}\n\nAnswer: Let\'s think step by step."""\nprompt = PromptTemplate.from_template(template)\n\nchain = prompt | llm\n\nquestion = "What is electroencephalography?"\n\nprint(chain.invoke({"question": question}))\n')),(0,r.kt)(s,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"     First, we need to understand what is an electroencephalogram. An electroencephalogram is a recording of brain activity. It is a recording of brain activity that is made by placing electrodes on the scalp. The electrodes are placed\n"))))}h.isMDXComponent=!0}}]);